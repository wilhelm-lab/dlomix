{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YWkUVjVr7qJ"
   },
   "source": [
    "# Peptide Detectability (Training and Fine-tuning) \n",
    "\n",
    "This notebook is prepared to be run in Google [Colaboratory](https://colab.research.google.com/). In order to train the model faster, please change the runtime of Colab to use Hardware Accelerator, either GPU or TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3DlTOq3r7qM"
   },
   "source": [
    "This notebook provides a concise walkthrough of the process for reading a dataset, training, and fine-tuning a model for peptide detectability prediction. \n",
    "\n",
    "The dataset used in this example is derived from:\n",
    "\n",
    "- **ProteomTools Dataset**: Includes data from the PRIDE repository with the following identifiers: `PXD004732`, `PXD010595`, and `PXD021013`.\n",
    "- **MAssIVE Dataset**: Deposited in the ProteomeXchange Consortium via the MAssIVE partner repository with the identifier `PXD024364`.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing the DLOmix Package\n",
    "\n",
    "If you have not installed the DLOmix package yet, you need to do so before running the code. \n",
    "\n",
    "You can install the DLOmix package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-69zbKsGey",
    "outputId": "c2064411-9f80-47e6-ca5b-312d547e0f6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment the following line to install the DLOmix package in the current environment using pip\n",
    "\n",
    "#!python -m pip install dlomix>0.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo7H9qzWr7qN"
   },
   "source": [
    "#### Importing Required Libraries\n",
    "\n",
    "Before running the code, ensure you import all the necessary libraries. These imports are essential for accessing the functionalities needed for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0CS0tFur7qN",
    "outputId": "664e0978-980a-4254-90d1-61e9f1603234"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import dlomix\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlomix.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41qXroyKr7qP"
   },
   "source": [
    "## 1. Load Data for Training\n",
    "\n",
    "You can import the `DetectabilityDataset` class and create an instance to manage data for training, validation, and testing. This instance handles TensorFlow dataset objects and simplifies configuring and controlling how your data is preprocessed and split.\n",
    "\n",
    "For the paramters of the dataset class, please refer to the DLOmix documentation: https://dlomix.readthedocs.io/en/main/dlomix.data.html#\n",
    "\n",
    "\n",
    "**Note**: If class labels are provided, the following encoding scheme should be used:\n",
    "- **Non-Flyer**: 0\n",
    "- **Weak Flyer**: 1\n",
    "- **Intermediate Flyer**: 2\n",
    "- **Strong Flyer**: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiXz_epEr7qQ"
   },
   "outputs": [],
   "source": [
    "from dlomix.data import DetectabilityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.constants import CLASSES_LABELS, alphabet, aa_to_int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LABELS, len(alphabet), aa_to_int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pep_length = 40\n",
    "BATCH_SIZE = 128 \n",
    "            \n",
    "# The Class handles all the inner details, we have to provide the column names and the alphabet for encoding\n",
    "# If the data is already split with a specific logic (which is generally recommended) -> val_data_source and test_data_source are available as well\n",
    "\n",
    "hf_data = \"Wilhelmlab/detectability-proteometools\"\n",
    "detectability_data = DetectabilityDataset(data_source=hf_data,\n",
    "                                          data_format='hub',\n",
    "                                          max_seq_len=max_pep_length,\n",
    "                                          label_column=\"Classes\",\n",
    "                                          sequence_column=\"Sequences\",\n",
    "                                          dataset_columns_to_keep=None,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          with_termini=False,\n",
    "                                          alphabet=aa_to_int_dict,\n",
    "                                          padding_value=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dataset with train, val, and test splits  \n",
    "# You can see the column names under each split (the columns starting with _ are internal, but can also be used to look up original sequences for example \"_parsed_sequence\")\n",
    "detectability_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing elements in the dataset is done by specificing the split name and then the column name\n",
    "# Example here for one sequence after encoding & padding comapred to the original sequence\n",
    "\n",
    "detectability_data[\"train\"][\"Sequences\"][0], \"\".join(detectability_data[\"train\"][\"_parsed_sequence\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWeVi0iar7qT"
   },
   "source": [
    "## 2. Model\n",
    "\n",
    "We can now create the model. The model architecture is an encoder-decoder with an attention mechanism, that is based on Bidirectional Recurrent Neural Network (BRNN) with Gated Recurrent Units (GRU). Both the Encoder and Decoder consists of a single layer, with the Decoder also including a Dense layer. The model has the default working arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8SGTvfRr7qT"
   },
   "outputs": [],
   "source": [
    "from dlomix.models import DetectabilityModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqrsF6APr7qU"
   },
   "outputs": [],
   "source": [
    "total_num_classes = len(CLASSES_LABELS)\n",
    "input_dimension = len(alphabet)\n",
    "num_cells = 64\n",
    "\n",
    "model = DetectabilityModel(num_units=num_cells, num_classes=total_num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adD60VwQr7qU"
   },
   "source": [
    "## 3. Training and saving the model\n",
    "\n",
    "You can train the model using the standard Keras approach. The training parameters provided here are those initially configured for the detectability model. However, you have the flexibility to modify these parameters to suit your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the Model\n",
    "\n",
    "Compile the model with the selected settings. You can use built-in TensorFlow options or define and pass custom settings for the optimizer, loss function, and metrics. The default configurations match those used in the original study, but you can modify these settings according to your preferences.\n",
    "\n",
    "Early stopping is also configured with the original settings, but the parameters can be adjusted based on user preferences. Early stopping monitors a performance metric (e.g., validation loss) and halts training when no improvement is observed for a specified number of epochs. This feature helps prevent overfitting and ensures efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLy32wk7r7qU",
    "outputId": "34f9961e-1abc-4f8f-904c-7aac4a404241"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                            mode = 'min', \n",
    "                                            verbose = 1, \n",
    "                                            patience = 5)\n",
    "\n",
    "\n",
    "model_save_path = 'output/weights/new_base_model/base_model_weights_detectability'\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,\n",
    "                                                      monitor='val_sparse_categorical_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True, \n",
    "                                                      save_weights_only=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='SparseCategoricalCrossentropy', \n",
    "              metrics='sparse_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtEUn_vdr7qV"
   },
   "source": [
    "We save the results of the training process to enable a detailed examination of the metrics and losses at a later stage. We define the number of epochs for training and supply the training and validation data previously generated. This approach allows us to effectively monitor the model’s performance and make any necessary adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E14EcoYTr7qV",
    "outputId": "9c88b2d5-e1cb-46b4-e263-73468e222554",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access to the tensorflow datasets is done by referencing the tensor_train_data or tensor_val_data\n",
    "\n",
    "history = model.fit(detectability_data.tensor_train_data,\n",
    "                    validation_data = detectability_data.tensor_val_data,\n",
    "                    epochs = 50, \n",
    "                    callbacks=[callback, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukZ4AyMr7qV"
   },
   "source": [
    "## 4. Testing and Reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the test dataset to assess our model's performance, which is only applicable if labels are available. The `DetectabilityReport` class allows us to compute various metrics, generate reports, and create plots for a comprehensive evaluation of the model.\n",
    "\n",
    "Note: The reporting module is currently under development, so some features may be unstable or subject to change.\n",
    "\n",
    "In the next cell, set the path to the model weights. By default, it points to the newly trained base model. If using different weights, update the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading best model weights \n",
    "\n",
    "# uncomment to define again if the run was interrupted and you want to load the best model weights from the previous finetuning\n",
    "# else, it should still be in the environment\n",
    "\n",
    "# model_save_path = 'output/weights/new_base_model/base_model_weights_detectability'\n",
    "\n",
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictions on Test Data Using `model.predict`\n",
    "\n",
    "To obtain predictions for your test data, use the Keras `model.predict` method. Simply pass your test dataset to this method, and it will return the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrvR8Cl3r7qV"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(detectability_data.tensor_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reports and calculate evaluation metrics against predictions, we obtain the targets and the data for the specific dataset split. This can be achieved using the `DetectabilityDataset` class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access val dataset and get the Classes column\n",
    "test_targets = detectability_data[\"test\"][\"Classes\"]\n",
    "\n",
    "\n",
    "# if needed, the decoded version of the classes can be retrieved by looking up the class names\n",
    "test_targets_decoded = [CLASSES_LABELS[x] for x in test_targets]\n",
    "\n",
    "\n",
    "test_targets[0:5], test_targets_decoded[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe needed for the report\n",
    "\n",
    "test_data_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Sequences\": detectability_data[\"test\"][\"_parsed_sequence\"], # get the raw parsed sequences\n",
    "        \"Classes\": test_targets, # get the test targets from above\n",
    "#         \"Proteins\": detectability_data[\"test\"][\"Proteins\"] # get the Proteins column from the dataset object (if the dataset has \"Proteins\" column)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_data_df.Sequences = test_data_df.Sequences.apply(lambda x: \"\".join(x)) # join the sequences since they are a list of string amino acids.\n",
    "test_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kzCh0gwr7qX"
   },
   "outputs": [],
   "source": [
    "from dlomix.reports.DetectabilityReport import DetectabilityReport, predictions_report\n",
    "WANDB_REPORT_API_DISABLE_MESSAGE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Report Using the `DetectabilityReport` Class\n",
    "\n",
    "The `DetectabilityReport` class provides a comprehensive way to evaluate your model by generating detailed reports and visualizations. The outputs include:\n",
    "\n",
    "1. **A PDF Report**: This includes evaluation metrics and plots.\n",
    "2. **A CSV File**: Contains the model’s predictions.\n",
    "3. **Independent Image Files**: Visualizations are saved as separate image files.\n",
    "\n",
    "To generate a report, provide the following parameters to the `DetectabilityReport` class:\n",
    "\n",
    "- **targets**: The true labels for the dataset, which are used to assess the model’s performance.\n",
    "- **predictions**: The model’s output predictions for the dataset, which will be compared against the true labels.\n",
    "- **input_data_df**: The DataFrame containing the input data used for generating predictions.\n",
    "- **output_path**: The directory path where the generated reports, images, and CSV file will be saved.\n",
    "- **history**: The training history object (e.g., containing metrics from training) if available. Set this to `None` if not applicable, such as when the report is generated for predictions without training.\n",
    "- **rank_by_prot**: A boolean indicating whether to rank peptides based on their associated proteins (`True` or `False`). Defaults to `False`.\n",
    "- **threshold**: The classification threshold used to adjust the decision boundary for predictions. By default, this is set to `None`, meaning no specific threshold is applied.\n",
    "- **name_of_dataset**: The name of the dataset used for generating predictions, which will be included in the report to provide context.\n",
    "- **name_of_model**: The name of the model used to generate the predictions, which will be specified in the report for reference.\n",
    "\n",
    "Note: The reporting module is currently under development, so some features may be unstable or subject to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the detectabiliy report expects the true labels in one-hot encoded format, we expand them here.\n",
    "\n",
    "num_classes = np.max(test_targets) + 1\n",
    "test_targets_one_hot = np.eye(num_classes)[test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7LJZ3TLr7qX"
   },
   "outputs": [],
   "source": [
    "report = DetectabilityReport(targets = test_targets_one_hot, \n",
    "                             predictions = predictions, \n",
    "                             input_data_df = test_data_df,\n",
    "                             output_path = \"./output/report_on_ProteomeTools\",\n",
    "                             history = history, \n",
    "                             rank_by_prot = False,\n",
    "                             threshold = None,\n",
    "                             name_of_dataset = 'ProteomeTools',\n",
    "                             name_of_model = 'Base model (new)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = report.detectability_report_table\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Evaluation Plots with `DetectabilityReport`\n",
    "\n",
    "The `DetectabilityReport` class enables you to generate a range of plots to visualize and evaluate model performance. It offers a comprehensive suite of visualizations to help you interpret the results of your model's predictions. Here’s how to use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Validation Metrics\n",
    "\n",
    "These plots show the training and validation metrics over epochs. The first plot displays the loss, and the second shows the categorical accuracy. Both plots are generated from the `history` object recorded during the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "1iI-_Nufr7qX",
    "outputId": "25baa9f5-1d5b-47ed-d75a-def6a55e43bc"
   },
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"sparse_categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_roc_curve_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_confusion_matrix_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_confusion_matrix_multiclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Average Error Between Actual and Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_heatmap_prediction_prob_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce a complete report with all the relevant plots in one PDF file by calling the `generate_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Defining a Classification Threshold\n",
    "\n",
    "In the following example, a specific classification threshold is defined to adjust the decision boundary for the model's predictions. By setting a threshold, you can control the sensitivity of the model, influencing how it categorizes the output into different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_using_threshold = DetectabilityReport(test_targets_one_hot, \n",
    "                                             predictions, \n",
    "                                             test_data_df, \n",
    "                                             output_path = \"./output/report_on_ProteomeTools_with_threshold\", \n",
    "                                             history = history, \n",
    "                                             rank_by_prot = False,\n",
    "                                             threshold = 0.5,                              \n",
    "                                             name_of_dataset = 'ProteomeTools',\n",
    "                                             name_of_model = 'Base model (new) with threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_using_threshold.detectability_report_table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a complete PDF report using the `generate_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_using_threshold.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load data for fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning, the process mirrors the steps used during training. Simply create a `DetectabilityDataset` object with the fine-tuning data (refer to **Section 1: Load Data for Training**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pep_length = 40\n",
    "BATCH_SIZE = 128 \n",
    "            \n",
    "# The Class handles all the inner details, we have to provide the column names and the alphabet for encoding\n",
    "# If the data is already split with a specific logic (which is generally recommended) -> val_data_source and test_data_source are available as well\n",
    "\n",
    "hf_data = \"Wilhelmlab/detectability-sinitcyn\"\n",
    "fine_tune_data = DetectabilityDataset(data_source=hf_data,\n",
    "                                      data_format='hub',\n",
    "                                      max_seq_len=max_pep_length,\n",
    "                                      label_column=\"Classes\",\n",
    "                                      sequence_column=\"Sequences\",\n",
    "                                      dataset_columns_to_keep=['Proteins'],\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      with_termini=False,\n",
    "                                      alphabet=aa_to_int_dict,\n",
    "                                      padding_value=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine tuning the model\n",
    "\n",
    "In the next cell, we create the model and load its weights for fine-tuning. By default, the path is set to the weights of the most recently trained base model. To use different weights, update the path to point to your desired model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define again if not in environment from training run\n",
    "load_model_path = model_save_path #'output/weights/new_base_model/base_model_weights_detectability'\n",
    "\n",
    "fine_tuned_model = DetectabilityModel(num_units=num_cells,  \n",
    "                                      num_classes=total_num_classes)\n",
    "\n",
    "fine_tuned_model.load_weights(load_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the Model\n",
    "\n",
    "Compile the model with the selected settings. You can use built-in TensorFlow options or define and pass custom settings for the optimizer, loss function, and metrics. The default configurations match those used in the original study, but you can modify these settings according to your preferences.Early stopping is also configured with the original settings, but the parameters can be adjusted based on user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model  with the optimizer and the metrics we want to use.\n",
    "callback_FT = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                               mode = 'min', \n",
    "                                               verbose = 1, \n",
    "                                               patience = 5)\n",
    "\n",
    "\n",
    "model_save_path_FT = 'output/weights/new_fine_tuned_model/fine_tuned_model_weights_detectability'\n",
    "\n",
    "model_checkpoint_FT = tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path_FT,\n",
    "                                                         monitor='val_sparse_categorical_accuracy', \n",
    "                                                         mode='max',\n",
    "                                                         verbose=1,\n",
    "                                                         save_best_only=True, \n",
    "                                                         save_weights_only=True)\n",
    "\n",
    "fine_tuned_model.compile(optimizer='adam',\n",
    "                         loss='SparseCategoricalCrossentropy', \n",
    "                         metrics='sparse_categorical_accuracy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the result of training so that we can explore the metrics and the losses later. We specify the number of epochs for training and pass the training and validation data as previously described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fine_tuned = fine_tuned_model.fit(fine_tune_data.tensor_train_data,\n",
    "                                          validation_data=fine_tune_data.tensor_val_data,\n",
    "                                          epochs=50, \n",
    "                                          callbacks=[callback_FT, model_checkpoint_FT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing and Reporting (Fine-Tuned Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we load the best model weights obtained from fine-tuning. By default, the path points to the most recently fine-tuned model from the previous cell. Update the path if you wish to load different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading best model weights \n",
    "\n",
    "# uncomment to define again if the run was interrupted and you want to load the best model weights from the previous finetuning\n",
    "# else, it should still be in the environment\n",
    "\n",
    "#model_save_path_FT = 'output/weights/new_fine_tuned_model/fine_tuned_model_weights_detectability'\n",
    "\n",
    "fine_tuned_model.load_weights(model_save_path_FT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating predictions on the test data using the fine-tuned model with `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_FT = fine_tuned_model.predict(fine_tune_data.tensor_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_FT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reports and calculate evaluation metrics against predictions, we obtain the targets and the data for the specific dataset split. This can be achieved using the DetectabilityDataset class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access val dataset and get the Classes column\n",
    "test_targets_FT = fine_tune_data[\"test\"][\"Classes\"]\n",
    "\n",
    "\n",
    "# if needed, the decoded version of the classes can be retrieved by looking up the class names\n",
    "test_targets_decoded_FT = [CLASSES_LABELS[x] for x in test_targets_FT]\n",
    "\n",
    "\n",
    "test_targets_FT[0:5], test_targets_decoded_FT[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe needed for the report\n",
    "\n",
    "test_data_df_FT = pd.DataFrame(\n",
    "    {\n",
    "        \"Sequences\": fine_tune_data[\"test\"][\"_parsed_sequence\"], # get the raw parsed sequences\n",
    "        \"Classes\": test_targets_FT, # get the test targets from above\n",
    "        \"Proteins\": fine_tune_data[\"test\"][\"Proteins\"] # get the Proteins column from the dataset object\n",
    "    }\n",
    ")\n",
    "\n",
    "test_data_df_FT.Sequences = test_data_df_FT.Sequences.apply(lambda x: \"\".join(x)) # join the sequences since they are a list of string amino acids.\n",
    "test_data_df_FT.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a report object with the test targets, predictions, and history to generate metrics and plots for the fine-tuned model. For more details, refer to Section 4: Testing and Reporting, which provides a detailed description of the same process for the initial or base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the detectabiliy report expects the true labels in one-hot encoded format, we expand them here. \n",
    "\n",
    "num_classes = np.max(test_targets_FT) + 1\n",
    "test_targets_FT_one_hot = np.eye(num_classes)[test_targets_FT]\n",
    "test_targets_FT_one_hot.shape, len(test_targets_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT = DetectabilityReport(test_targets_FT_one_hot, \n",
    "                                predictions_FT, \n",
    "                                test_data_df_FT, \n",
    "                                output_path = './output/report_on_Sinitcyn (Fine-tuned model)', \n",
    "                                history = history_fine_tuned, \n",
    "                                rank_by_prot = True,\n",
    "                                threshold = None,                              \n",
    "                                name_of_dataset = 'Sinitcyn test dataset',\n",
    "                                name_of_model = 'Fine tuned model (new)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions report (Fine-tuned model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_FT = report_FT.detectability_report_table\n",
    "results_df_FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a complete PDF report using the `generate_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the Evaluation Plots for the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Validation Metrics\n",
    "\n",
    "These plots show the training and validation metrics over epochs. The first plot displays the loss, and the second shows the categorical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_keras_metric(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_keras_metric(\"sparse_categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_roc_curve_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_confusion_matrix_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_confusion_matrix_multiclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Average Error Between Actual and Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_FT.plot_heatmap_prediction_prob_error()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Example_RTModel_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YWkUVjVr7qJ"
   },
   "source": [
    "# Peptide Detectability Prediction \n",
    "\n",
    "This notebook is prepared to be run in Google [Colaboratory](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3DlTOq3r7qM"
   },
   "source": [
    "One of the example datasets used in this notebook is deposited in the ProteomeXchange Consortium via the MAssIVE partner repository with the identifier PXD024364. The other dataset is deposited to the ProteomeXchange Consortium via the PRIDE partner repository with identifier PXD010154. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing the DLOmix Package\n",
    "\n",
    "If you have not installed the DLOmix package yet, you need to do so before running the code. \n",
    "\n",
    "You can install the DLOmix package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-69zbKsGey",
    "outputId": "c2064411-9f80-47e6-ca5b-312d547e0f6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment the following line to install the DLOmix package in the current environment using pip\n",
    "\n",
    "#!python -m pip install dlomix>0.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo7H9qzWr7qN"
   },
   "source": [
    "#### Importing Required Libraries\n",
    "\n",
    "Before running the code, ensure you import all the necessary libraries. These imports are essential for accessing the functionalities needed for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0CS0tFur7qN",
    "outputId": "664e0978-980a-4254-90d1-61e9f1603234"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import dlomix\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWeVi0iar7qT"
   },
   "source": [
    "## Model\n",
    "\n",
    "We can now create the model. The model architecture is an encoder-decoder with an attention mechanism, that is based on Bidirectional Recurrent Neural Network (BRNN) with Gated Recurrent Units (GRU). Both the Encoder and Decoder consists of a single layer, with the Decoder also including a Dense layer. The model has the default working arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8SGTvfRr7qT"
   },
   "outputs": [],
   "source": [
    "from dlomix.models import DetectabilityModel\n",
    "from dlomix.constants import CLASSES_LABELS, alphabet, aa_to_int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LABELS, len(alphabet), aa_to_int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqrsF6APr7qU"
   },
   "outputs": [],
   "source": [
    "total_num_classes = len(CLASSES_LABELS)\n",
    "input_dimension = len(alphabet)\n",
    "num_cells = 64\n",
    "\n",
    "model = DetectabilityModel(num_units = num_cells, num_clases = total_num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Weights Configuration\n",
    "\n",
    "In the following section, you need to specify the path to the model weights you wish to use. The default path provided is set to the weights for the **Pfly** model, which is the fine-tuned model mentioned in the publication associated with this notebook.\n",
    "\n",
    "- **Using the Default Pfly Model**: If you are utilizing the fine-tuned Pfly model as described in the publication, you can keep the default path unchanged. This will load the model weights for Pfly.\n",
    "\n",
    "- **Using the Base Model or Different Weights**: If you intend to use the base model or have different weights (e.g., for a custom model), you should update the path to reflect the location of these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading model weights \n",
    "\n",
    "model_save_path = 'output/weights/new_fine_tuned_model/fine_tuned_model_weights_detectability'\n",
    "\n",
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "This notebook supports two different workflows depending on your dataset:\n",
    "\n",
    "- **Labeled Data**: Use this pipeline when your dataset includes ground truth labels. This setup not only makes predictions but also allows for detailed evaluation by comparing the true labels with the predicted values, facilitating the generation of a comprehensive evaluation report.\n",
    "\n",
    "- **Unlabeled Data**: Use this pipeline when your dataset does not include labels. Here, the focus is on making predictions only, without generating a detailed performance report, as there are no labels to compare against.\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "Subtitles throughout the notebook indicate the sections for each type of data:\n",
    "\n",
    "- **Labeled Data Section**: Follow these when your dataset includes labels to receive predictions and a comprehensive evaluation report.\n",
    "\n",
    "- **Unlabeled Data Section**: Use these when your dataset lacks labels, focusing solely on generating predictions.\n",
    "\n",
    "Make sure to select the appropriate pipeline based on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41qXroyKr7qP"
   },
   "source": [
    "## 1. Load Data \n",
    "\n",
    "You can import the `DetectabilityDataset` class and create an instance to manage data for training, validation, and testing. This instance handles TensorFlow dataset objects and simplifies configuring and controlling how your data is preprocessed and split.\n",
    "\n",
    "For the paramters of the dataset class, please refer to the DLOmix documentation: https://dlomix.readthedocs.io/en/main/dlomix.data.html#\n",
    "\n",
    "\n",
    "**Note**: If class labels are provided, the following encoding scheme should be used:\n",
    "- **Non-Flyer**: 0\n",
    "- **Weak Flyer**: 1\n",
    "- **Intermediate Flyer**: 2\n",
    "- **Strong Flyer**: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiXz_epEr7qQ"
   },
   "outputs": [],
   "source": [
    "from dlomix.data import DetectabilityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from huggingface for prediction\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# pick one of the available datasets on the HuggingFace Hub\n",
    "# Collection: https://huggingface.co/collections/Wilhelmlab/detectability-datasets-671e76fb77035878c50a9c1d\n",
    "\n",
    "hf_data_name = \"Wilhelmlab/detectability-sinitcyn\"\n",
    "#hf_data_name = \"Wilhelmlab/detectability-wang\"\n",
    "\n",
    "hf_dataset_split = load_dataset(hf_data_name, split=\"test\")\n",
    "hf_dataset = DatasetDict({\"test\": hf_dataset_split})\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_pep_length = 40\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "detectability_data = DetectabilityDataset(data_source=hf_dataset,\n",
    "                                          data_format='hf',\n",
    "                                          max_seq_len=max_pep_length,\n",
    "                                          label_column=\"Classes\",\n",
    "                                          sequence_column=\"Sequences\",\n",
    "                                          dataset_columns_to_keep=['Proteins'],\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          with_termini=False,\n",
    "                                          alphabet=aa_to_int_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzNXJ-s6r7qQ"
   },
   "outputs": [],
   "source": [
    "# This is the dataset with the test split  \n",
    "# You can see the column names under each split (the columns starting with _ are internal, but can also be used to look up original sequences for example \"_parsed_sequence\")\n",
    "detectability_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing elements in the dataset is done by specificing the split name and then the column name\n",
    "# Example here for one sequence after encoding & padding comapred to the original sequence\n",
    "\n",
    "detectability_data[\"test\"][\"Sequences\"][0], \"\".join(detectability_data[\"test\"][\"_parsed_sequence\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukZ4AyMr7qV"
   },
   "source": [
    "## 2. Testing and Reporting\n",
    "\n",
    "We use the test dataset to assess our model's performance, which is only applicable if labels are available. The `DetectabilityReport` class allows us to compute various metrics, generate reports, and create plots for a comprehensive evaluation of the model.\n",
    "\n",
    "Note: The reporting module is currently under development, so some features may be unstable or subject to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Predictions on Test Data Using `model.predict`\n",
    "\n",
    "To obtain predictions for your test data, use the Keras `model.predict` method. Simply pass your test dataset to this method, and it will return the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrvR8Cl3r7qV"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(detectability_data.tensor_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reports and calculate evaluation metrics against predictions, we obtain the targets and the data for the specific dataset split. This can be achieved using the `DetectabilityDataset` class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKk7MD7Wr7qW"
   },
   "outputs": [],
   "source": [
    "# access val dataset and get the Classes column\n",
    "test_targets = detectability_data[\"test\"][\"Classes\"]\n",
    "\n",
    "\n",
    "# if needed, the decoded version of the classes can be retrieved by looking up the class names\n",
    "test_targets_decoded = [CLASSES_LABELS[x] for x in test_targets]\n",
    "\n",
    "\n",
    "test_targets[0:5], test_targets_decoded[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe needed for the report\n",
    "\n",
    "test_data_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Sequences\": detectability_data[\"test\"][\"_parsed_sequence\"], # get the raw parsed sequences\n",
    "        \"Classes\": test_targets, # get the test targets from above\n",
    "        \"Proteins\": detectability_data[\"test\"][\"Proteins\"] # get the Proteins column from the dataset object\n",
    "    }\n",
    ")\n",
    "\n",
    "test_data_df.Sequences = test_data_df.Sequences.apply(lambda x: \"\".join(x)) # join the sequences since they are a list of string amino acids.\n",
    "test_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kzCh0gwr7qX"
   },
   "outputs": [],
   "source": [
    "from dlomix.reports.DetectabilityReport import DetectabilityReport, predictions_report\n",
    "WANDB_REPORT_API_DISABLE_MESSAGE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Report Using the `DetectabilityReport` Class\n",
    "\n",
    "The `DetectabilityReport` class provides a comprehensive way to evaluate your model by generating detailed reports and visualizations. The outputs include:\n",
    "\n",
    "1. **A PDF Report**: This includes evaluation metrics and plots.\n",
    "2. **A CSV File**: Contains the model’s predictions.\n",
    "3. **Independent Image Files**: Visualizations are saved as separate image files.\n",
    "\n",
    "To generate a report, provide the following parameters to the `DetectabilityReport` class:\n",
    "\n",
    "- **targets**: The true labels for the dataset, which are used to assess the model’s performance.\n",
    "- **predictions**: The model’s output predictions for the dataset, which will be compared against the true labels.\n",
    "- **input_data_df**: The DataFrame containing the input data used for generating predictions.\n",
    "- **output_path**: The directory path where the generated reports, images, and CSV file will be saved.\n",
    "- **history**: The training history object (e.g., containing metrics from training) if available. Set this to `None` if not applicable, such as when the report is generated for predictions without training.\n",
    "- **rank_by_prot**: A boolean indicating whether to rank peptides based on their associated proteins (`True` or `False`). Defaults to `False`.\n",
    "- **threshold**: The classification threshold used to adjust the decision boundary for predictions. By default, this is set to `None`, meaning no specific threshold is applied.\n",
    "- **name_of_dataset**: The name of the dataset used for generating predictions, which will be included in the report to provide context.\n",
    "- **name_of_model**: The name of the model used to generate the predictions, which will be specified in the report for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the detectabiliy report expects the true labels in one-hot encoded format, we expand them here.\n",
    "\n",
    "num_classes = np.max(test_targets) + 1\n",
    "test_targets_one_hot = np.eye(num_classes)[test_targets]\n",
    "test_targets_one_hot.shape, len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7LJZ3TLr7qX"
   },
   "outputs": [],
   "source": [
    "report = DetectabilityReport(targets = test_targets_one_hot, \n",
    "                             predictions = predictions, \n",
    "                             input_data_df = test_data_df, \n",
    "                             output_path = \"./output/report_on_Sinitcyn_2000_proteins_test_set_labeled\", \n",
    "                             history = None, \n",
    "                             rank_by_prot = True,\n",
    "                             threshold = None,\n",
    "                             name_of_dataset = 'Sinitcyn 2000 proteins test set',\n",
    "                             name_of_model = 'Fine-tuned model (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = report.detectability_report_table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Evaluation Plots with `DetectabilityReport`\n",
    "\n",
    "The `DetectabilityReport` class enables you to generate a range of plots to visualize and evaluate model performance. It offers a comprehensive suite of visualizations to help you interpret the results of your model's predictions. Here’s how to use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_roc_curve_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_confusion_matrix_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_confusion_matrix_multiclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Average Error Between Actual and Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_heatmap_prediction_prob_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce a complete evaluation report with all the relevant plots in one PDF file by calling the `generate_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting on unlabeled data, follow the same workflow as described earlier (refer to the \"Load Data\" section for labeled data). Specifically, create an instance of the `DetectabilityDataset` class using your unlabeled data.The configuration below ensures that the entire dataset is treated as test data without generating additional splits (i.e., training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from huggingface for prediction\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# pick one of the available datasets on the HuggingFace Hub\n",
    "# Collection: https://huggingface.co/collections/Wilhelmlab/detectability-datasets-671e76fb77035878c50a9c1d\n",
    "\n",
    "hf_data_name = \"Wilhelmlab/detectability-sinitcyn\"\n",
    "#hf_data_name = \"Wilhelmlab/detectability-wang\"\n",
    "\n",
    "hf_dataset_split = load_dataset(hf_data_name, split=\"test\")\n",
    "hf_dataset = DatasetDict({\"test\": hf_dataset_split})\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate that the class labels are not there (insert None), but we keep the column since it is needed for the dataset class\n",
    "\n",
    "hf_dataset = hf_dataset.map(lambda example: {**example, 'Classes': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_pep_length = 40\n",
    "BATCH_SIZE = 128\n",
    "       \n",
    "test_data_unlabeled = DetectabilityDataset(data_source=hf_dataset,\n",
    "                                           data_format='hf',\n",
    "                                           max_seq_len=max_pep_length,\n",
    "                                           label_column='Classes',\n",
    "                                           sequence_column=\"Sequences\",\n",
    "                                           dataset_columns_to_keep=['Proteins'],\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           with_termini=False,\n",
    "                                           alphabet=aa_to_int_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_unlabeled[\"test\"][\"Classes\"][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1uYK1ZWr7qZ"
   },
   "source": [
    "## 2. Predicting and reporting\n",
    "\n",
    "We use the previously loaded model to generate predictions on the dataset. If labels are not available, you can utilize the `predictions_report` function to produce a clear and organized report based on these predictions. Note that the `predictions_report` function is specifically designed for scenarios where labels are not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Predictions on Test Data Using `model.predict`\n",
    "\n",
    "To obtain predictions for your test data, use the Keras `model.predict` method. Simply pass your test dataset to this method, and it will return the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unlabeled = model.predict(test_data_unlabeled.tensor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reports we obtain the data for the specific dataset split. This can be achieved using the `DetectabilityDataset` class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe needed for the report\n",
    "\n",
    "test_data_unlabeled_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Sequences\": test_data_unlabeled[\"test\"][\"_parsed_sequence\"], # get the raw parsed sequences\n",
    "        \"Proteins\": test_data_unlabeled[\"test\"][\"Proteins\"] # get the Proteins column from the dataset object\n",
    "    }\n",
    ")\n",
    "\n",
    "test_data_unlabeled_df.Sequences = test_data_unlabeled_df.Sequences.apply(lambda x: \"\".join(x)) # join the sequences since they are a list of string amino acids.\n",
    "test_data_unlabeled_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a report using the `predictions_report` class by providing the following parameters:\n",
    "\n",
    "- **predictions**: The model's output predictions for the dataset.\n",
    "- **input_data_df**: The DataFrame containing the input data used for generating the predictions.\n",
    "- **output_path**: The path where the generated report (in CSV format) will be saved.\n",
    "- **rank_by_prot**: A boolean indicating whether to rank peptides based on their associated proteins (`True` or `False`). Defaults to `False`.\n",
    "- **threshold**: The classification threshold used to adjust the decision boundary for predictions. By default, this is set to `None`, meaning no specific threshold is applied.\n",
    "\n",
    "The `predictions_report` class processes the model’s predictions and generates a comprehensive CSV report with the results, including any specified settings, which facilitates evaluation and interpretation of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_report = predictions_report(predictions = predictions_unlabeled, \n",
    "                                            input_data_df = test_data_unlabeled_df, \n",
    "                                            output_path = \"./output/report_on_Sinitcyn_2000_proteins_test_set_unlabeled\", \n",
    "                                            rank_by_prot = True,\n",
    "                                            threshold = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unlabeled_df = new_predictions_report.predictions_report\n",
    "results_unlabeled_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Example_RTModel_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

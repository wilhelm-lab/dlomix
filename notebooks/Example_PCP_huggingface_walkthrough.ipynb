{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faaee3a",
   "metadata": {},
   "source": [
    "# Precursor Charge State Prediction\n",
    "\n",
    "This notebook presents a short walkthrough the process of reading a dataset and training a model for precursor charge state prediction. The dataset is an example dataset extracted from a ProteomTools dataset generated in the **Chair of Bioanalytics** at the **School of Life Sciences** at the **Technical University of Munich**.\n",
    "\n",
    "DLOmix is the framework being used and is a custom wrapper on top of Keras/TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d989c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the DLOmix package in the current environment using pip\n",
    "\n",
    "!python -m pip install -q dlomix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlomix\n",
    "dlomix.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad34f9",
   "metadata": {},
   "source": [
    "The available modules in the framework are as follows:\n",
    "\n",
    "- `constants`: constants to be used in the framework (e.g. Aminoacid alphabet mapping)\n",
    "- `data`:  classes for representing dataset, wrappers around HuggingFace datasets to process input data and generate tensor datasets\n",
    "- `eval`: custom evaluation metrics implemented in Keras/TF to work as `metrics` for model training\n",
    "- `layers`: custom layer implementation required for the different models\n",
    "- `models`: different model implementations for Retention Time Prediction\n",
    "- `pipelines`: complete pipelines to run a task (e.g. Retention Time prediction)\n",
    "\n",
    "**Note**: reports and pipelines are work-in-progress, some funtionalities are not complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix import constants, data, eval, layers, models, pipelines, reports\n",
    "print([x for x in dir(dlomix) if not x.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86656f2e",
   "metadata": {},
   "source": [
    "## Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875394b-3f3d-4179-b2ad-004ac33901c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "from dlomix.data import ChargeStateDataset\n",
    "from dlomix.eval import adjusted_mean_absolute_error\n",
    "from dlomix.models import ChargeStatePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a5cd4",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We can import the dataset class and create an object of type `ChargeStateDataset`. This object wraps around a Hugging Face dataset that can generate TensorFlow Dataset objects or Torch Dataset for training, validation, or testing. This can be controlled by the arguments `val_ratio`, `val_data_source`, and `test_data_source`.\n",
    "\n",
    "The most important columns of the charge state dataset are:\n",
    "* \"modified_sequence\", representing the peptide sequences, modifications are annotated using the UNIMOD encoding.\n",
    "* \"charge_state_dist\", representing the relative charge state distribution per peptide. It is to be used together with the model_flavour=\"relative\", which is the default.\n",
    "* \"most_abundant_charge_state\", representing the most abundant charge state (as binary vector) per peptide. It is to be used together with the model_flavour=\"dominant\".\n",
    "* \"observed_charge_states\", representing all observed charge states (as binary vector) per peptide. It is to be used together with the model_flavour=\"observed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef81bd4-cbb7-45c8-ac3b-e9b2f8b597f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Wilhelmlab/prospect-ptms-charge\"   # complete PROSPECT dataset prepared for charge state prediction\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f43fa-5084-49d3-98f7-a7cfc167ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ChargeStateDataset(\n",
    "    data_format=\"hub\",\n",
    "    data_source=DATA_PATH,\n",
    "    sequence_column=\"modified_sequence\",\n",
    "    label_column=\"charge_state_dist\",   # use this column for relative charge state distribution\n",
    "    max_seq_len=30,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc837dcb",
   "metadata": {},
   "source": [
    "Now we have an CS dataset that can be used directly with standard or custom `Keras` models. This wrapper contains the splits we chose when creating it. In our case, they are training and validation splits. To get the TF Dataset, we call the attributes `.tensor_rain_data` and `.tensor_val_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86208de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hugging Face Dataset:\", d)\n",
    "\n",
    "print(\"Training examples:\", len(d[\"train\"]))\n",
    "print(\"One training batch looks like:\")\n",
    "for x in d.tensor_train_data:\n",
    "    print(x)\n",
    "    break\n",
    "\n",
    "print(\"Validation examples:\", len(d[\"val\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ed4e7",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "We can now create the model. We will use the relative charge state distribution version (set via the parameter `model_flavour=\"relative\"`) of the Prosit-based Precursor Charge State Prediction model `ChargeStatePredictor`. It has default working arguments, but most of the parameters can be customized.\n",
    "\n",
    "**Note**: Important is to ensure that the padding length used for the dataset object is equal to the sequence length passed to the model.\n",
    "\n",
    "The three model flavours of `ChargeStatePredictor` are:\n",
    "\n",
    "1. Dominant Charge State Prediction:\n",
    "   - Task: Predict the dominant charge state of a given peptide sequence.\n",
    "   - Model: Uses a deep learning model (RNN-based) inspired by Prosit's architecture to predict the most likely charge state.\n",
    "\n",
    "2. Observed Charge State Prediction:\n",
    "   - Task: Predict the observed charge states for a given peptide sequence.\n",
    "   - Model: Uses a multi-label classification approach to predict all possible charge states.\n",
    "\n",
    "3. Relative Charge State Prediction:\n",
    "   - Task: Predict the proportion of each charge state for a given peptide sequence.\n",
    "   - Model: Uses a regression approach to predict the proportion of each charge state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef3817-6f98-4e1d-86e3-479c8c8252ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChargeStatePredictor(\n",
    "    num_classes=6, seq_length=30, alphabet=PTMS_ALPHABET, model_flavour=\"relative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9149df",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "We can then train the model like a standard Keras model. You can observe the decreasing loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[adjusted_mean_absolute_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50afb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    d.tensor_train_data, \n",
    "    validation_data=d.tensor_val_data,\n",
    "    epochs=1,  # reduced for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef5a2f",
   "metadata": {},
   "source": [
    "### Train History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, title='Learning Curves'):\n",
    "    history_dict = history.history\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict.get('val_loss', [])\n",
    "    \n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, title='Charge State Distribution Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb70557",
   "metadata": {},
   "source": [
    "## 3. Testing\n",
    "\n",
    "The ChargeStateDataset also contains a test dataset to test our model.\n",
    "\n",
    "**Note**: Currently there is no reporting module available for CS prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fe0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = d[\"test\"][\"charge_state_dist\"]\n",
    "test_sequences = d[\"test\"][\"modified_sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02983f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_sequences)\n",
    "print(test_sequences[:5])\n",
    "print(test_targets[:5])\n",
    "print(predictions[:5])\n",
    "print(predictions.shape, len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa5a89",
   "metadata": {},
   "source": [
    "## 4. Saving and Loading Models\n",
    "\n",
    "Models can be saved normally the same Keras models would be saved. It is better to save the weights and the not the model since it makes it easier and more platform-indepdent when loading the model again. The extra step needed is to create a model object and then load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weights\n",
    "\n",
    "save_path = \"./output/csd_model\"\n",
    "model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40757b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models can be later loaded by creating a model object and then loading the weights\n",
    "\n",
    "trained_model = ChargeStatePredictor(seq_length=30)\n",
    "trained_model.load_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b289c14-3bf2-444c-8153-abca5a307f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the loaded model\n",
    "loaded_model_predictions = trained_model.predict(test_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

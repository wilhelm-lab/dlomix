{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = './config_files/baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = config['dataset']['hf_home']\n",
    "os.environ['HF_DATASETS_CACHE'] = config['dataset']['hf_cache']\n",
    "\n",
    "os.environ['XLA_FLAGS'] ='--xla_gpu_cuda_data_dir=/usr/lib/cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 14:55:12.802275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-31 14:55:12.802310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-31 14:55:12.803951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-31 14:55:12.812529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 14:55:14.157853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/j.poschenrieder/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(config['dataset']['processed_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 14:55:16.666191: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'modified_sequence': <tf.Tensor: shape=(1024, 30), dtype=int64, numpy=\n",
      "array([[21,  6, 14, ...,  0,  0,  0],\n",
      "       [21,  6,  5, ...,  0,  0,  0],\n",
      "       [21, 18,  7, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [21,  6, 13, ...,  0,  0,  0],\n",
      "       [21, 17, 12, ...,  0,  0,  0],\n",
      "       [21,  8, 13, ...,  0,  0,  0]])>, 'precursor_charge_onehot': <tf.Tensor: shape=(1024, 6), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.]], dtype=float32)>, 'collision_energy_aligned_normed': <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
      "array([0.21443671, 0.3644403 , 0.35      , ..., 0.25060007, 0.21204613,\n",
      "       0.35      ], dtype=float32)>, 'method_nbr': <tf.Tensor: shape=(1024,), dtype=float32, numpy=array([2., 2., 1., ..., 2., 2., 1.], dtype=float32)>}, <tf.Tensor: shape=(1024, 174), dtype=float32, numpy=\n",
      "array([[ 0.01      ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.24      ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       ...,\n",
      "       [ 0.15151516,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.03      ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.tensor_val_data:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modified_sequence': array([[21, 16,  8, ...,  0,  0,  0],\n",
      "       [56,  1,  1, ...,  0,  0,  0],\n",
      "       [21, 15, 10, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [21,  4, 17, ...,  0,  0,  0],\n",
      "       [21,  8, 10, ...,  0,  0,  0],\n",
      "       [56,  5, 16, ...,  0,  0,  0]]), 'precursor_charge_onehot': array([[0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.]], dtype=float32), 'collision_energy_aligned_normed': array([0.3706906 , 0.25769696, 0.22851454, 0.23789184, 0.35      ,\n",
      "       0.51638985, 0.23172608, 0.40745035, 0.35      , 0.26870653,\n",
      "       0.37040877, 0.2821617 , 0.38349447, 0.21373801, 0.35      ,\n",
      "       0.32424766, 0.37311274, 0.31784788, 0.21831833, 0.23825108,\n",
      "       0.24516802, 0.2479425 , 0.3194331 , 0.3254857 , 0.35      ,\n",
      "       0.35      , 0.38135216, 0.35      , 0.3894306 , 0.35      ,\n",
      "       0.30430368, 0.35      , 0.42888635, 0.3368805 , 0.35      ,\n",
      "       0.30622974, 0.24104159, 0.35      , 0.2758781 , 0.31246135,\n",
      "       0.2735104 , 0.26537815, 0.35      , 0.23756039, 0.31036693,\n",
      "       0.35      , 0.32005653, 0.21660109, 0.42136973, 0.38403454,\n",
      "       0.22318801, 0.3473417 , 0.35      , 0.35      , 0.35      ,\n",
      "       0.37408793, 0.25506374, 0.3825067 , 0.35      , 0.35      ,\n",
      "       0.27426237, 0.29113454, 0.27473605, 0.31369257, 0.35      ,\n",
      "       0.3773698 , 0.35      , 0.2212173 , 0.35      , 0.32581308,\n",
      "       0.35      , 0.20388344, 0.25625405, 0.35      , 0.3134453 ,\n",
      "       0.24097718, 0.32722306, 0.31421295, 0.36129043, 0.26746908,\n",
      "       0.32856214, 0.37487698, 0.2412544 , 0.31779668, 0.29329678,\n",
      "       0.35      , 0.3776473 , 0.21673304, 0.34275073, 0.4176063 ,\n",
      "       0.31101933, 0.26454833, 0.5117347 , 0.30125037, 0.26362434,\n",
      "       0.445009  , 0.25627494, 0.35      , 0.3600429 , 0.419642  ,\n",
      "       0.32313815, 0.3308916 , 0.37689555, 0.32626745, 0.35      ,\n",
      "       0.316617  , 0.35      , 0.2661673 , 0.35      , 0.25938144,\n",
      "       0.24309808, 0.32934365, 0.24806099, 0.45864186, 0.4838949 ,\n",
      "       0.21630299, 0.20964974, 0.35      , 0.3167425 , 0.50618243,\n",
      "       0.31892282, 0.35      , 0.3146293 , 0.35      , 0.37549967,\n",
      "       0.35      , 0.26170212, 0.4235525 , 0.46282658, 0.35      ,\n",
      "       0.41731206, 0.35      , 0.4191277 , 0.21999091, 0.3176371 ,\n",
      "       0.35      , 0.2586336 , 0.26500043, 0.38646597, 0.32376924,\n",
      "       0.25506395, 0.35      , 0.21895325, 0.21828705, 0.31511518,\n",
      "       0.26001903, 0.3177974 , 0.35      , 0.35      , 0.21387786,\n",
      "       0.29010037, 0.50177723, 0.2216901 , 0.36961085, 0.26006907,\n",
      "       0.35      , 0.35      , 0.32917714, 0.38625908, 0.35      ,\n",
      "       0.3388616 , 0.29845625, 0.22318284, 0.37392554, 0.37395847,\n",
      "       0.4325705 , 0.35668105, 0.35      , 0.32522315, 0.24607018,\n",
      "       0.24810567, 0.3156575 , 0.39089394, 0.35      , 0.33955252,\n",
      "       0.2478423 , 0.30358288, 0.35      , 0.4186066 , 0.34128916,\n",
      "       0.35      , 0.37040135, 0.2552128 , 0.35      , 0.31187838,\n",
      "       0.2970218 , 0.3237603 , 0.2595267 , 0.2134685 , 0.31403515,\n",
      "       0.4785456 , 0.35      , 0.272145  , 0.2742229 , 0.25055283,\n",
      "       0.24224757, 0.35      , 0.34177458, 0.3193881 , 0.22306806,\n",
      "       0.46950823, 0.35      , 0.39419258, 0.3426976 , 0.2168864 ,\n",
      "       0.41841587, 0.37051177, 0.32143945, 0.35      , 0.49919954,\n",
      "       0.35      , 0.36661884, 0.28043795, 0.37871882, 0.24573494,\n",
      "       0.35      , 0.4820462 , 0.36884654, 0.32653213, 0.32312754,\n",
      "       0.35      , 0.35      , 0.24134941, 0.2833244 , 0.35      ,\n",
      "       0.35      , 0.31001854, 0.40512645, 0.35      , 0.28375742,\n",
      "       0.42173275, 0.31899604, 0.27265322, 0.38104537, 0.3009832 ,\n",
      "       0.32378396, 0.3165997 , 0.25286677, 0.37614974, 0.32097366,\n",
      "       0.37779903, 0.22291675, 0.35      , 0.37779903, 0.317838  ,\n",
      "       0.35      , 0.21917874, 0.47946042, 0.30539292, 0.28817156,\n",
      "       0.26733384, 0.3032283 , 0.36205912, 0.21354279, 0.3706906 ,\n",
      "       0.25000745, 0.35      , 0.38081533, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.32144475, 0.27116695,\n",
      "       0.3856144 , 0.31553113, 0.3691837 , 0.37222514, 0.3156858 ,\n",
      "       0.35      , 0.2608084 , 0.24144869, 0.35      , 0.22170511,\n",
      "       0.29056025, 0.25333908, 0.4175313 , 0.3398439 , 0.27293637,\n",
      "       0.2144701 , 0.326669  , 0.2609124 , 0.41539344, 0.32753527,\n",
      "       0.26009497, 0.35      , 0.35      , 0.31549668, 0.28626743,\n",
      "       0.24993218, 0.35      , 0.46269006, 0.31521732, 0.27226412,\n",
      "       0.38240525, 0.27244586, 0.35      , 0.31977236, 0.31608814,\n",
      "       0.3630045 , 0.29419464, 0.35      , 0.26251444, 0.37213963,\n",
      "       0.35      , 0.35      , 0.37208155, 0.39523953, 0.35      ,\n",
      "       0.21771614, 0.35      , 0.25471967, 0.34914586, 0.35      ,\n",
      "       0.25306457, 0.36743134, 0.3636982 , 0.27383032, 0.35      ,\n",
      "       0.3580536 , 0.35      , 0.32143834, 0.23853955, 0.35      ,\n",
      "       0.23230974, 0.3159726 , 0.2960538 , 0.45288527, 0.35      ,\n",
      "       0.31459367, 0.35      , 0.3299249 , 0.3169496 , 0.3690313 ,\n",
      "       0.35      , 0.26186535, 0.35      , 0.39805707, 0.47512153,\n",
      "       0.35      , 0.35      , 0.35      , 0.30422664, 0.26269007,\n",
      "       0.34014922, 0.31993058, 0.34756106, 0.33467326, 0.22356285,\n",
      "       0.3155894 , 0.27466878, 0.24864079, 0.35      , 0.363745  ,\n",
      "       0.35      , 0.35      , 0.3318167 , 0.3654193 , 0.32459375,\n",
      "       0.35      , 0.32165512, 0.25333908, 0.35      , 0.22224784,\n",
      "       0.35      , 0.21051802, 0.35      , 0.27553952, 0.35      ,\n",
      "       0.35      , 0.4054488 , 0.21089295, 0.27546334, 0.29077724,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.32232398,\n",
      "       0.32514527, 0.25288528, 0.35      , 0.35      , 0.32566145,\n",
      "       0.28064314, 0.26733384, 0.27027062, 0.31125626, 0.35      ,\n",
      "       0.35730076, 0.35      , 0.35      , 0.32506213, 0.35      ,\n",
      "       0.22816704, 0.35      , 0.35      , 0.35      , 0.37581074,\n",
      "       0.3130045 , 0.2737493 , 0.28683633, 0.35      , 0.32060957,\n",
      "       0.35      , 0.35      , 0.2681864 , 0.31795567, 0.27459374,\n",
      "       0.37005505, 0.47808424, 0.37432024, 0.40884835, 0.31334424,\n",
      "       0.35      , 0.21241996, 0.3671228 , 0.20949517, 0.32626745,\n",
      "       0.41309226, 0.32835647, 0.3152834 , 0.3759884 , 0.33835384,\n",
      "       0.30335003, 0.35      , 0.35      , 0.3710516 , 0.37644514,\n",
      "       0.37642056, 0.35      , 0.2783815 , 0.35      , 0.37247378,\n",
      "       0.24528673, 0.35      , 0.27473605, 0.2678881 , 0.3192582 ,\n",
      "       0.30805412, 0.24309808, 0.35      , 0.30960813, 0.35      ,\n",
      "       0.24030183, 0.33272478, 0.32551318, 0.3373977 , 0.37455845,\n",
      "       0.27867687, 0.35      , 0.35      , 0.46964198, 0.26096404,\n",
      "       0.35      , 0.4134458 , 0.21534173, 0.44702134, 0.30853575,\n",
      "       0.35      , 0.24461037, 0.26707074, 0.32208094, 0.35      ,\n",
      "       0.26662412, 0.28583524, 0.35      , 0.26702514, 0.35      ,\n",
      "       0.42489326, 0.24028918, 0.35      , 0.21445125, 0.35      ,\n",
      "       0.31238124, 0.36366436, 0.26013267, 0.30539292, 0.36553943,\n",
      "       0.2833602 , 0.35      , 0.32745868, 0.35      , 0.2888616 ,\n",
      "       0.2494851 , 0.35      , 0.23632105, 0.37735704, 0.35      ,\n",
      "       0.3962153 , 0.35      , 0.35992178, 0.28353104, 0.26261714,\n",
      "       0.35      , 0.35      , 0.2713678 , 0.20845617, 0.35      ,\n",
      "       0.2973417 , 0.31164593, 0.39133745, 0.35      , 0.35      ,\n",
      "       0.36920115, 0.24070247, 0.35      , 0.35      , 0.35      ,\n",
      "       0.26645684, 0.36730084, 0.22238939, 0.35      , 0.36966422,\n",
      "       0.35      , 0.29522642, 0.5183183 , 0.36299512, 0.31627145,\n",
      "       0.33342516, 0.25124404, 0.38342518, 0.35      , 0.3392548 ,\n",
      "       0.3114017 , 0.27041492, 0.28927997, 0.35      , 0.22844005,\n",
      "       0.35633886, 0.29036963, 0.21373801, 0.35      , 0.35      ,\n",
      "       0.25179693, 0.27761734, 0.33564654, 0.35      , 0.35      ,\n",
      "       0.30674192, 0.2660404 , 0.35      , 0.37361494, 0.34814408,\n",
      "       0.25722772, 0.24806099, 0.35      , 0.30366543, 0.30849737,\n",
      "       0.35      , 0.36629984, 0.29390818, 0.23732392, 0.31457537,\n",
      "       0.35      , 0.31788808, 0.35      , 0.35      , 0.35871604,\n",
      "       0.26047352, 0.27902654, 0.21877532, 0.30635518, 0.35      ,\n",
      "       0.22859523, 0.29796857, 0.35      , 0.32260022, 0.35      ,\n",
      "       0.32748988, 0.35      , 0.26709294, 0.23930088, 0.32733837,\n",
      "       0.26753578, 0.33293036, 0.46286014, 0.31454834, 0.36288157,\n",
      "       0.2910104 , 0.28413087, 0.28863978, 0.36571997, 0.35      ,\n",
      "       0.35      , 0.35      , 0.4186066 , 0.357139  , 0.3867462 ,\n",
      "       0.35      , 0.35      , 0.2529197 , 0.36531252, 0.27041492,\n",
      "       0.26661885, 0.31594595, 0.35      , 0.2211286 , 0.35      ,\n",
      "       0.35      , 0.33293036, 0.388221  , 0.34514868, 0.3233514 ,\n",
      "       0.29099786, 0.35      , 0.35      , 0.30972376, 0.35      ,\n",
      "       0.4196211 , 0.3631453 , 0.32473603, 0.35      , 0.2655669 ,\n",
      "       0.35      , 0.22037226, 0.35      , 0.4054111 , 0.35      ,\n",
      "       0.32105482, 0.35      , 0.24624148, 0.24707909, 0.37517667,\n",
      "       0.35      , 0.36604652, 0.315143  , 0.35      , 0.3371076 ,\n",
      "       0.25832626, 0.35      , 0.35      , 0.33796892, 0.32307   ,\n",
      "       0.22887744, 0.42243046, 0.21660109, 0.37563738, 0.28796893,\n",
      "       0.32876664, 0.37136778, 0.35      , 0.51070946, 0.3172526 ,\n",
      "       0.44914615, 0.39089394, 0.31989488, 0.35      , 0.25837022,\n",
      "       0.26760527, 0.35      , 0.260152  , 0.32676265, 0.3318167 ,\n",
      "       0.35      , 0.32826746, 0.35      , 0.4800721 , 0.33173326,\n",
      "       0.35      , 0.27665222, 0.38204724, 0.25051382, 0.2936473 ,\n",
      "       0.35      , 0.35      , 0.41371435, 0.28001136, 0.35      ,\n",
      "       0.26539344, 0.35      , 0.35      , 0.45838064, 0.32767388,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.30787948,\n",
      "       0.36223698, 0.36454833, 0.35      , 0.27442887, 0.35      ,\n",
      "       0.35      , 0.37789518, 0.35      , 0.35      , 0.27545193,\n",
      "       0.46728572, 0.22384584, 0.35      , 0.35      , 0.37473604,\n",
      "       0.33113965, 0.27222514, 0.3237429 , 0.38485348, 0.35      ,\n",
      "       0.3250042 , 0.3461314 , 0.3265309 , 0.35      , 0.316662  ,\n",
      "       0.47797665, 0.31900695, 0.2525332 , 0.27944577, 0.21683231,\n",
      "       0.27134076, 0.35      , 0.24322207, 0.27889875, 0.2758781 ,\n",
      "       0.35      , 0.26526278, 0.35      , 0.35      , 0.37060958,\n",
      "       0.21919058, 0.22289202, 0.51657385, 0.35      , 0.37333962,\n",
      "       0.3760154 , 0.35383713, 0.35      , 0.2681864 , 0.35      ,\n",
      "       0.26270458, 0.26700106, 0.24674138, 0.35      , 0.3165997 ,\n",
      "       0.27559602, 0.43398258, 0.29855523, 0.35      , 0.41009495,\n",
      "       0.35      , 0.2695947 , 0.35      , 0.3188556 , 0.35      ,\n",
      "       0.38098422, 0.35      , 0.2875845 , 0.35      , 0.3239509 ,\n",
      "       0.37660226, 0.32507518, 0.23280767, 0.32473603, 0.28179693,\n",
      "       0.37229687, 0.35      , 0.2908462 , 0.31914315, 0.35      ,\n",
      "       0.3901492 , 0.36620924, 0.35      , 0.35      , 0.358723  ,\n",
      "       0.31464264, 0.32523698, 0.34929594, 0.21221785, 0.32770854,\n",
      "       0.35      , 0.21204613, 0.35      , 0.24543999, 0.2619624 ,\n",
      "       0.38590547, 0.307628  , 0.222778  , 0.36825973, 0.38574708,\n",
      "       0.31260344, 0.35      , 0.35861117, 0.31018552, 0.24494512,\n",
      "       0.26965928, 0.34245208, 0.29165784, 0.35      , 0.35      ,\n",
      "       0.27923933, 0.35      , 0.32136115, 0.43148822, 0.3736261 ,\n",
      "       0.51537013, 0.27197197, 0.24784558, 0.3087011 , 0.33134302,\n",
      "       0.27491432, 0.2706873 , 0.35      , 0.35      , 0.37505692,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.2868458 ,\n",
      "       0.35      , 0.4759749 , 0.34597707, 0.31193185, 0.35      ,\n",
      "       0.35      , 0.35      , 0.22208619, 0.2504587 , 0.31440663,\n",
      "       0.32097432, 0.37083793, 0.45243663, 0.24225166, 0.35      ,\n",
      "       0.35      , 0.51447636, 0.35      , 0.20708223, 0.35      ,\n",
      "       0.2144701 , 0.35      , 0.32191837, 0.35      , 0.26549056,\n",
      "       0.22969766, 0.3153323 , 0.26684737, 0.35      , 0.32495183,\n",
      "       0.3299249 , 0.3208179 , 0.33104393, 0.36450016, 0.36027566,\n",
      "       0.38872373, 0.35      , 0.31359366, 0.26957476, 0.35      ,\n",
      "       0.36313272, 0.35      , 0.35      , 0.3280732 , 0.35      ,\n",
      "       0.32226413, 0.31028262, 0.35      , 0.31919834, 0.35      ,\n",
      "       0.4209792 , 0.3614892 , 0.46930298, 0.35      , 0.37309003,\n",
      "       0.32432255, 0.35      , 0.30066255, 0.35      , 0.35      ,\n",
      "       0.35      , 0.23957478, 0.3423328 , 0.2901492 , 0.22235234,\n",
      "       0.29889834, 0.27333963, 0.22187817, 0.2636342 , 0.30481398,\n",
      "       0.2692288 , 0.33637995, 0.3045206 , 0.29817364, 0.3179926 ,\n",
      "       0.54889834, 0.41133752, 0.37449604, 0.31707072, 0.27032468,\n",
      "       0.31739482, 0.35      , 0.3631718 , 0.41798827, 0.40538967,\n",
      "       0.21707909, 0.317322  , 0.37484837, 0.35      , 0.45145902,\n",
      "       0.24810567, 0.35      , 0.52944833, 0.2251425 , 0.51136506,\n",
      "       0.35      , 0.28362092, 0.25766772, 0.45776707, 0.35      ,\n",
      "       0.295506  , 0.28892744, 0.35      , 0.31536692, 0.3167304 ,\n",
      "       0.39458284, 0.32221478, 0.29531026, 0.451646  , 0.22045869,\n",
      "       0.25083065, 0.35      , 0.3400065 ], dtype=float32), 'method_nbr': array([2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
      "       2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 1., 2., 2.,\n",
      "       1., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
      "       2., 1., 1., 1., 2., 2., 2., 1., 1., 2., 2., 2., 2., 1., 2., 1., 2.,\n",
      "       1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
      "       2., 2., 1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2.,\n",
      "       2., 2., 1., 2., 1., 2., 1., 2., 2., 2., 1., 2., 1., 2., 2., 2., 1.,\n",
      "       2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
      "       2., 2., 1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
      "       2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2.,\n",
      "       2., 2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2.,\n",
      "       2., 2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1.,\n",
      "       1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 1., 2., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2.,\n",
      "       2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2.,\n",
      "       2., 2., 1., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 1.,\n",
      "       1., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2.,\n",
      "       2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 2., 2., 1., 2., 1., 2., 2.,\n",
      "       1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 1., 1.,\n",
      "       2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 1., 2., 1., 1., 2., 2., 2.,\n",
      "       2., 1., 1., 1., 1., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2., 1., 2.,\n",
      "       1., 1., 2., 1., 2., 1., 1., 1., 2., 2., 2., 2., 1., 2., 1., 1., 2.,\n",
      "       2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 1., 1., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
      "       1., 2., 1., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 1., 2., 2., 2.,\n",
      "       2., 1., 2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2.,\n",
      "       2., 2., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 1., 2., 1., 2.,\n",
      "       2., 2., 1., 1., 2., 2., 1., 2., 2., 2., 1., 1., 2., 2., 1., 1., 1.,\n",
      "       2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2.,\n",
      "       2., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 1., 1., 2., 2., 1., 2.,\n",
      "       2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 2., 1., 2., 1., 1., 2., 2.,\n",
      "       2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2., 2., 2., 2., 1., 1., 1., 2., 2., 2., 1., 1., 2., 2., 2.,\n",
      "       2., 2., 1., 2., 1., 1., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 2.,\n",
      "       2., 1., 2., 1., 2., 1., 2., 1., 2., 1., 2., 2., 2., 1., 2., 2., 1.,\n",
      "       2., 2., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2.,\n",
      "       2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2.,\n",
      "       2., 2., 1., 1., 2., 2., 1., 2., 1., 1., 2., 2., 1., 1., 1., 1., 2.,\n",
      "       2., 2., 1., 2., 1., 1., 2., 1., 1., 2., 2., 2., 1., 1., 2., 2., 2.,\n",
      "       2., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
      "       2., 1., 2., 1., 1., 2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 1., 2.,\n",
      "       2., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 2., 1., 2., 1., 2.,\n",
      "       1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 1., 1., 2.,\n",
      "       2., 2., 2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1.,\n",
      "       2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 2., 2., 2., 1., 1.,\n",
      "       1., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 1., 2.,\n",
      "       1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 1.,\n",
      "       2., 1., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 1., 2., 2., 1.,\n",
      "       2., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
      "       1., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2., 1., 2.], dtype=float32)}\n",
      "min: -9223372036854775808, max: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/j.poschenrieder/miniconda3/envs/dlomix/lib/python3.10/site-packages/datasets/utils/tf_utils.py:131: RuntimeWarning: invalid value encountered in cast\n",
      "  array = array.astype(cast_dtype)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_val = 1000\n",
    "max_val = -1000\n",
    "for x in dataset.tensor_val_data.as_numpy_iterator():\n",
    "    seq = x[0]['modified_sequence']\n",
    "\n",
    "    line_min = np.min(seq)\n",
    "    if line_min < 0:\n",
    "        print(x[0])\n",
    "        # for s in seq:\n",
    "        #     print(s)\n",
    "\n",
    "    min_val = np.min([min_val, np.min(seq)])\n",
    "    max_val = np.max([max_val, np.max(seq)])\n",
    "\n",
    "print(f\"min: {min_val}, max: {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

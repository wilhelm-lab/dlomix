{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Callbacks for Model Refinement and Transfer Learning\n",
    "This script facilitates the exploration of various callback mechanisms within TensorFlow and Keras. The objective is to develop accessible functions that enable further training, refinement, and transfer learning of Prosit Models, with the intention of integrating these into DLOmix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'name': 'noptm_baseline_small_bs1024', 'hf_home': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets', 'hf_cache': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/hf_cache', 'parquet_path': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean', 'processed_path': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024', 'seq_length': 30, 'batch_size': 1024}, 'training': {'learning_rate': 0.0001, 'num_epochs': 2}, 'model': {'save_dir': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/callback_models'}, 'processing': {'num_proc': 40}, 'callbacks': {'early_stopping': {'monitor': 'val_loss', 'min_delta': 0.001, 'patience': 20, 'restore_best_weights': True}, 'reduce_lr': {'monitor': 'val_loss', 'factor': 0.1, 'patience': 10, 'min_lr': '1e-6', 'mode': 'auto', 'min_delta': '1e-4', 'verbose': 1}, 'learning_rate_scheduler': {'initial_lr': 0.0001, 'decay_rate': 0.9}, 'lambda_callback': {'on_epoch_end': \"lambda epoch, logs: print(f'Epoch {epoch} ended with logs: {logs}')\"}, 'csv_logger': {'filename': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/logs/training_log.csv', 'append': False}, 'model_checkpoint': {'filepath': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/checkpoints/model-{epoch:02d}-{val_loss:.2f}.keras', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': False, 'mode': 'auto', 'save_freq': 'epoch', 'verbose': 1}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Manually specify the path to the configuration file (Note: change path according to your directories)\n",
    "config_file_path = '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/config_files/baseline_noptm_baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "\n",
    "# Show config containing the configuration data\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure environment\n",
    "import os\n",
    "os.environ['HF_HOME'] = config['dataset']['hf_home']\n",
    "os.environ['HF_DATASETS_CACHE'] = config['dataset']['hf_cache']\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.tf_device_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/s.baier/miniconda3/envs/dlomix/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2024-06-19 11:04:23.982480: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 11:04:23.982629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 11:04:24.274614: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 11:04:25.658767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 11:04:38.113164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/nfs/home/students/s.baier/miniconda3/envs/dlomix/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msylvie-baier\u001b[0m (\u001b[33mmapra_dlomix\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/tutorials/wandb/run-20240619_110455-r8zz6da3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3' target=\"_blank\">rose-oath-43</a></strong> to <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff9d86b2920>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uuid to ensure unique identifiers\n",
    "import uuid\n",
    "# initialize weights and biases\n",
    "import wandb\n",
    "# import WandbCallback\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# set id for run using uuid\n",
    "config['run_id'] = uuid.uuid4()\n",
    "\n",
    "# set up wandb for this project\n",
    "project_name = f'callback model training'\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config=config,\n",
    "    tags=[config['dataset']['name']], \n",
    "    entity = 'mapra_dlomix'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DLOmix dataset \n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "\n",
    "# Own dataset\n",
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(wandb.config['dataset']['processed_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 11:05:06.376816: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorFlow and the optimizer\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=wandb.config['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau,\n",
    "    LambdaCallback, TerminateOnNaN, CSVLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=wandb.config['callbacks']['early_stopping']['monitor'],\n",
    "    min_delta=wandb.config['callbacks']['early_stopping']['min_delta'],\n",
    "    patience=wandb.config['callbacks']['early_stopping']['patience'],\n",
    "    restore_best_weights=wandb.config['callbacks']['early_stopping']['restore_best_weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce LR on Plateau Callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=wandb.config['callbacks']['reduce_lr']['monitor'],\n",
    "    factor=wandb.config['callbacks']['reduce_lr']['factor'],\n",
    "    patience=wandb.config['callbacks']['reduce_lr']['patience'],\n",
    "    min_lr=wandb.config['callbacks']['reduce_lr']['min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler Callback\n",
    "learning_rate_scheduler = LearningRateScheduler(\n",
    "    schedule=lambda epoch: wandb.config['callbacks']['learning_rate_scheduler']['initial_lr'] * wandb.config['callbacks']['learning_rate_scheduler']['decay_rate'] ** epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate On NaN Callback: Callback that terminates training when a NaN loss is encountered.\n",
    "terminate_on_nan = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Callback \n",
    "lambda_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"Starting epoch {epoch + 1}\")\n",
    "    # on_epoch_end=None,\n",
    "    # on_train_begin=None,\n",
    "    # on_train_end=None,\n",
    "    # on_train_batch_begin=None,\n",
    "    # on_train_batch_end=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger Callback (Note: not necessary when using wandb)\n",
    "csv_logger = CSVLogger(\n",
    "    filename=wandb.config['callbacks']['csv_logger']['filename'], \n",
    "    append=wandb.config['callbacks']['csv_logger']['append']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint Callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=wandb.config['callbacks']['model_checkpoint']['filepath'],\n",
    "    monitor=wandb.config['callbacks']['model_checkpoint']['monitor'],\n",
    "    save_best_only=wandb.config['callbacks']['model_checkpoint']['save_best_only'],\n",
    "    save_weights_only=wandb.config['callbacks']['model_checkpoint']['save_weights_only'],\n",
    "    mode=wandb.config['callbacks']['model_checkpoint']['mode'],\n",
    "    save_freq=wandb.config['callbacks']['model_checkpoint']['save_freq'],\n",
    "    verbose=wandb.config['callbacks']['model_checkpoint']['verbose']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "from dlomix.models import PrositIntensityPredictor # predictor for intensity \n",
    "from dlomix.constants import PTMS_ALPHABET # alphabet with PTMs (can be adapted based on the data)\n",
    "\n",
    "input_mapping = {\n",
    "    \"SEQUENCE_KEY\": \"modified_sequence\",\n",
    "    \"COLLISION_ENERGY_KEY\": \"collision_energy_aligned_normed\",\n",
    "    \"PRECURSOR_CHARGE_KEY\": \"precursor_charge_onehot\",\n",
    "    \"FRAGMENTATION_TYPE_KEY\": \"method_nbr\",\n",
    "}\n",
    "\n",
    "meta_data_keys = [\"collision_energy_aligned_normed\", \"precursor_charge_onehot\", \"method_nbr\"]\n",
    "\n",
    "# initialize prosit model\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=wandb.config['dataset']['seq_length'],\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False,\n",
    "    input_keys=input_mapping,\n",
    "    meta_data_keys=meta_data_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch 1/2\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7057 - masked_pearson_correlation_distance: 0.6142\n",
      "Epoch 1: val_loss improved from inf to 0.67107, saving model to /nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/checkpoints/model-01-0.67.keras\n",
      "8/8 [==============================] - 27s 2s/step - loss: 0.7057 - masked_pearson_correlation_distance: 0.6142 - val_loss: 0.6711 - val_masked_pearson_correlation_distance: 0.5634 - lr: 1.0000e-04\n",
      "Starting epoch 2\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6693 - masked_pearson_correlation_distance: 0.5620\n",
      "Epoch 2: val_loss improved from 0.67107 to 0.66755, saving model to /nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/checkpoints/model-02-0.67.keras\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6693 - masked_pearson_correlation_distance: 0.5620 - val_loss: 0.6676 - val_masked_pearson_correlation_distance: 0.5576 - lr: 9.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff9b82b5fc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=wandb.config['training']['num_epochs'],\n",
    "    callbacks=[WandbCallback(save_model=False, log_batch_frequency=True), \n",
    "               early_stopping, \n",
    "               reduce_lr, \n",
    "               learning_rate_scheduler,           \n",
    "               terminate_on_nan, \n",
    "               lambda_callback, \n",
    "               csv_logger, # (Note: not necessary when using wandb; shown for completeness)\n",
    "               model_checkpoint\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/callback_models/noptm_baseline_small_bs1024/b2877e59-3e23-4ced-a751-ff5bcbf2c331.keras\n"
     ]
    }
   ],
   "source": [
    "# model path to save to the model to (Note: The file needs to end with the .keras extension.)\n",
    "model_path = f\"{wandb.config['model']['save_dir']}/{wandb.config['dataset']['name']}/{wandb.config['run_id']}.keras\"\n",
    "\n",
    "# save the model\n",
    "model.save(model_path)  \n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38aafe4cfd9846388e4b4d3f0e7fd671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.66755</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.66934</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.56202</td></tr><tr><td>val_loss</td><td>0.66755</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.55755</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-oath-43</strong> at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/r8zz6da3</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_110455-r8zz6da3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# load the trained model \n",
    "reconstructed_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Output Dimension: 16\n",
      "Sequence Length: 30\n",
      "Alphabet Dictionary: {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, '[]-': 21, '-[]': 22, '[UNIMOD:737]-': 56, 'M[UNIMOD:35]': 23, 'S[UNIMOD:21]': 24, 'T[UNIMOD:21]': 25, 'Y[UNIMOD:21]': 26, 'R[UNIMOD:7]': 27, 'Q[UNIMOD:7]': 4, 'N[UNIMOD:7]': 3, 'K[UNIMOD:1]': 28, 'K[UNIMOD:121]': 29, 'Q[UNIMOD:28]': 30, 'R[UNIMOD:34]': 31, 'K[UNIMOD:34]': 32, 'T[UNIMOD:43]': 35, 'S[UNIMOD:43]': 36, 'C[UNIMOD:4]': 37, '[UNIMOD:1]-': 38, 'E[UNIMOD:27]': 39, 'K[UNIMOD:36]': 40, 'K[UNIMOD:37]': 41, 'K[UNIMOD:122]': 42, 'K[UNIMOD:58]': 43, 'K[UNIMOD:1289]': 44, 'K[UNIMOD:747]': 45, 'K[UNIMOD:64]': 46, 'K[UNIMOD:1848]': 47, 'K[UNIMOD:1363]': 48, 'K[UNIMOD:1849]': 49, 'K[UNIMOD:3]': 50, 'K[UNIMOD:737]': 55, 'R[UNIMOD:36]': 51, 'R[UNIMOD:36a]': 52, 'P[UNIMOD:35]': 53, 'Y[UNIMOD:354]': 54}\n",
      "Dropout Rate: 0.2\n",
      "Latent Dropout Rate: 0.1\n",
      "Recurrent Layers Sizes: (256, 512)\n",
      "Regressor Layer Size: 512\n",
      "Use Prosit PTM Features: False\n",
      "Input Keys: {'SEQUENCE_KEY': 'modified_sequence', 'COLLISION_ENERGY_KEY': 'collision_energy_aligned_normed', 'PRECURSOR_CHARGE_KEY': 'precursor_charge_onehot', 'FRAGMENTATION_TYPE_KEY': 'method_nbr'}\n",
      "Default Input Keys: {'SEQUENCE_KEY': 'sequence', 'COLLISION_ENERGY_KEY': 'collision_energy', 'PRECURSOR_CHARGE_KEY': 'precursor_charge', 'FRAGMENTATION_TYPE_KEY': 'fragmentation_type'}\n",
      "Meta Data Keys (Attribute): ['COLLISION_ENERGY_KEY', 'PRECURSOR_CHARGE_KEY', 'FRAGMENTATION_TYPE_KEY']\n",
      "PTM Input Keys: ['mod_loss', 'delta_mass', 'mod_gain', 'atom_count', 'red_smiles']\n"
     ]
    }
   ],
   "source": [
    "# Model summary \n",
    "\n",
    "# Print parameters\n",
    "print(\"Embedding Output Dimension:\", reconstructed_model.embedding_output_dim)\n",
    "print(\"Sequence Length:\", reconstructed_model.seq_length)\n",
    "print(\"Alphabet Dictionary:\", reconstructed_model.alphabet)\n",
    "print(\"Dropout Rate:\", reconstructed_model.dropout_rate)\n",
    "print(\"Latent Dropout Rate:\", reconstructed_model.latent_dropout_rate)\n",
    "print(\"Recurrent Layers Sizes:\", reconstructed_model.recurrent_layers_sizes)\n",
    "print(\"Regressor Layer Size:\", reconstructed_model.regressor_layer_size)\n",
    "print(\"Use Prosit PTM Features:\", reconstructed_model.use_prosit_ptm_features)\n",
    "print(\"Input Keys:\", reconstructed_model.input_keys)\n",
    "\n",
    "# Print attributes\n",
    "print(\"Default Input Keys:\", reconstructed_model.DEFAULT_INPUT_KEYS)\n",
    "print(\"Meta Data Keys (Attribute):\", reconstructed_model.META_DATA_KEYS)\n",
    "print(\"PTM Input Keys:\", reconstructed_model.PTM_INPUT_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model at a certain checkpoint (Note: change path according to your directories)\n",
    "checkpoint_model = keras.models.load_model(\"/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/checkpoints/model-01-0.67.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Output Dimension: 16\n",
      "Sequence Length: 30\n",
      "Alphabet Dictionary: {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, '[]-': 21, '-[]': 22, '[UNIMOD:737]-': 56, 'M[UNIMOD:35]': 23, 'S[UNIMOD:21]': 24, 'T[UNIMOD:21]': 25, 'Y[UNIMOD:21]': 26, 'R[UNIMOD:7]': 27, 'Q[UNIMOD:7]': 4, 'N[UNIMOD:7]': 3, 'K[UNIMOD:1]': 28, 'K[UNIMOD:121]': 29, 'Q[UNIMOD:28]': 30, 'R[UNIMOD:34]': 31, 'K[UNIMOD:34]': 32, 'T[UNIMOD:43]': 35, 'S[UNIMOD:43]': 36, 'C[UNIMOD:4]': 37, '[UNIMOD:1]-': 38, 'E[UNIMOD:27]': 39, 'K[UNIMOD:36]': 40, 'K[UNIMOD:37]': 41, 'K[UNIMOD:122]': 42, 'K[UNIMOD:58]': 43, 'K[UNIMOD:1289]': 44, 'K[UNIMOD:747]': 45, 'K[UNIMOD:64]': 46, 'K[UNIMOD:1848]': 47, 'K[UNIMOD:1363]': 48, 'K[UNIMOD:1849]': 49, 'K[UNIMOD:3]': 50, 'K[UNIMOD:737]': 55, 'R[UNIMOD:36]': 51, 'R[UNIMOD:36a]': 52, 'P[UNIMOD:35]': 53, 'Y[UNIMOD:354]': 54}\n",
      "Dropout Rate: 0.2\n",
      "Latent Dropout Rate: 0.1\n",
      "Recurrent Layers Sizes: (256, 512)\n",
      "Regressor Layer Size: 512\n",
      "Use Prosit PTM Features: False\n",
      "Input Keys: {'SEQUENCE_KEY': 'modified_sequence', 'COLLISION_ENERGY_KEY': 'collision_energy_aligned_normed', 'PRECURSOR_CHARGE_KEY': 'precursor_charge_onehot', 'FRAGMENTATION_TYPE_KEY': 'method_nbr'}\n",
      "Default Input Keys: {'SEQUENCE_KEY': 'sequence', 'COLLISION_ENERGY_KEY': 'collision_energy', 'PRECURSOR_CHARGE_KEY': 'precursor_charge', 'FRAGMENTATION_TYPE_KEY': 'fragmentation_type'}\n",
      "Meta Data Keys (Attribute): ['COLLISION_ENERGY_KEY', 'PRECURSOR_CHARGE_KEY', 'FRAGMENTATION_TYPE_KEY']\n",
      "PTM Input Keys: ['mod_loss', 'delta_mass', 'mod_gain', 'atom_count', 'red_smiles']\n"
     ]
    }
   ],
   "source": [
    "# Model summary \n",
    "\n",
    "# Print parameters\n",
    "print(\"Embedding Output Dimension:\", checkpoint_model.embedding_output_dim)\n",
    "print(\"Sequence Length:\", checkpoint_model.seq_length)\n",
    "print(\"Alphabet Dictionary:\", checkpoint_model.alphabet)\n",
    "print(\"Dropout Rate:\", checkpoint_model.dropout_rate)\n",
    "print(\"Latent Dropout Rate:\", checkpoint_model.latent_dropout_rate)\n",
    "print(\"Recurrent Layers Sizes:\", checkpoint_model.recurrent_layers_sizes)\n",
    "print(\"Regressor Layer Size:\", checkpoint_model.regressor_layer_size)\n",
    "print(\"Use Prosit PTM Features:\", checkpoint_model.use_prosit_ptm_features)\n",
    "print(\"Input Keys:\", checkpoint_model.input_keys)\n",
    "\n",
    "# Print attributes\n",
    "print(\"Default Input Keys:\", checkpoint_model.DEFAULT_INPUT_KEYS)\n",
    "print(\"Meta Data Keys (Attribute):\", checkpoint_model.META_DATA_KEYS)\n",
    "print(\"PTM Input Keys:\", checkpoint_model.PTM_INPUT_KEYS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

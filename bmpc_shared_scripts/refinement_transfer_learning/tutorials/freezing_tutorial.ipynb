{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for freezing the Prosit Model\n",
    "This tutorial shows, how to freeze the Prosit Intensity Predictor model and only let the first and last layer remain trainable for refinement and transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 10:38:59.580655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 10:38:59.580703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 10:38:59.582164: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 10:38:59.590913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 10:39:01.048779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/nfs/home/students/l.willruth/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dlomix\n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 10:39:04.428381: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  928       \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582762 (13.67 MB)\n",
      "Trainable params: 3582762 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/baseline_models/noptm_baseline_full_bs1024/\"\n",
    "model = tf.keras.models.load_model(model_path + \"85c6c918-4a2a-42e5-aab1-e666121c69a6.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to freeze all layers except first and/or last layer\n",
    "def freeze_model(model:dlomix.models.prosit.PrositIntensityPredictor,optimizer:tf.keras.optimizers, trainable_first_layer:bool = False, trainable_last_layer:bool = False, loss:dlomix.losses=masked_spectral_distance, metrics:list=[masked_pearson_correlation_distance]) -> None:\n",
    "    ''' Freezes all layers of a PrositIntensityPredictor and keep first and/or last layer trainable.\n",
    "\n",
    "    First setting the whole model to trainable because this attribute overshadows the trainable attribute of every sublayer.\n",
    "    Then iterating through all sublayers and sets the trainable attribute of every layer to 'False', model is now frozen.\n",
    "    Next, setting the trainable attribute of either the first embedding layer or the last time density layer to trainable.\n",
    "    Finally, compile the model with the optimizer, loss, and metrics to make the changes take effect.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model                   : dlomix.models.prosit.PrositIntensityPredictor\n",
    "                              The model to be frozen.\n",
    "    optimizer               : tf.keras.optimizers\n",
    "                              The optimizer is needed for compiling the model.\n",
    "    trainable_first_layer   : bool\n",
    "                              Whether the first layer should remain trainable.\n",
    "    trainable_last_layer    : bool\n",
    "                              Whether the last layer should remain trainable\n",
    "    loss                    : dlomix.losses\n",
    "                              The loss for compiling the model. \n",
    "                              default: masked_spectral_distance\n",
    "    metrics                 : list[dlomix.losses]\n",
    "                              The metrics for compiling the model.\n",
    "                              default: [masked_pearson_correlation_distance] \n",
    "    --------\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.trainable = True \n",
    "    for lay in model.layers:\n",
    "        try:\n",
    "            for sublay in lay.layers:\n",
    "                sublay.trainable = False\n",
    "        except (AttributeError):\n",
    "            lay.trainable = False\n",
    "\n",
    "    if (trainable_first_layer):\n",
    "        first_layer = model.get_layer(name=\"embedding\")\n",
    "        first_layer.trainable = True\n",
    "\n",
    "    if (trainable_last_layer):\n",
    "        last_layer = model.get_layer(index = 6).get_layer(\"time_dense\")\n",
    "        last_layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print the trainable attribute of every layer\n",
    "def check_trainability(model, sublayers = False):\n",
    "    for lay in model.layers:\n",
    "        if(sublayers):\n",
    "            print()\n",
    "            try:\n",
    "                lay.layers\n",
    "                print(f'Layer: {lay.name}, Trainable: {lay.trainable}')\n",
    "                for lay2 in lay.layers:\n",
    "                    print(f\"Layer: {lay2.name}, Trainable: {lay2.trainable}\")\n",
    "            except(AttributeError):\n",
    "                print(f'Layer: {lay.name}, Trainable: {lay.trainable}')\n",
    "        else:\n",
    "            print(f'Layer: {lay.name}, Trainable: {lay.trainable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Freeze all layers except the first and the last layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer: embedding, Trainable: True\n",
      "\n",
      "Layer: sequential, Trainable: True\n",
      "Layer: bidirectional, Trainable: False\n",
      "Layer: dropout, Trainable: False\n",
      "Layer: gru_1, Trainable: False\n",
      "Layer: dropout_1, Trainable: False\n",
      "\n",
      "Layer: sequential_1, Trainable: True\n",
      "Layer: meta_in, Trainable: False\n",
      "Layer: meta_dense, Trainable: False\n",
      "Layer: meta_dense_do, Trainable: False\n",
      "\n",
      "Layer: sequential_2, Trainable: True\n",
      "Layer: decoder, Trainable: False\n",
      "Layer: dropout_2, Trainable: False\n",
      "Layer: decoder_attention_layer, Trainable: False\n",
      "\n",
      "Layer: encoder_att, Trainable: False\n",
      "\n",
      "Layer: sequential_3, Trainable: True\n",
      "Layer: add_meta, Trainable: False\n",
      "Layer: repeat, Trainable: False\n",
      "\n",
      "Layer: sequential_4, Trainable: True\n",
      "Layer: time_dense, Trainable: True\n",
      "Layer: activation, Trainable: False\n",
      "Layer: out, Trainable: False\n"
     ]
    }
   ],
   "source": [
    "freeze_model(model, optimizer, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare everything for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset and the PTM alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(\"/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Training with frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer: embedding, Trainable: True\n",
      "\n",
      "Layer: sequential, Trainable: True\n",
      "Layer: bidirectional, Trainable: False\n",
      "Layer: dropout, Trainable: False\n",
      "Layer: gru_1, Trainable: False\n",
      "Layer: dropout_1, Trainable: False\n",
      "\n",
      "Layer: sequential_1, Trainable: True\n",
      "Layer: meta_in, Trainable: False\n",
      "Layer: meta_dense, Trainable: False\n",
      "Layer: meta_dense_do, Trainable: False\n",
      "\n",
      "Layer: sequential_2, Trainable: True\n",
      "Layer: decoder, Trainable: False\n",
      "Layer: dropout_2, Trainable: False\n",
      "Layer: decoder_attention_layer, Trainable: False\n",
      "\n",
      "Layer: encoder_att, Trainable: False\n",
      "\n",
      "Layer: sequential_3, Trainable: True\n",
      "Layer: add_meta, Trainable: False\n",
      "Layer: repeat, Trainable: False\n",
      "\n",
      "Layer: sequential_4, Trainable: True\n",
      "Layer: time_dense, Trainable: True\n",
      "Layer: activation, Trainable: False\n",
      "Layer: out, Trainable: False\n"
     ]
    }
   ],
   "source": [
    "freeze_model(model,optimizer, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 22s 2s/step - loss: 0.1371 - masked_pearson_correlation_distance: 0.1454 - val_loss: 0.1177 - val_masked_pearson_correlation_distance: 0.1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7fd02ecc10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train again while only the first layer and the last layer are trainable\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 0 stayed the same: False\n",
      "weights 1 stayed the same: True\n",
      "weights 2 stayed the same: True\n",
      "weights 3 stayed the same: True\n",
      "weights 4 stayed the same: True\n",
      "weights 5 stayed the same: True\n",
      "weights 6 stayed the same: True\n",
      "weights 7 stayed the same: True\n",
      "weights 8 stayed the same: True\n",
      "weights 9 stayed the same: True\n",
      "weights 10 stayed the same: True\n",
      "weights 11 stayed the same: True\n",
      "weights 12 stayed the same: True\n",
      "weights 13 stayed the same: True\n",
      "weights 14 stayed the same: True\n",
      "weights 15 stayed the same: True\n",
      "weights 16 stayed the same: True\n",
      "weights 17 stayed the same: True\n",
      "weights 18 stayed the same: True\n",
      "weights 19 stayed the same: False\n",
      "weights 20 stayed the same: False\n"
     ]
    }
   ],
   "source": [
    "# check which weights have changed\n",
    "retrained_weights = model.get_weights()\n",
    "for i, w in enumerate(zip(original_weights, retrained_weights)):\n",
    "    print(f'weights {i} stayed the same: {(w[0]==w[1]).all()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two weight tensors changed for the last layer. Both tensors belong to the last time_dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15468788 0.05041085 0.01186818 0.07319512 0.0274905  0.00195869]\n",
      "512\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "3078\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_dense (TimeDistribute  (None, 29, 6)             3078      \n",
      " d)                                                              \n",
      "                                                                 \n",
      " activation (LeakyReLU)      (None, 29, 6)             0         \n",
      "                                                                 \n",
      " out (Flatten)               (None, 174)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3078 (12.02 KB)\n",
      "Trainable params: 3078 (12.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(retrained_weights[20]) # 6\n",
    "print(len(retrained_weights[19])) # 512\n",
    "print([len(x) for x in retrained_weights[19]]) # 6\n",
    "print(512 * 6 + 6) # 3078\n",
    "print(model.get_layer(name=\"sequential_4\").summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Release the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_model(model:PrositIntensityPredictor, optimizer_config: dict = {\"learning_rate\":1e-4}, loss:dlomix.losses=masked_spectral_distance, metrics:list=[masked_pearson_correlation_distance]) -> None:\n",
    "    '''Unfreezes all layers of a PrositIntensityPredictor model.\n",
    "\n",
    "        Sets the trainable attribute of every layer to 'True'.\n",
    "        Finally, compiles the model with the optimizer, loss, and metrics to make the changes take effect.\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        model                   : dlomix.models.prosit.PrositIntensityPredictor\n",
    "                                The model to be unfrozen.\n",
    "        optimizer_config        : dict\n",
    "                                The initialization parameters for the new optimizer needed for compiling the model.\n",
    "        loss                    : dlomix.losses\n",
    "                                The loss for compiling the model.\n",
    "                                default: masked_spectral_distance\n",
    "        metrics                 : list[dlomix.losses]\n",
    "                                The metrics for compiling the model.\n",
    "                                default: [masked_pearson_correlation_distance] \n",
    "        --------\n",
    "        '''\n",
    "    model.trainable = True\n",
    "\n",
    "    for lay in model.layers:\n",
    "        try:\n",
    "            for sublay in lay.layers:\n",
    "                sublay.trainable = True\n",
    "        except (AttributeError):\n",
    "            lay.trainable = True\n",
    "\n",
    "    new_optimizer = tf.keras.optimizers.Adam(**optimizer_config)\n",
    "    model.compile(\n",
    "        optimizer=new_optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "     \n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n",
      "\n",
      "Layer: embedding, Trainable: True\n",
      "\n",
      "Layer: sequential, Trainable: True\n",
      "Layer: bidirectional, Trainable: True\n",
      "Layer: dropout, Trainable: True\n",
      "Layer: gru_1, Trainable: True\n",
      "Layer: dropout_1, Trainable: True\n",
      "\n",
      "Layer: sequential_1, Trainable: True\n",
      "Layer: meta_in, Trainable: True\n",
      "Layer: meta_dense, Trainable: True\n",
      "Layer: meta_dense_do, Trainable: True\n",
      "\n",
      "Layer: sequential_2, Trainable: True\n",
      "Layer: decoder, Trainable: True\n",
      "Layer: dropout_2, Trainable: True\n",
      "Layer: decoder_attention_layer, Trainable: True\n",
      "\n",
      "Layer: encoder_att, Trainable: True\n",
      "\n",
      "Layer: sequential_3, Trainable: True\n",
      "Layer: add_meta, Trainable: True\n",
      "Layer: repeat, Trainable: True\n",
      "\n",
      "Layer: sequential_4, Trainable: True\n",
      "Layer: time_dense, Trainable: True\n",
      "Layer: activation, Trainable: True\n",
      "Layer: out, Trainable: True\n"
     ]
    }
   ],
   "source": [
    "release_model(model, optimizer_config = {\"learning_rate\": 1e-4})\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 23s 2s/step - loss: 0.1368 - masked_pearson_correlation_distance: 0.1454 - val_loss: 0.1171 - val_masked_pearson_correlation_distance: 0.1340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7f60490b20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train again while model is released again\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 0 stayed the same: False\n",
      "weights 1 stayed the same: False\n",
      "weights 2 stayed the same: False\n",
      "weights 3 stayed the same: False\n",
      "weights 4 stayed the same: False\n",
      "weights 5 stayed the same: False\n",
      "weights 6 stayed the same: False\n",
      "weights 7 stayed the same: False\n",
      "weights 8 stayed the same: False\n",
      "weights 9 stayed the same: False\n",
      "weights 10 stayed the same: False\n",
      "weights 11 stayed the same: False\n",
      "weights 12 stayed the same: False\n",
      "weights 13 stayed the same: False\n",
      "weights 14 stayed the same: False\n",
      "weights 15 stayed the same: False\n",
      "weights 16 stayed the same: False\n",
      "weights 17 stayed the same: False\n",
      "weights 18 stayed the same: False\n",
      "weights 19 stayed the same: False\n",
      "weights 20 stayed the same: False\n"
     ]
    }
   ],
   "source": [
    "# check which weights have changed\n",
    "retrained_weights = model.get_weights()\n",
    "for i, w in enumerate(zip(original_weights, retrained_weights)):\n",
    "    print(f'weights {i} stayed the same: {(w[0]==w[1]).all()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the utils script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau,\n",
    "    LambdaCallback, TerminateOnNaN, CSVLogger\n",
    ")\n",
    "from wandb.integration.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model: PrositIntensityPredictor, new_optimizer: tf.keras.optimizers, loss: dlomix.losses = masked_spectral_distance, metrics: list = [masked_pearson_correlation_distance]) -> None:\n",
    "    '''Unfreezes all layers of a PrositIntensityPredictor model.\n",
    "\n",
    "    Sets the trainable attribute of every layer to 'True'.\n",
    "    Finally, compiles the model with the optimizer, loss, and metrics to make the changes take effect.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model                   : dlomix.models.prosit.PrositIntensityPredictor\n",
    "                              The model to be unfrozen.\n",
    "    optimizer               : tf.keras.optimizers\n",
    "                              The optimizer is needed for compiling the model.\n",
    "    loss                    : dlomix.losses\n",
    "                              The loss for compiling the model.\n",
    "                              default: masked_spectral_distance\n",
    "    metrics                 : list[dlomix.losses]\n",
    "                              The metrics for compiling the model.\n",
    "                              default: [masked_pearson_correlation_distance] \n",
    "    --------\n",
    "    '''\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    for lay in model.layers:\n",
    "        try:\n",
    "            for sublay in lay.layers:\n",
    "                sublay.trainable = True\n",
    "        except (AttributeError):\n",
    "            lay.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        # optimizer=new_optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    print(\"model compiled successfully\")\n",
    "\n",
    "\n",
    "def release_model_at_epoch(model: PrositIntensityPredictor, release_epoch: int, optimizer_config: dict = {\"learning_rate\":1e-4}, loss: dlomix.losses = masked_spectral_distance, metrics: list = [masked_pearson_correlation_distance]) -> LambdaCallback:\n",
    "    '''Releases the layers of the model at a specific epoch.\n",
    "    Will create a new optimizer to match the new trainable dimensions when defined epoch is reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model                   : dlomix.models.prosit.PrositIntensityPredictor\n",
    "                              The model to be released.\n",
    "    release_epoch           : int\n",
    "                              The epoch at which to unfreeze the model.\n",
    "    loss                    : dlomix.losses\n",
    "                              The loss for compiling the model.\n",
    "                              default: masked_spectral_distance\n",
    "    metrics                 : list[dlomix.losses]\n",
    "                              The metrics for compiling the model.\n",
    "                              default: [masked_pearson_correlation_distance] \n",
    "    --------\n",
    "    '''\n",
    "\n",
    "    def on_epoch_end(epoch, logs=None):\n",
    "        if epoch == release_epoch:\n",
    "            # optimizer = tf.keras.optimizers.Adam(**optimizer_config)\n",
    "            unfreeze_model(model, optimizer, loss, metrics)\n",
    "            print(f\"Model unfrozen at epoch {epoch + 1}\")\n",
    "\n",
    "    return LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'name': 'noptm_baseline_small_bs1024', 'hf_home': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets', 'hf_cache': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/hf_cache', 'parquet_path': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean', 'processed_path': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024', 'seq_length': 30, 'batch_size': 1024}, 'training': {'learning_rate': 0.0001, 'num_epochs': 2}, 'model': {'save_dir': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/callback_models'}, 'processing': {'num_proc': 40}, 'callbacks': {'early_stopping': {'monitor': 'val_loss', 'min_delta': 0.001, 'patience': 20, 'restore_best_weights': True}, 'reduce_lr': {'monitor': 'val_loss', 'factor': 0.1, 'patience': 10, 'min_lr': '1e-6', 'mode': 'auto', 'min_delta': '1e-4', 'verbose': 1}, 'learning_rate_scheduler': {'initial_lr': 0.0001, 'decay_rate': 0.9}, 'lambda_callback': {'on_epoch_end': \"lambda epoch, logs: print(f'Epoch {epoch} ended with logs: {logs}')\"}, 'csv_logger': {'filename': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/logs/training_log.csv', 'append': False}, 'model_checkpoint': {'filepath': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/checkpoints/model-{epoch:02d}-{val_loss:.2f}.keras', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': False, 'mode': 'auto', 'save_freq': 'epoch', 'verbose': 1}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Manually specify the path to the configuration file (Note: change path according to your directories)\n",
    "config_file_path = '/nfs/home/students/l.willruth/mapra/dlomix/bmpc_shared_scripts/baseline_training/config_files/baseline_noptm_baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "\n",
    "# Show config containing the configuration data\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights and biases for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dimalxgi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-oath-1</strong> at: <a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/dimalxgi' target=\"_blank\">https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/dimalxgi</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test' target=\"_blank\">https://wandb.ai/mapra_dlomix/releasing%20layers%20test</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240625_073628-dimalxgi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dimalxgi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/students/l.willruth/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning/tutorials/wandb/run-20240625_074328-9cqotfxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/9cqotfxn' target=\"_blank\">lunar-dawn-2</a></strong> to <a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test' target=\"_blank\">https://wandb.ai/mapra_dlomix/releasing%20layers%20test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/9cqotfxn' target=\"_blank\">https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/9cqotfxn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mapra_dlomix/releasing%20layers%20test/runs/9cqotfxn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4db49957e0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = f'releasing layers test'\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config=config,\n",
    "    entity = 'mapra_dlomix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer: embedding, Trainable: False\n",
      "\n",
      "Layer: sequential, Trainable: True\n",
      "Layer: bidirectional, Trainable: False\n",
      "Layer: dropout, Trainable: False\n",
      "Layer: gru_1, Trainable: False\n",
      "Layer: dropout_1, Trainable: False\n",
      "\n",
      "Layer: sequential_1, Trainable: True\n",
      "Layer: meta_in, Trainable: False\n",
      "Layer: meta_dense, Trainable: False\n",
      "Layer: meta_dense_do, Trainable: False\n",
      "\n",
      "Layer: sequential_2, Trainable: True\n",
      "Layer: decoder, Trainable: False\n",
      "Layer: dropout_2, Trainable: False\n",
      "Layer: decoder_attention_layer, Trainable: False\n",
      "\n",
      "Layer: encoder_att, Trainable: False\n",
      "\n",
      "Layer: sequential_3, Trainable: True\n",
      "Layer: add_meta, Trainable: False\n",
      "Layer: repeat, Trainable: False\n",
      "\n",
      "Layer: sequential_4, Trainable: True\n",
      "Layer: time_dense, Trainable: False\n",
      "Layer: activation, Trainable: False\n",
      "Layer: out, Trainable: False\n"
     ]
    }
   ],
   "source": [
    "freeze_model(model,optimizer, trainable_first_layer=False, trainable_last_layer=False)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 13s 1s/step - loss: 0.1294 - masked_pearson_correlation_distance: 0.1429 - val_loss: 0.1161 - val_masked_pearson_correlation_distance: 0.1334\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1301 - masked_pearson_correlation_distance: 0.1434model compiled successfully\n",
      "Model unfrozen at epoch 2\n",
      "8/8 [==============================] - 6s 834ms/step - loss: 0.1301 - masked_pearson_correlation_distance: 0.1434 - val_loss: 0.1161 - val_masked_pearson_correlation_distance: 0.1334\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_val_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_batch_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mrelease_model_at_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelease_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlomix/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dlomix/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=3,\n",
    "    callbacks=[WandbCallback(save_model=False, log_batch_frequency=True), \n",
    "               release_model_at_epoch(model, release_epoch=1)\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer: embedding, Trainable: True\n",
      "\n",
      "Layer: sequential, Trainable: True\n",
      "Layer: bidirectional, Trainable: True\n",
      "Layer: dropout, Trainable: True\n",
      "Layer: gru_1, Trainable: True\n",
      "Layer: dropout_1, Trainable: True\n",
      "\n",
      "Layer: sequential_1, Trainable: True\n",
      "Layer: meta_in, Trainable: True\n",
      "Layer: meta_dense, Trainable: True\n",
      "Layer: meta_dense_do, Trainable: True\n",
      "\n",
      "Layer: sequential_2, Trainable: True\n",
      "Layer: decoder, Trainable: True\n",
      "Layer: dropout_2, Trainable: True\n",
      "Layer: decoder_attention_layer, Trainable: True\n",
      "\n",
      "Layer: encoder_att, Trainable: True\n",
      "\n",
      "Layer: sequential_3, Trainable: True\n",
      "Layer: add_meta, Trainable: True\n",
      "Layer: repeat, Trainable: True\n",
      "\n",
      "Layer: sequential_4, Trainable: True\n",
      "Layer: time_dense, Trainable: True\n",
      "Layer: activation, Trainable: True\n",
      "Layer: out, Trainable: True\n"
     ]
    }
   ],
   "source": [
    "check_trainability(model, sublayers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

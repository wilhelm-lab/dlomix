{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for freezing the Prosit Model\n",
    "This tutorial shows, how to freeze the Prosit Intensity Predictor model and only let the first and last layer remain trainable for refinement and transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 12:30:14.892621: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 12:30:14.892658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 12:30:14.893911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 12:30:14.901013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 12:30:16.230077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import dlomix\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/l.willruth/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 12:30:19.274046: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  928       \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582762 (13.67 MB)\n",
      "Trainable params: 3582762 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/baseline_models/noptm_baseline_full_bs1024/\"\n",
    "model = tf.keras.models.load_model(model_path + \"85c6c918-4a2a-42e5-aab1-e666121c69a6.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to freeze all layers except first and/or last layer\n",
    "def freeze_model(model:dlomix.models.prosit.PrositIntensityPredictor, trainable_first_layer:bool = False, trainable_last_layer:bool = False) -> None:\n",
    "    \n",
    "    # reset everything to trainable, 'model.trainable = False' overshadowes trainable arguments of sublayers\n",
    "    model.trainable = True \n",
    "    \n",
    "    # go through layers and set trainable to False at lowest level so trainable argument is not overshadowed\n",
    "    for lay in model.layers:\n",
    "        try:\n",
    "            for sublay in lay.layers:\n",
    "                sublay.trainable = False\n",
    "        except (AttributeError):\n",
    "            lay.trainable = False\n",
    "\n",
    "    if (trainable_first_layer):\n",
    "        first_layer = model.get_layer(name=\"embedding\")\n",
    "        first_layer.trainable = True\n",
    "\n",
    "    if (trainable_last_layer):\n",
    "        last_layer = model.get_layer(name = \"sequential_4\").get_layer(name = \"time_dense\")\n",
    "        last_layer.trainable = True\n",
    "\n",
    "    # compile the model again to make changes take effect\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=masked_spectral_distance,\n",
    "        metrics=[masked_pearson_correlation_distance]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print the trainable attribute of every layer\n",
    "def check_trainability(model, sublayers = False):\n",
    "    for lay in model.layers:\n",
    "        if(sublayers):\n",
    "            print()\n",
    "            try:\n",
    "                lay.layers\n",
    "                print(f'Sequential {lay} trainable: {lay.trainable}')\n",
    "                for lay2 in lay.layers:\n",
    "                    print(f'{lay2} trainable: {lay2.trainable}')\n",
    "            except(AttributeError):\n",
    "                print(f'{lay} trainable: {lay.trainable}')\n",
    "        else:\n",
    "            print(f'{lay} trainable: {lay.trainable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Freeze all layers except the first and the last layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<keras.src.layers.core.embedding.Embedding object at 0x7fa0c2f23880> trainable: True\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c13c0> trainable: True\n",
      "<keras.src.layers.rnn.bidirectional.Bidirectional object at 0x7fa0c2f230d0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c03a0> trainable: False\n",
      "<keras.src.layers.rnn.gru.GRU object at 0x7fa0b87c06a0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c1090> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c2320> trainable: True\n",
      "<keras.src.layers.merging.concatenate.Concatenate object at 0x7fa0b87c18d0> trainable: False\n",
      "<keras.src.layers.core.dense.Dense object at 0x7fa0b87c1c60> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c2050> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c3820> trainable: True\n",
      "<keras.src.layers.rnn.gru.GRU object at 0x7fa0b87c26e0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c33a0> trainable: False\n",
      "<dlomix.layers.attention.DecoderAttentionLayer object at 0x7fa0b87c3670> trainable: False\n",
      "\n",
      "<dlomix.layers.attention.AttentionLayer object at 0x7fa0b87c3be0> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b85fc5e0> trainable: True\n",
      "<keras.src.layers.merging.multiply.Multiply object at 0x7fa0b85fc1f0> trainable: False\n",
      "<keras.src.layers.reshaping.repeat_vector.RepeatVector object at 0x7fa0b85fc3d0> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b85fd630> trainable: True\n",
      "<keras.src.layers.rnn.time_distributed.TimeDistributed object at 0x7fa0b85fd030> trainable: True\n",
      "<keras.src.layers.activation.leaky_relu.LeakyReLU object at 0x7fa0b85fd1b0> trainable: False\n",
      "<keras.src.layers.reshaping.flatten.Flatten object at 0x7fa0b85fd3c0> trainable: False\n"
     ]
    }
   ],
   "source": [
    "freeze_model(model, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare everything for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset and the PTM alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(\"/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Training with frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<keras.src.layers.core.embedding.Embedding object at 0x7fa0c2f23880> trainable: True\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c13c0> trainable: True\n",
      "<keras.src.layers.rnn.bidirectional.Bidirectional object at 0x7fa0c2f230d0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c03a0> trainable: False\n",
      "<keras.src.layers.rnn.gru.GRU object at 0x7fa0b87c06a0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c1090> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c2320> trainable: True\n",
      "<keras.src.layers.merging.concatenate.Concatenate object at 0x7fa0b87c18d0> trainable: False\n",
      "<keras.src.layers.core.dense.Dense object at 0x7fa0b87c1c60> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c2050> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b87c3820> trainable: True\n",
      "<keras.src.layers.rnn.gru.GRU object at 0x7fa0b87c26e0> trainable: False\n",
      "<keras.src.layers.regularization.dropout.Dropout object at 0x7fa0b87c33a0> trainable: False\n",
      "<dlomix.layers.attention.DecoderAttentionLayer object at 0x7fa0b87c3670> trainable: False\n",
      "\n",
      "<dlomix.layers.attention.AttentionLayer object at 0x7fa0b87c3be0> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b85fc5e0> trainable: True\n",
      "<keras.src.layers.merging.multiply.Multiply object at 0x7fa0b85fc1f0> trainable: False\n",
      "<keras.src.layers.reshaping.repeat_vector.RepeatVector object at 0x7fa0b85fc3d0> trainable: False\n",
      "\n",
      "Sequential <keras.src.engine.sequential.Sequential object at 0x7fa0b85fd630> trainable: True\n",
      "<keras.src.layers.rnn.time_distributed.TimeDistributed object at 0x7fa0b85fd030> trainable: True\n",
      "<keras.src.layers.activation.leaky_relu.LeakyReLU object at 0x7fa0b85fd1b0> trainable: False\n",
      "<keras.src.layers.reshaping.flatten.Flatten object at 0x7fa0b85fd3c0> trainable: False\n"
     ]
    }
   ],
   "source": [
    "freeze_model(model, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 22s 2s/step - loss: 0.1377 - masked_pearson_correlation_distance: 0.1461 - val_loss: 0.1177 - val_masked_pearson_correlation_distance: 0.1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa0a011ca30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train again while only the first layer and the last layer are trainable\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 0 stayed the same: False\n",
      "weights 1 stayed the same: True\n",
      "weights 2 stayed the same: True\n",
      "weights 3 stayed the same: True\n",
      "weights 4 stayed the same: True\n",
      "weights 5 stayed the same: True\n",
      "weights 6 stayed the same: True\n",
      "weights 7 stayed the same: True\n",
      "weights 8 stayed the same: True\n",
      "weights 9 stayed the same: True\n",
      "weights 10 stayed the same: True\n",
      "weights 11 stayed the same: True\n",
      "weights 12 stayed the same: True\n",
      "weights 13 stayed the same: True\n",
      "weights 14 stayed the same: True\n",
      "weights 15 stayed the same: True\n",
      "weights 16 stayed the same: True\n",
      "weights 17 stayed the same: True\n",
      "weights 18 stayed the same: True\n",
      "weights 19 stayed the same: False\n",
      "weights 20 stayed the same: False\n"
     ]
    }
   ],
   "source": [
    "# check which weights have changed\n",
    "retrained_weights = model.get_weights()\n",
    "for i, w in enumerate(zip(original_weights, retrained_weights)):\n",
    "    print(f'weights {i} stayed the same: {(w[0]==w[1]).all()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two weight tensors changed for the last layer. Both tensors belong to the last time_dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15456411 0.05034332 0.01185355 0.07343634 0.02754287 0.00198995]\n",
      "512\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "3078\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_dense (TimeDistribute  (None, 29, 6)             3078      \n",
      " d)                                                              \n",
      "                                                                 \n",
      " activation (LeakyReLU)      (None, 29, 6)             0         \n",
      "                                                                 \n",
      " out (Flatten)               (None, 174)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3078 (12.02 KB)\n",
      "Trainable params: 3078 (12.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(retrained_weights[20]) # 6\n",
    "print(len(retrained_weights[19])) # 512\n",
    "print([len(x) for x in retrained_weights[19]]) # 6\n",
    "print(512 * 6 + 6) # 3078\n",
    "print(model.get_layer(name=\"sequential_4\").summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Freezing Strategies for Refinement/Transfer Learning\n",
    "This script is for trying out different freezing mechanisms and controls using tensorflow and keras. Goal is to have some easy access functions that we can use for further training, refinement and transfer learning for the Prosit Models and ultimately implement in DLOmix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlomix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import yaml\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stuff for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"baseline_training/config_files/noptm_baseline_small_bs1024.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset and the PTM alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "\n",
    "# from misc import PTMS_ALPHABET\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "\n",
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(config['dataset']['processed_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the optimizer and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['training']['learning_rate'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=20,\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.models import PrositIntensityPredictor\n",
    "\n",
    "input_mapping = {\n",
    "        \"SEQUENCE_KEY\": \"modified_sequence\",\n",
    "        \"COLLISION_ENERGY_KEY\": \"collision_energy_aligned_normed\",\n",
    "        \"PRECURSOR_CHARGE_KEY\": \"precursor_charge_onehot\",\n",
    "        \"FRAGMENTATION_TYPE_KEY\": \"method_nbr\",\n",
    "    }\n",
    "\n",
    "meta_data_keys=[\"collision_energy_aligned_normed\", \"precursor_charge_onehot\", \"method_nbr\"]\n",
    "\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=config['dataset']['seq_length'],\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False,\n",
    "    input_keys=input_mapping,\n",
    "    meta_data_keys=meta_data_keys\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=config['training']['num_epochs'],\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if weights are correctly assigned to trainable/non-trainable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'layers with weigths: {len(model.weights)}')\n",
    "print(f'layers with trainable weigths: {len(model.trainable_weights)}')\n",
    "print(f'layers with non-trainable weigths: {len(model.non_trainable_weights)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze the whole model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'layers with weigths: {len(model.weights)}')\n",
    "print(f'layers with trainable weigths: {len(model.trainable_weights)}')\n",
    "print(f'layers with non-trainable weigths: {len(model.non_trainable_weights)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze the model and only keep the first and/or the last layer trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to freeze all layers except first and/or last layer\n",
    "def freeze_model(model:dlomix.models.prosit.PrositIntensityPredictor, trainable_first_layer:bool = False, trainable_last_layer:bool = False) -> None:\n",
    "    \n",
    "    # reset everything to trainable, 'model.trainable = False' overshadowes trainable arguments of sublayers\n",
    "    model.trainable = True \n",
    "    \n",
    "    # go through layers and set trainable to False at lowest level so trainable argument is not overshadowed\n",
    "    for lay in model.layers:\n",
    "        try:\n",
    "            for sublay in lay.layers:\n",
    "                sublay.trainable = False\n",
    "        except (AttributeError):\n",
    "            lay.trainable = False\n",
    "\n",
    "    if (trainable_first_layer):\n",
    "        first_layer = model.get_layer(name=\"embedding\")\n",
    "        first_layer.trainable = True\n",
    "\n",
    "    if (trainable_last_layer):\n",
    "        last_layer = model.get_layer(name = \"sequential_4\").get_layer(name = \"time_dense\")\n",
    "        last_layer.trainable = True\n",
    "\n",
    "    # compile the model again to make changes take effect\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=masked_spectral_distance,\n",
    "        metrics=[masked_pearson_correlation_distance]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainability(model, sublayers = False):\n",
    "    for lay in model.layers:\n",
    "        if(sublayers):\n",
    "            print()\n",
    "            try:\n",
    "                lay.layers\n",
    "                print(f'Sequential {lay} trainable: {lay.trainable}')\n",
    "                for lay2 in lay.layers:\n",
    "                    print(f'{lay2} trainable: {lay2.trainable}')\n",
    "            except(AttributeError):\n",
    "                print(f'{lay} trainable: {lay.trainable}')\n",
    "        else:\n",
    "            print(f'{lay} trainable: {lay.trainable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Freeze all layers except the first layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model(model, trainable_first_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Freeze all layers except the last layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model(model, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Freeze all layers except the first and the last layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model(model, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Training with frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model(model, trainable_first_layer=True, trainable_last_layer=True)\n",
    "check_trainability(model, sublayers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train again while only the first layer and the last layer are trainable\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which weights have changed\n",
    "retrained_weights = model.get_weights()\n",
    "for i, w in enumerate(zip(original_weights, retrained_weights)):\n",
    "    print(f'weights {i} stayed the same: {(w[0]==w[1]).all()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two weight tensors changed for the last layer. Both tensors belong to the last time_dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrained_weights[20]) # 6\n",
    "print(len(retrained_weights[19])) # 512\n",
    "print([len(x) for x in retrained_weights[19]]) # 6\n",
    "print(512 * 6 + 6)\n",
    "print(model.get_layer(name=\"sequential_4\").summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

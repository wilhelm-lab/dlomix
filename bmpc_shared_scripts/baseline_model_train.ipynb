{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Baseline Model\n",
    "\n",
    "## Initialization steps\n",
    "\n",
    "### local dlomix package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this repo's code as dlomix package\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..')) + '/src'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "project_name = 'baseline model'\n",
    "wandb.init(project=project_name)\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "config.seq_length = 30\n",
    "config.batch_size = 2\n",
    "# config.val_ratio = 0.2\n",
    "config.learning_rate = 1.0e-4\n",
    "config.epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset\n",
    "\n",
    "*Here, I currently import the \"small\" dataset that Joel had in his script.*\n",
    "\n",
    "**TODO:** Find baseline dataset and load that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "\n",
    "# from misc import PTMS_ALPHABET\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "\n",
    "# path to dataset\n",
    "datset_base_path = \"/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean\"\n",
    "dataset_train_path = f\"{datset_base_path}_train.parquet\"\n",
    "dataset_val_path = f\"{datset_base_path}_val.parquet\"\n",
    "dataset_test_path = f\"{datset_base_path}_test.parquet\"\n",
    "\n",
    "dataset = FragmentIonIntensityDataset(\n",
    "    data_source=dataset_train_path,\n",
    "    val_data_source=dataset_val_path,\n",
    "    test_data_source=dataset_test_path,\n",
    "    data_format=\"parquet\", \n",
    "    # val_ratio=config.val_ratio, # why do we need this if we already have splits?\n",
    "    batch_size=config.batch_size,\n",
    "    max_seq_len=config.seq_length,\n",
    "    encoding_scheme=\"naive-mods\",\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    model_features=[]\n",
    "    # model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\",\"method_nbr\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize relevant stuff for training\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=20,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "\n",
    "input_mapping = {\n",
    "    \"SEQUENCE_KEY\": \"modified_sequence\"\n",
    "}\n",
    "\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=config.seq_length,\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False\n",
    "    # input_keys=input_mapping\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=[WandbCallback(), early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

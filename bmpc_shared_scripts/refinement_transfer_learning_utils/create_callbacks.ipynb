{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Creating Callbacks for Refinement/Transfer Learning\n",
    "This script is for trying out different callback mechanisms using tensorflow and keras. Goal is to have some easy access functions that we can use for further training, refinement and transfer learning for the Prosit Models and ultimately implement in DLOmix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# Parsing the configuration file (required when using a script instead of a notebook)\n",
    "# parser = argparse.ArgumentParser(prog='Baseline Model Training')\n",
    "# parser.add_argument('--config', type=str, required=True)\n",
    "# parser.add_argument('--tf-device-nr', type=str, required=True)\n",
    "# args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'name': 'noptm_baseline_small_bs1024', 'hf_home': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets', 'hf_cache': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/hf_cache', 'parquet_path': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean', 'processed_path': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024', 'seq_length': 30, 'batch_size': 1024}, 'training': {'learning_rate': 0.0001, 'num_epochs': 2}, 'model': {'save_dir': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/models/callback_models'}, 'processing': {'num_proc': 40}, 'callbacks': {'early_stopping': {'monitor': 'val_loss', 'min_delta': 0.001, 'patience': 20, 'restore_best_weights': True}, 'model_checkpoint': {'filepath': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/checkpoints/model-{epoch:02d}-{val_loss:.2f}.hdf5', 'monitor': 'val_loss', 'save_best_only': False, 'save_weights_only': True, 'mode': 'auto', 'save_freq': 'epoch', 'verbose': 1}, 'reduce_lr': {'monitor': 'val_loss', 'factor': 0.1, 'patience': 10, 'min_lr': '1e-6', 'mode': 'auto', 'min_delta': '1e-4', 'verbose': 1}, 'lambda_callback': {'on_epoch_end': \"lambda epoch, logs: print(f'Epoch {epoch} ended with logs: {logs}')\"}, 'csv_logger': {'filename': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/logs/training_log.csv', 'append': False}, 'learning_rate_scheduler': {'initial_lr': 0.0001, 'decay_rate': 0.9}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Manually specify the path to the configuration file\n",
    "config_file_path = '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/config_files/baseline_noptm_baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "\n",
    "# Show config containing the configuration data\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = config['dataset']['hf_home']\n",
    "os.environ['HF_DATASETS_CACHE'] = config['dataset']['hf_cache']\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.tf_device_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4ko0v3u1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84aaba5a3e83441286a29bcc2e53f749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-field-26</strong> at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/4ko0v3u1' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/4ko0v3u1</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_145113-4ko0v3u1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4ko0v3u1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/wandb/run-20240605_150057-at05auxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr' target=\"_blank\">firm-waterfall-27</a></strong> to <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe5fcce7040>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "# initialize weights and biases\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "\n",
    "config['run_id'] = uuid.uuid4()\n",
    "\n",
    "project_name = f'callback model training'\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config=config,\n",
    "    tags=[config['dataset']['name']], \n",
    "    entity = 'mapra_dlomix'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "# DLOmix dataset \n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "\n",
    "# Own dataset\n",
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(wandb.config['dataset']['processed_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow and the optimizer\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=wandb.config['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau,\n",
    "    LambdaCallback, TerminateOnNaN, CSVLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=wandb.config['callbacks']['early_stopping']['monitor'],\n",
    "    min_delta=wandb.config['callbacks']['early_stopping']['min_delta'],\n",
    "    patience=wandb.config['callbacks']['early_stopping']['patience'],\n",
    "    restore_best_weights=wandb.config['callbacks']['early_stopping']['restore_best_weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint Callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=wandb.config['callbacks']['model_checkpoint']['filepath'],\n",
    "    monitor=wandb.config['callbacks']['model_checkpoint']['monitor'],\n",
    "    save_best_only=wandb.config['callbacks']['model_checkpoint']['save_best_only'],\n",
    "    save_weights_only=wandb.config['callbacks']['model_checkpoint']['save_weights_only'],\n",
    "    mode=wandb.config['callbacks']['model_checkpoint']['mode'],\n",
    "    save_freq=wandb.config['callbacks']['model_checkpoint']['save_freq'],\n",
    "    verbose=wandb.config['callbacks']['model_checkpoint']['verbose']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce LR on Plateau Callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=wandb.config['callbacks']['reduce_lr']['monitor'],\n",
    "    factor=wandb.config['callbacks']['reduce_lr']['factor'],\n",
    "    patience=wandb.config['callbacks']['reduce_lr']['patience'],\n",
    "    min_lr=wandb.config['callbacks']['reduce_lr']['min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger Callback\n",
    "csv_logger = CSVLogger(\n",
    "    filename=wandb.config['callbacks']['csv_logger']['filename']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler Callback\n",
    "learning_rate_scheduler = LearningRateScheduler(\n",
    "    schedule=lambda epoch: wandb.config['callbacks']['learning_rate_scheduler']['initial_lr'] * wandb.config['callbacks']['learning_rate_scheduler']['decay_rate'] ** epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate On NaN Callback: Callback that terminates training when a NaN loss is encountered.\n",
    "terminate_on_nan = TerminateOnNaN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Callback (example: logging epoch start)\n",
    "lambda_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"Starting epoch {epoch}\"), \n",
    "    on_epoch_end=None,\n",
    "    on_train_begin=None,\n",
    "    on_train_end=None,\n",
    "    on_train_batch_begin=None,\n",
    "    on_train_batch_end=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "\n",
    "input_mapping = {\n",
    "    \"SEQUENCE_KEY\": \"modified_sequence\",\n",
    "    \"COLLISION_ENERGY_KEY\": \"collision_energy_aligned_normed\",\n",
    "    \"PRECURSOR_CHARGE_KEY\": \"precursor_charge_onehot\",\n",
    "    \"FRAGMENTATION_TYPE_KEY\": \"method_nbr\",\n",
    "}\n",
    "\n",
    "meta_data_keys = [\"collision_energy_aligned_normed\", \"precursor_charge_onehot\", \"method_nbr\"]\n",
    "\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=wandb.config['dataset']['seq_length'],\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False,\n",
    "    input_keys=input_mapping,\n",
    "    meta_data_keys=meta_data_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8/8 [==============================] - 48s 5s/step - loss: 0.7181 - masked_pearson_correlation_distance: 0.6237 - val_loss: 0.6724 - val_masked_pearson_correlation_distance: 0.5615 - lr: 1.0000e-04\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 31s 4s/step - loss: 0.6695 - masked_pearson_correlation_distance: 0.5586 - val_loss: 0.6679 - val_masked_pearson_correlation_distance: 0.5555 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe5fcce52a0>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=wandb.config['training']['num_epochs'],\n",
    "    callbacks=[WandbCallback(save_model=False, log_batch_frequency=True), \n",
    "               early_stopping, \n",
    "               reduce_lr, \n",
    "            #  learning_rate_scheduler,           \n",
    "               terminate_on_nan, \n",
    "            #  lambda_callback, \n",
    "            #  csv_logger,   \n",
    "            #  model_checkpoint\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{wandb.config['model']['save_dir']}/{wandb.config['dataset']['name']}/{wandb.config['run_id']}.keras\"\n",
    "\n",
    "model.save(model_path)  # The file needs to end with the .keras extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a92ba18609143d38905ba4036255ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.66794</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.6695</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.55861</td></tr><tr><td>val_loss</td><td>0.66794</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.55546</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-waterfall-27</strong> at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training/runs/at05auxr</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/callback%20model%20training' target=\"_blank\">https://wandb.ai/mapra_dlomix/callback%20model%20training</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_150057-at05auxr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "reconstructed_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Output Dimension: 16\n",
      "Sequence Length: 30\n",
      "Alphabet Dictionary: {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, '[]-': 21, '-[]': 22, '[UNIMOD:737]-': 56, 'M[UNIMOD:35]': 23, 'S[UNIMOD:21]': 24, 'T[UNIMOD:21]': 25, 'Y[UNIMOD:21]': 26, 'R[UNIMOD:7]': 27, 'Q[UNIMOD:7]': 4, 'N[UNIMOD:7]': 3, 'K[UNIMOD:1]': 28, 'K[UNIMOD:121]': 29, 'Q[UNIMOD:28]': 30, 'R[UNIMOD:34]': 31, 'K[UNIMOD:34]': 32, 'T[UNIMOD:43]': 35, 'S[UNIMOD:43]': 36, 'C[UNIMOD:4]': 37, '[UNIMOD:1]-': 38, 'E[UNIMOD:27]': 39, 'K[UNIMOD:36]': 40, 'K[UNIMOD:37]': 41, 'K[UNIMOD:122]': 42, 'K[UNIMOD:58]': 43, 'K[UNIMOD:1289]': 44, 'K[UNIMOD:747]': 45, 'K[UNIMOD:64]': 46, 'K[UNIMOD:1848]': 47, 'K[UNIMOD:1363]': 48, 'K[UNIMOD:1849]': 49, 'K[UNIMOD:3]': 50, 'K[UNIMOD:737]': 55, 'R[UNIMOD:36]': 51, 'R[UNIMOD:36a]': 52, 'P[UNIMOD:35]': 53, 'Y[UNIMOD:354]': 54}\n",
      "Dropout Rate: 0.2\n",
      "Latent Dropout Rate: 0.1\n",
      "Recurrent Layers Sizes: (256, 512)\n",
      "Regressor Layer Size: 512\n",
      "Use Prosit PTM Features: False\n",
      "Input Keys: {'SEQUENCE_KEY': 'modified_sequence', 'COLLISION_ENERGY_KEY': 'collision_energy_aligned_normed', 'PRECURSOR_CHARGE_KEY': 'precursor_charge_onehot', 'FRAGMENTATION_TYPE_KEY': 'method_nbr'}\n",
      "Default Input Keys: {'SEQUENCE_KEY': 'sequence', 'COLLISION_ENERGY_KEY': 'collision_energy', 'PRECURSOR_CHARGE_KEY': 'precursor_charge', 'FRAGMENTATION_TYPE_KEY': 'fragmentation_type'}\n",
      "Meta Data Keys (Attribute): ['COLLISION_ENERGY_KEY', 'PRECURSOR_CHARGE_KEY', 'FRAGMENTATION_TYPE_KEY']\n",
      "PTM Input Keys: ['mod_loss', 'delta_mass', 'mod_gain', 'atom_count', 'red_smiles']\n"
     ]
    }
   ],
   "source": [
    "# Model summary \n",
    "\n",
    "# Print parameters\n",
    "print(\"Embedding Output Dimension:\", reconstructed_model.embedding_output_dim)\n",
    "print(\"Sequence Length:\", reconstructed_model.seq_length)\n",
    "print(\"Alphabet Dictionary:\", reconstructed_model.alphabet)\n",
    "print(\"Dropout Rate:\", reconstructed_model.dropout_rate)\n",
    "print(\"Latent Dropout Rate:\", reconstructed_model.latent_dropout_rate)\n",
    "print(\"Recurrent Layers Sizes:\", reconstructed_model.recurrent_layers_sizes)\n",
    "print(\"Regressor Layer Size:\", reconstructed_model.regressor_layer_size)\n",
    "print(\"Use Prosit PTM Features:\", reconstructed_model.use_prosit_ptm_features)\n",
    "print(\"Input Keys:\", reconstructed_model.input_keys)\n",
    "\n",
    "# Print attributes\n",
    "print(\"Default Input Keys:\", reconstructed_model.DEFAULT_INPUT_KEYS)\n",
    "print(\"Meta Data Keys (Attribute):\", reconstructed_model.META_DATA_KEYS)\n",
    "print(\"PTM Input Keys:\", reconstructed_model.PTM_INPUT_KEYS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

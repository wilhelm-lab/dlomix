{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Creating Callbacks for Refinement/Transfer Learning\n",
    "This script is for trying out different callback mechanisms using tensorflow and keras. Goal is to have some easy access functions that we can use for further training, refinement and transfer learning for the Prosit Models and ultimately implement in DLOmix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the configuration file (required when using a script instead of a notebook)\n",
    "#parser = argparse.ArgumentParser(prog='Extended Model Training')\n",
    "#parser.add_argument('--config', type=str, required=True)\n",
    "#args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'hf_home': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets', 'hf_cache': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/hf_cache', 'parquet_path': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean', 'processed_path': '/cmnfs/proj/prosit_astral/bmpc_dlomix_group/datasets/processed/noptm_baseline_small_bs1024', 'seq_length': 30, 'batch_size': 1024}, 'training': {'learning_rate': 0.0001, 'num_epochs': 2}, 'processing': {'num_proc': 40}, 'callbacks': {'early_stopping': {'monitor': 'val_loss', 'min_delta': 0.001, 'patience': 20, 'restore_best_weights': True}, 'model_checkpoint': {'filepath': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/checkpoints/model-{epoch:02d}-{val_loss:.2f}.hdf5', 'monitor': 'val_loss', 'save_best_only': False, 'save_weights_only': True, 'mode': 'auto', 'save_freq': 'epoch', 'verbose': 1}, 'reduce_lr': {'monitor': 'val_loss', 'factor': 0.1, 'patience': 10, 'min_lr': '1e-6', 'mode': 'auto', 'min_delta': '1e-4', 'verbose': 1}, 'lambda_callback': {'on_epoch_end': \"lambda epoch, logs: print(f'Epoch {epoch} ended with logs: {logs}')\"}, 'csv_logger': {'filename': '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/logs/training_log.csv', 'append': False}, 'learning_rate_scheduler': {'initial_lr': 0.0001, 'decay_rate': 0.9}}}\n"
     ]
    }
   ],
   "source": [
    "# Manually specify the path to the configuration file\n",
    "config_file_path = '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/config_files/baseline_noptm_baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "\n",
    "# Show config containing the configuration data\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb for experiment tracking\n",
    "# import wandb\n",
    "# from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# project_name = 'extended_model_training'\n",
    "# wandb.init(project=project_name)\n",
    "# wandb.config = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(config['dataset']['processed_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow and the optimizer\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau,\n",
    "    LambdaCallback, TerminateOnNaN, CSVLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=config['callbacks']['early_stopping']['monitor'],\n",
    "    min_delta=config['callbacks']['early_stopping']['min_delta'],\n",
    "    patience=config['callbacks']['early_stopping']['patience'],\n",
    "    restore_best_weights=config['callbacks']['early_stopping']['restore_best_weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint Callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=config['callbacks']['model_checkpoint']['filepath'],\n",
    "    monitor=config['callbacks']['model_checkpoint']['monitor'],\n",
    "    save_best_only=config['callbacks']['model_checkpoint']['save_best_only'],\n",
    "    save_weights_only=config['callbacks']['model_checkpoint']['save_weights_only'],\n",
    "    mode=config['callbacks']['model_checkpoint']['mode'],\n",
    "    save_freq=config['callbacks']['model_checkpoint']['save_freq'],\n",
    "    verbose=config['callbacks']['model_checkpoint']['verbose']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce LR on Plateau Callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=config['callbacks']['reduce_lr']['monitor'],\n",
    "    factor=config['callbacks']['reduce_lr']['factor'],\n",
    "    patience=config['callbacks']['reduce_lr']['patience'],\n",
    "    min_lr=config['callbacks']['reduce_lr']['min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger Callback\n",
    "csv_logger = CSVLogger(\n",
    "    filename=config['callbacks']['csv_logger']['filename']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler Callback\n",
    "learning_rate_scheduler = LearningRateScheduler(\n",
    "    schedule=lambda epoch: config['callbacks']['learning_rate_scheduler']['initial_lr'] * config['callbacks']['learning_rate_scheduler']['decay_rate'] ** epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate On NaN Callback: Callback that terminates training when a NaN loss is encountered.\n",
    "terminate_on_nan = TerminateOnNaN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Callback (example: logging epoch start)\n",
    "lambda_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"Starting epoch {epoch}\"), \n",
    "    on_epoch_end=None,\n",
    "    on_train_begin=None,\n",
    "    on_train_end=None,\n",
    "    on_train_batch_begin=None,\n",
    "    on_train_batch_end=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "\n",
    "input_mapping = {\n",
    "    \"SEQUENCE_KEY\": \"modified_sequence\",\n",
    "    \"COLLISION_ENERGY_KEY\": \"collision_energy_aligned_normed\",\n",
    "    \"PRECURSOR_CHARGE_KEY\": \"precursor_charge_onehot\",\n",
    "    \"FRAGMENTATION_TYPE_KEY\": \"method_nbr\",\n",
    "}\n",
    "\n",
    "meta_data_keys = [\"collision_energy_aligned_normed\", \"precursor_charge_onehot\", \"method_nbr\"]\n",
    "\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=config['dataset']['seq_length'],\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False,\n",
    "    input_keys=input_mapping,\n",
    "    meta_data_keys=meta_data_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7132 - masked_pearson_correlation_distance: 0.6173\n",
      "Epoch 1: saving model to /nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/checkpoints/model-01-0.67.hdf5\n",
      "8/8 [==============================] - 42s 4s/step - loss: 0.7132 - masked_pearson_correlation_distance: 0.6173 - val_loss: 0.6720 - val_masked_pearson_correlation_distance: 0.5631\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6688 - masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 2: saving model to /nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/saved_models/checkpoints/model-02-0.67.hdf5\n",
      "8/8 [==============================] - 29s 4s/step - loss: 0.6688 - masked_pearson_correlation_distance: 0.5599 - val_loss: 0.6672 - val_masked_pearson_correlation_distance: 0.5566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe600669c00>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with various callbacks\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=config['training']['num_epochs'],\n",
    "    callbacks=[\n",
    "        # WandbCallback(),\n",
    "        early_stopping,\n",
    "        model_checkpoint, \n",
    "        # reduce_lr,\n",
    "        # csv_logger, \n",
    "        # lr_scheduler, \n",
    "        # terminate_on_nan, \n",
    "        # lambda_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the wandb run\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

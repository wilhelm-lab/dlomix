{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Callbacks for Model Refinement and Transfer Learning\n",
    "This script facilitates the exploration of various callback mechanisms within TensorFlow and Keras. The objective is to develop accessible functions that enable further training, refinement, and transfer learning of Prosit Models, with the intention of integrating these into DLOmix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Manually specify the path to the configuration file (Note: change path according to your directories) \n",
    "config_file_path = '/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/config_files/baseline_noptm_baseline_small_bs1024.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "\n",
    "# Show config containing the configuration data\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure environment\n",
    "import os\n",
    "os.environ['HF_HOME'] = config['dataset']['hf_home']\n",
    "os.environ['HF_DATASETS_CACHE'] = config['dataset']['hf_cache']\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.tf_device_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid to ensure unique identifiers\n",
    "import uuid\n",
    "# initialize weights and biases\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# set id for run using uuid\n",
    "config['run_id'] = uuid.uuid4()\n",
    "\n",
    "# set up wandb for this project\n",
    "project_name = f'callback model training'\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config=config,\n",
    "    tags=[config['dataset']['name']], \n",
    "    entity = 'mapra_dlomix'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLOmix dataset \n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "\n",
    "# Own dataset\n",
    "from dlomix.data import load_processed_dataset\n",
    "dataset = load_processed_dataset(wandb.config['dataset']['processed_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow and the optimizer\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=wandb.config['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau,\n",
    "    LambdaCallback, TerminateOnNaN, CSVLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=wandb.config['callbacks']['early_stopping']['monitor'],\n",
    "    min_delta=wandb.config['callbacks']['early_stopping']['min_delta'],\n",
    "    patience=wandb.config['callbacks']['early_stopping']['patience'],\n",
    "    restore_best_weights=wandb.config['callbacks']['early_stopping']['restore_best_weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce LR on Plateau Callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=wandb.config['callbacks']['reduce_lr']['monitor'],\n",
    "    factor=wandb.config['callbacks']['reduce_lr']['factor'],\n",
    "    patience=wandb.config['callbacks']['reduce_lr']['patience'],\n",
    "    min_lr=wandb.config['callbacks']['reduce_lr']['min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler Callback\n",
    "learning_rate_scheduler = LearningRateScheduler(\n",
    "    schedule=lambda epoch: wandb.config['callbacks']['learning_rate_scheduler']['initial_lr'] * wandb.config['callbacks']['learning_rate_scheduler']['decay_rate'] ** epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate On NaN Callback: Callback that terminates training when a NaN loss is encountered.\n",
    "terminate_on_nan = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Callback \n",
    "lambda_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"Starting epoch {epoch + 1}\")\n",
    "    # on_epoch_end=None,\n",
    "    # on_train_begin=None,\n",
    "    # on_train_end=None,\n",
    "    # on_train_batch_begin=None,\n",
    "    # on_train_batch_end=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger Callback (Note: not necessary when using wandb)\n",
    "csv_logger = CSVLogger(\n",
    "    filename=wandb.config['callbacks']['csv_logger']['filename'], \n",
    "    append=wandb.config['callbacks']['csv_logger']['append']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint Callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=wandb.config['callbacks']['model_checkpoint']['filepath'],\n",
    "    monitor=wandb.config['callbacks']['model_checkpoint']['monitor'],\n",
    "    save_best_only=wandb.config['callbacks']['model_checkpoint']['save_best_only'],\n",
    "    save_weights_only=wandb.config['callbacks']['model_checkpoint']['save_weights_only'],\n",
    "    mode=wandb.config['callbacks']['model_checkpoint']['mode'],\n",
    "    save_freq=wandb.config['callbacks']['model_checkpoint']['save_freq'],\n",
    "    verbose=wandb.config['callbacks']['model_checkpoint']['verbose']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "from dlomix.models import PrositIntensityPredictor # predictor for intensity \n",
    "from dlomix.constants import PTMS_ALPHABET # alphabet with PTMs (can be adapted based on the data)\n",
    "\n",
    "input_mapping = {\n",
    "    \"SEQUENCE_KEY\": \"modified_sequence\",\n",
    "    \"COLLISION_ENERGY_KEY\": \"collision_energy_aligned_normed\",\n",
    "    \"PRECURSOR_CHARGE_KEY\": \"precursor_charge_onehot\",\n",
    "    \"FRAGMENTATION_TYPE_KEY\": \"method_nbr\",\n",
    "}\n",
    "\n",
    "meta_data_keys = [\"collision_energy_aligned_normed\", \"precursor_charge_onehot\", \"method_nbr\"]\n",
    "\n",
    "# initialize prosit model\n",
    "model = PrositIntensityPredictor(\n",
    "    seq_length=wandb.config['dataset']['seq_length'],\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    use_prosit_ptm_features=False,\n",
    "    with_termini=False,\n",
    "    input_keys=input_mapping,\n",
    "    meta_data_keys=meta_data_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_spectral_distance,\n",
    "    metrics=[masked_pearson_correlation_distance]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.fit(\n",
    "    dataset.tensor_train_data,\n",
    "    validation_data=dataset.tensor_val_data,\n",
    "    epochs=wandb.config['training']['num_epochs'],\n",
    "    callbacks=[WandbCallback(save_model=False, log_batch_frequency=True), \n",
    "               early_stopping, \n",
    "               reduce_lr, \n",
    "               learning_rate_scheduler,           \n",
    "               terminate_on_nan, \n",
    "               lambda_callback, \n",
    "               csv_logger, # (Note: not necessary when using wandb; shown for completeness)\n",
    "               model_checkpoint\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path to save to the model to (Note: The file needs to end with the .keras extension.)\n",
    "model_path = f\"{wandb.config['model']['save_dir']}/{wandb.config['dataset']['name']}/{wandb.config['run_id']}.keras\"\n",
    "\n",
    "# save the model\n",
    "model.save(model_path)  \n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# load the trained model \n",
    "reconstructed_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary \n",
    "\n",
    "# Print parameters\n",
    "print(\"Embedding Output Dimension:\", reconstructed_model.embedding_output_dim)\n",
    "print(\"Sequence Length:\", reconstructed_model.seq_length)\n",
    "print(\"Alphabet Dictionary:\", reconstructed_model.alphabet)\n",
    "print(\"Dropout Rate:\", reconstructed_model.dropout_rate)\n",
    "print(\"Latent Dropout Rate:\", reconstructed_model.latent_dropout_rate)\n",
    "print(\"Recurrent Layers Sizes:\", reconstructed_model.recurrent_layers_sizes)\n",
    "print(\"Regressor Layer Size:\", reconstructed_model.regressor_layer_size)\n",
    "print(\"Use Prosit PTM Features:\", reconstructed_model.use_prosit_ptm_features)\n",
    "print(\"Input Keys:\", reconstructed_model.input_keys)\n",
    "\n",
    "# Print attributes\n",
    "print(\"Default Input Keys:\", reconstructed_model.DEFAULT_INPUT_KEYS)\n",
    "print(\"Meta Data Keys (Attribute):\", reconstructed_model.META_DATA_KEYS)\n",
    "print(\"PTM Input Keys:\", reconstructed_model.PTM_INPUT_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# load the model at a certain checkpoint (Note: change path according to your directories)\n",
    "checkpoint_model = keras.models.load_model(\"/nfs/home/students/s.baier/mapra/dlomix/bmpc_shared_scripts/refinement_transfer_learning_utils/checkpoints/model-01-0.67.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary \n",
    "\n",
    "# Print parameters\n",
    "print(\"Embedding Output Dimension:\", checkpoint_model.embedding_output_dim)\n",
    "print(\"Sequence Length:\", checkpoint_model.seq_length)\n",
    "print(\"Alphabet Dictionary:\", checkpoint_model.alphabet)\n",
    "print(\"Dropout Rate:\", checkpoint_model.dropout_rate)\n",
    "print(\"Latent Dropout Rate:\", checkpoint_model.latent_dropout_rate)\n",
    "print(\"Recurrent Layers Sizes:\", checkpoint_model.recurrent_layers_sizes)\n",
    "print(\"Regressor Layer Size:\", checkpoint_model.regressor_layer_size)\n",
    "print(\"Use Prosit PTM Features:\", checkpoint_model.use_prosit_ptm_features)\n",
    "print(\"Input Keys:\", checkpoint_model.input_keys)\n",
    "\n",
    "# Print attributes\n",
    "print(\"Default Input Keys:\", checkpoint_model.DEFAULT_INPUT_KEYS)\n",
    "print(\"Meta Data Keys (Attribute):\", checkpoint_model.META_DATA_KEYS)\n",
    "print(\"PTM Input Keys:\", checkpoint_model.PTM_INPUT_KEYS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

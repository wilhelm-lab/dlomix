{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:38:42.664133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-26 22:38:42.664182: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-26 22:38:42.665621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 22:38:42.674619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 22:38:44.502416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/cmnfs/home/f.kapitza/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../bmpc_shared_scripts/refinement_transfer_learning')\n",
    "sys.path.append('../bmpc_shared_scripts/oktoberfest_interface')\n",
    "from oktoberfest_interface import load_keras_model, process_dataset\n",
    "from automatic_rl_tl import AutomaticRlTlTraining, AutomaticRlTlTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "os.environ['HF_HOME'] = '/cmnfs/proj/bmpc_dlomix/datasets'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/cmnfs/proj/bmpc_dlomix/datasets/hf_cache'\n",
    "\n",
    "num_proc = 16\n",
    "os.environ[\"OMP_NUM_THREADS\"] = f\"{num_proc}\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = f\"{num_proc}\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = f\"{num_proc}\"\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_proc)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset\n",
    "parquet_path = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'\n",
    "modifications = ['K[UNIMOD:122]']\n",
    "# large single ptm\n",
    "# parquet_path= '/cmnfs/data/proteomics/Prosit_PTMs/TUM_mod_monomethyl.parquet'\n",
    "# modifications = ['K[UNIMOD:34]', 'R[UNIMOD:34]']\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:39:14.061251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7401 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  464       \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582298 (13.67 MB)\n",
      "Trainable params: 3582298 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load baseline model\n",
    "model = load_keras_model('baseline')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n",
      "Mapping SequenceParsingProcessor:   0%|          | 0/7169 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 11176.28 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 10076.52 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 16289.00 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 11338.21 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 18560.89 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 13148.61 examples/s]\n",
      "Filter: 100%|██████████| 7169/7169 [00:00<00:00, 215407.62 examples/s]\n",
      "Filter: 100%|██████████| 1793/1793 [00:00<00:00, 99476.02 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7169/7169 [00:01<00:00, 4677.46 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1793/1793 [00:00<00:00, 4941.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = process_dataset(\n",
    "    parquet_file_path=parquet_path,\n",
    "    model=model,\n",
    "    modifications=modifications,\n",
    "    ion_types=ion_types,\n",
    "    label_column='intensities_raw',\n",
    "    val_ratio=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutomaticRlTlTrainingConfig(\n",
    "    dataset=dataset,\n",
    "    baseline_model=model,\n",
    "    use_wandb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfinnkap\u001b[0m (\u001b[33mmapra_dlomix\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cmnfs/home/f.kapitza/dlomix/finn_notebooks/wandb/run-20240726_223939-00cre4wo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/00cre4wo' target=\"_blank\">misty-rain-56</a></strong> to <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/00cre4wo' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/00cre4wo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embedding layer]  model and dataset modifications do not match\n",
      "[embedding layer]  can reuse old embedding weights\n",
      "[regressor layer]  matching ion types\n"
     ]
    }
   ],
   "source": [
    "trainer = AutomaticRlTlTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:39:49.389520: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 12ms/step - loss: 0.6804 - masked_pearson_correlation_distance: 0.5601\n",
      "validation loss: 0.6803836226463318, pearson distance: 0.5600541234016418\n",
      "Epoch 1/10554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:39:56.614626: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f419811e5d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-26 22:39:56.614672: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-07-26 22:39:56.620975: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722033596.737265  188212 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/113 [>.............................] - ETA: 1s - loss: 0.6824 - masked_pearson_correlation_distance: 0.5717   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_end` time: 0.0120s). Check your callbacks.\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6836 - masked_pearson_correlation_distance: 0.5717Epoch 00000: val_loss improved from inf to 0.68037\n",
      "113/113 [==============================] - 9s 33ms/step - loss: 0.6835 - masked_pearson_correlation_distance: 0.5709 - val_loss: 0.6804 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 2/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6829 - masked_pearson_correlation_distance: 0.5710Epoch 00001: val_loss improved from 0.68037 to 0.68036\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6829 - masked_pearson_correlation_distance: 0.5700 - val_loss: 0.6804 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 3/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6825 - masked_pearson_correlation_distance: 0.5696Epoch 00002: val_loss improved from 0.68036 to 0.68034\n",
      "113/113 [==============================] - 3s 25ms/step - loss: 0.6824 - masked_pearson_correlation_distance: 0.5686 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 4/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6826 - masked_pearson_correlation_distance: 0.5701Epoch 00003: val_loss improved from 0.68034 to 0.68033\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6824 - masked_pearson_correlation_distance: 0.5696 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 5/10554\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6825 - masked_pearson_correlation_distance: 0.5686Epoch 00004: val_loss improved from 0.68033 to 0.68032\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6825 - masked_pearson_correlation_distance: 0.5686 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 6/10554\n",
      "111/113 [============================>.] - ETA: 0s - loss: 0.6828 - masked_pearson_correlation_distance: 0.5703Epoch 00005: val_loss improved from 0.68032 to 0.68030\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6827 - masked_pearson_correlation_distance: 0.5690 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5600\n",
      "Epoch 7/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6821 - masked_pearson_correlation_distance: 0.5697Epoch 00006: val_loss improved from 0.68030 to 0.68029\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6819 - masked_pearson_correlation_distance: 0.5689 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 8/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6830 - masked_pearson_correlation_distance: 0.5710Epoch 00007: val_loss improved from 0.68029 to 0.68028\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6830 - masked_pearson_correlation_distance: 0.5695 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 9/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6824 - masked_pearson_correlation_distance: 0.5708Epoch 00008: val_loss improved from 0.68028 to 0.68026\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6824 - masked_pearson_correlation_distance: 0.5700 - val_loss: 0.6803 - val_masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 10/10554\n",
      "111/113 [============================>.] - ETA: 0s - loss: 0.6828 - masked_pearson_correlation_distance: 0.5703Epoch 00009: val_loss improved from 0.68026 to 0.68025\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6827 - masked_pearson_correlation_distance: 0.5694 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 11/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6823 - masked_pearson_correlation_distance: 0.5700Epoch 00010: val_loss improved from 0.68025 to 0.68023\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6820 - masked_pearson_correlation_distance: 0.5701 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5599\n",
      "Epoch 12/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6820 - masked_pearson_correlation_distance: 0.5692Epoch 00011: val_loss improved from 0.68023 to 0.68022\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6820 - masked_pearson_correlation_distance: 0.5687 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5598\n",
      "Epoch 13/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6823 - masked_pearson_correlation_distance: 0.5692Epoch 00012: val_loss improved from 0.68022 to 0.68020\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6821 - masked_pearson_correlation_distance: 0.5686 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5598\n",
      "Epoch 14/10554\n",
      "111/113 [============================>.] - ETA: 0s - loss: 0.6828 - masked_pearson_correlation_distance: 0.5708Epoch 00013: val_loss improved from 0.68020 to 0.68018\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6826 - masked_pearson_correlation_distance: 0.5690 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5598\n",
      "Epoch 15/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6820 - masked_pearson_correlation_distance: 0.5694Epoch 00014: val_loss improved from 0.68018 to 0.68017\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6818 - masked_pearson_correlation_distance: 0.5686 - val_loss: 0.6802 - val_masked_pearson_correlation_distance: 0.5598\n",
      "Epoch 16/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6822 - masked_pearson_correlation_distance: 0.5699Epoch 00015: val_loss improved from 0.68017 to 0.68015\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6822 - masked_pearson_correlation_distance: 0.5690 - val_loss: 0.6801 - val_masked_pearson_correlation_distance: 0.5598\n",
      "Epoch 17/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6818 - masked_pearson_correlation_distance: 0.5692Epoch 00016: val_loss improved from 0.68015 to 0.68013\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6816 - masked_pearson_correlation_distance: 0.5685 - val_loss: 0.6801 - val_masked_pearson_correlation_distance: 0.5597\n",
      "Epoch 18/10554\n",
      "110/113 [============================>.] - ETA: 0s - loss: 0.6821 - masked_pearson_correlation_distance: 0.5697Epoch 00017: val_loss improved from 0.68013 to 0.68011\n",
      "113/113 [==============================] - 3s 25ms/step - loss: 0.6819 - masked_pearson_correlation_distance: 0.5689 - val_loss: 0.6801 - val_masked_pearson_correlation_distance: 0.5597\n",
      "Epoch 19/10554\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.6825 - masked_pearson_correlation_distance: 0.5700Epoch 00018: val_loss improved from 0.68011 to 0.68010\n",
      "113/113 [==============================] - 3s 24ms/step - loss: 0.6824 - masked_pearson_correlation_distance: 0.5696 - val_loss: 0.6801 - val_masked_pearson_correlation_distance: 0.5597\n",
      "Epoch 20/10554\n",
      " 37/113 [========>.....................] - ETA: 1s - loss: 0.6821 - masked_pearson_correlation_distance: 0.5679"
     ]
    }
   ],
   "source": [
    "new_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>epoch_total</td><td>▁▃▆█</td></tr><tr><td>es_avg_change</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>es_curr_change</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>freeze_embedding_layer</td><td>▁</td></tr><tr><td>freeze_inner_layers</td><td>▁</td></tr><tr><td>freeze_old_embedding_weights</td><td>▁</td></tr><tr><td>freeze_old_regressor_weights</td><td>▁</td></tr><tr><td>freeze_regressor_layer</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▄▆▇▇▇▇█▇▇▇▇▆▇▇▇███▇▇▇▄▆▇████▇▇▆▁▅▆▇▇▇█▇▇</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█▅▇▇▆▆▆▅▅▅▅▇▆▇▇▇▇▆▅▅▅▄▁▄▆▆▇▆▅▅▄▃▁▄▅▆▆▆▆▄</td></tr><tr><td>patience_counter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.65546</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>epoch_total</td><td>3</td></tr><tr><td>es_avg_change</td><td>0.03264</td></tr><tr><td>es_curr_change</td><td>0.03264</td></tr><tr><td>freeze_embedding_layer</td><td>0</td></tr><tr><td>freeze_inner_layers</td><td>1</td></tr><tr><td>freeze_old_embedding_weights</td><td>1</td></tr><tr><td>freeze_old_regressor_weights</td><td>0</td></tr><tr><td>freeze_regressor_layer</td><td>1</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.66359</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.54234</td></tr><tr><td>patience_counter</td><td>0</td></tr><tr><td>val_loss</td><td>0.65546</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.52869</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-universe-51</strong> at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/0ipnbmhu' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/0ipnbmhu</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240724_155636-0ipnbmhu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:56:17.022077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 15:56:17.022120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 15:56:17.023306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 15:56:17.030080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:56:18.924557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/cmnfs/home/f.kapitza/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../bmpc_shared_scripts/refinement_transfer_learning')\n",
    "sys.path.append('../bmpc_shared_scripts/oktoberfest_interface')\n",
    "from oktoberfest_interface import load_keras_model, process_dataset\n",
    "from automatic_rl_tl import AutomaticRlTlTraining, AutomaticRlTlTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "os.environ['HF_HOME'] = '/cmnfs/proj/bmpc_dlomix/datasets'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/cmnfs/proj/bmpc_dlomix/datasets/hf_cache'\n",
    "\n",
    "num_proc = 16\n",
    "os.environ[\"OMP_NUM_THREADS\"] = f\"{num_proc}\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = f\"{num_proc}\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = f\"{num_proc}\"\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_proc)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset\n",
    "# parquet_path = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'\n",
    "# modifications = ['K[UNIMOD:122]']\n",
    "# large single ptm\n",
    "parquet_path= '/cmnfs/data/proteomics/Prosit_PTMs/TUM_mod_monomethyl.parquet'\n",
    "modifications = ['K[UNIMOD:34]', 'R[UNIMOD:34]']\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached model: /cmnfs/home/f.kapitza/.dlomix/models/prosit_baseline_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:56:24.767294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7505 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  464       \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582298 (13.67 MB)\n",
      "Trainable params: 3582298 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load baseline model\n",
    "model = load_keras_model('baseline')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/dlomix/finn_notebooks/../bmpc_shared_scripts/oktoberfest_interface/oktoberfest_interface.py:134: UserWarning: \n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 335479 examples [00:13, 24372.80 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 268383/268383 [00:24<00:00, 11013.65 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 67096/67096 [00:05<00:00, 11921.95 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 268383/268383 [00:13<00:00, 20180.17 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 67096/67096 [00:03<00:00, 18931.00 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 268383/268383 [00:12<00:00, 20791.23 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 67096/67096 [00:03<00:00, 20104.18 examples/s]\n",
      "Filter: 100%|██████████| 268383/268383 [00:00<00:00, 523850.54 examples/s]\n",
      "Filter: 100%|██████████| 67096/67096 [00:00<00:00, 435750.25 examples/s]\n",
      "Casting the dataset: 100%|██████████| 268383/268383 [00:47<00:00, 5623.44 examples/s]\n",
      "Casting the dataset: 100%|██████████| 67096/67096 [00:12<00:00, 5330.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: train, val\n"
     ]
    }
   ],
   "source": [
    "dataset = process_dataset(\n",
    "    parquet_file_path=parquet_path,\n",
    "    model=model,\n",
    "    modifications=modifications,\n",
    "    ion_types=ion_types,\n",
    "    label_column='intensities_raw',\n",
    "    val_ratio=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutomaticRlTlTrainingConfig(\n",
    "    dataset=dataset,\n",
    "    baseline_model=model,\n",
    "    use_wandb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cmnfs/home/f.kapitza/dlomix/finn_notebooks/wandb/run-20240724_160303-12o0559k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/12o0559k' target=\"_blank\">dashing-pyramid-52</a></strong> to <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/12o0559k' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/12o0559k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embedding layer]  model and dataset modifications do not match\n",
      "[embedding layer]  can reuse old embedding weights\n",
      "[regressor layer]  matching ion types\n"
     ]
    }
   ],
   "source": [
    "trainer = AutomaticRlTlTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.7488 - masked_pearson_correlation_distance: 0.6686\n",
      "validation loss: 0.7488385438919067, pearson distance: 0.668613612651825\n",
      "Epoch 1/10015\n",
      "4192/4194 [============================>.] - ETA: 0s - loss: 0.7640 - masked_pearson_correlation_distance: 0.6984Epoch 00000: val_loss improved from inf to 0.70686\n",
      "4194/4194 [==============================] - 78s 18ms/step - loss: 0.7640 - masked_pearson_correlation_distance: 0.6984 - val_loss: 0.7069 - val_masked_pearson_correlation_distance: 0.6018\n",
      "Epoch 2/10015\n",
      "4191/4194 [============================>.] - ETA: 0s - loss: 0.7633 - masked_pearson_correlation_distance: 0.6977Epoch 00001: val_loss improved from 0.70686 to 0.70484\n",
      "4194/4194 [==============================] - 69s 17ms/step - loss: 0.7633 - masked_pearson_correlation_distance: 0.6977 - val_loss: 0.7048 - val_masked_pearson_correlation_distance: 0.5994\n",
      "Epoch 3/10015\n",
      "4192/4194 [============================>.] - ETA: 0s - loss: 0.7608 - masked_pearson_correlation_distance: 0.6949Epoch 00002: val_loss improved from 0.70484 to 0.70053\n",
      "4194/4194 [==============================] - 71s 17ms/step - loss: 0.7608 - masked_pearson_correlation_distance: 0.6949 - val_loss: 0.7005 - val_masked_pearson_correlation_distance: 0.5942\n",
      "Epoch 4/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.7559 - masked_pearson_correlation_distance: 0.6897Epoch 00003: val_loss improved from 0.70053 to 0.69140\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.7559 - masked_pearson_correlation_distance: 0.6897 - val_loss: 0.6914 - val_masked_pearson_correlation_distance: 0.5833\n",
      "Epoch 5/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.7456 - masked_pearson_correlation_distance: 0.6785Epoch 00004: val_loss improved from 0.69140 to 0.67256\n",
      "4194/4194 [==============================] - 73s 17ms/step - loss: 0.7456 - masked_pearson_correlation_distance: 0.6785 - val_loss: 0.6726 - val_masked_pearson_correlation_distance: 0.5608\n",
      "Epoch 6/10015\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.7255 - masked_pearson_correlation_distance: 0.6560Epoch 00005: val_loss improved from 0.67256 to 0.63749\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.7255 - masked_pearson_correlation_distance: 0.6560 - val_loss: 0.6375 - val_masked_pearson_correlation_distance: 0.5188\n",
      "Epoch 7/10015\n",
      "4191/4194 [============================>.] - ETA: 0s - loss: 0.6940 - masked_pearson_correlation_distance: 0.6172Epoch 00006: val_loss improved from 0.63749 to 0.59335\n",
      "4194/4194 [==============================] - 73s 17ms/step - loss: 0.6939 - masked_pearson_correlation_distance: 0.6172 - val_loss: 0.5934 - val_masked_pearson_correlation_distance: 0.4619\n",
      "Epoch 8/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.6596 - masked_pearson_correlation_distance: 0.5678Epoch 00007: val_loss improved from 0.59335 to 0.55945\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.6596 - masked_pearson_correlation_distance: 0.5678 - val_loss: 0.5594 - val_masked_pearson_correlation_distance: 0.4127\n",
      "Epoch 9/10015\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.6306 - masked_pearson_correlation_distance: 0.5200Epoch 00008: val_loss improved from 0.55945 to 0.54204\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.6306 - masked_pearson_correlation_distance: 0.5200 - val_loss: 0.5420 - val_masked_pearson_correlation_distance: 0.3871\n",
      "Epoch 10/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.6100 - masked_pearson_correlation_distance: 0.4883Epoch 00009: val_loss improved from 0.54204 to 0.52855\n",
      "4194/4194 [==============================] - 73s 17ms/step - loss: 0.6100 - masked_pearson_correlation_distance: 0.4883 - val_loss: 0.5286 - val_masked_pearson_correlation_distance: 0.3728\n",
      "Epoch 11/10015\n",
      "4191/4194 [============================>.] - ETA: 0s - loss: 0.5953 - masked_pearson_correlation_distance: 0.4709Epoch 00010: val_loss improved from 0.52855 to 0.50786\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.5953 - masked_pearson_correlation_distance: 0.4709 - val_loss: 0.5079 - val_masked_pearson_correlation_distance: 0.3473\n",
      "Epoch 12/10015\n",
      "4191/4194 [============================>.] - ETA: 0s - loss: 0.5856 - masked_pearson_correlation_distance: 0.4577Epoch 00011: val_loss improved from 0.50786 to 0.49885\n",
      "4194/4194 [==============================] - 71s 17ms/step - loss: 0.5856 - masked_pearson_correlation_distance: 0.4576 - val_loss: 0.4989 - val_masked_pearson_correlation_distance: 0.3331\n",
      "Epoch 13/10015\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.5818 - masked_pearson_correlation_distance: 0.4467Epoch 00012: val_loss improved from 0.49885 to 0.49727\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.5818 - masked_pearson_correlation_distance: 0.4467 - val_loss: 0.4973 - val_masked_pearson_correlation_distance: 0.3302\n",
      "Epoch 14/10015\n",
      "4194/4194 [==============================] - 72s 17ms/step - loss: 0.5802 - masked_pearson_correlation_distance: 0.4420 - val_loss: 0.4977 - val_masked_pearson_correlation_distance: 0.3303\n",
      "Epoch 15/10015\n",
      "4194/4194 [==============================] - 70s 17ms/step - loss: 0.5799 - masked_pearson_correlation_distance: 0.4410 - val_loss: 0.4989 - val_masked_pearson_correlation_distance: 0.3315\n",
      "Epoch 16/10015\n",
      "4194/4194 [==============================] - 60s 14ms/step - loss: 0.5798 - masked_pearson_correlation_distance: 0.4407 - val_loss: 0.4994 - val_masked_pearson_correlation_distance: 0.3332\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.5738 - masked_pearson_correlation_distance: 0.4328\n",
      "validation loss: 0.573819100856781, pearson distance: 0.43280208110809326\n",
      "Epoch 1/10015\n",
      "4194/4194 [==============================] - 145s 32ms/step - loss: 0.5793 - masked_pearson_correlation_distance: 0.4400 - val_loss: 0.5736 - val_masked_pearson_correlation_distance: 0.4330\n",
      "Epoch 2/10015\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.5789 - masked_pearson_correlation_distance: 0.4401 - val_loss: 0.5732 - val_masked_pearson_correlation_distance: 0.4333\n",
      "Epoch 3/10015\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.5783 - masked_pearson_correlation_distance: 0.4400 - val_loss: 0.5725 - val_masked_pearson_correlation_distance: 0.4334\n",
      "Epoch 4/10015\n",
      "4194/4194 [==============================] - 134s 32ms/step - loss: 0.5771 - masked_pearson_correlation_distance: 0.4392 - val_loss: 0.5707 - val_masked_pearson_correlation_distance: 0.4320\n",
      "Epoch 5/10015\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.5730 - masked_pearson_correlation_distance: 0.4351 - val_loss: 0.5611 - val_masked_pearson_correlation_distance: 0.4231\n",
      "Epoch 6/10015\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.5605 - masked_pearson_correlation_distance: 0.4245 - val_loss: 0.5466 - val_masked_pearson_correlation_distance: 0.4111\n",
      "Epoch 7/10015\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.5498 - masked_pearson_correlation_distance: 0.4109 - val_loss: 0.5358 - val_masked_pearson_correlation_distance: 0.3942\n",
      "Epoch 8/10015\n",
      "4194/4194 [==============================] - 132s 31ms/step - loss: 0.5367 - masked_pearson_correlation_distance: 0.3926 - val_loss: 0.5143 - val_masked_pearson_correlation_distance: 0.3680\n",
      "Epoch 9/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.4994 - masked_pearson_correlation_distance: 0.3720Epoch 00008: val_loss improved from 0.49727 to 0.45712\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.4994 - masked_pearson_correlation_distance: 0.3720 - val_loss: 0.4571 - val_masked_pearson_correlation_distance: 0.3470\n",
      "Epoch 10/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.4484 - masked_pearson_correlation_distance: 0.3527Epoch 00009: val_loss improved from 0.45712 to 0.40581\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.4484 - masked_pearson_correlation_distance: 0.3527 - val_loss: 0.4058 - val_masked_pearson_correlation_distance: 0.3176\n",
      "Epoch 11/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.4013 - masked_pearson_correlation_distance: 0.3127Epoch 00010: val_loss improved from 0.40581 to 0.35646\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.4013 - masked_pearson_correlation_distance: 0.3127 - val_loss: 0.3565 - val_masked_pearson_correlation_distance: 0.2691\n",
      "Epoch 12/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.3482 - masked_pearson_correlation_distance: 0.2742Epoch 00011: val_loss improved from 0.35646 to 0.29642\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.3482 - masked_pearson_correlation_distance: 0.2742 - val_loss: 0.2964 - val_masked_pearson_correlation_distance: 0.2354\n",
      "Epoch 13/10015\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.3039 - masked_pearson_correlation_distance: 0.2499Epoch 00012: val_loss improved from 0.29642 to 0.26125\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.3039 - masked_pearson_correlation_distance: 0.2499 - val_loss: 0.2612 - val_masked_pearson_correlation_distance: 0.2297\n",
      "Epoch 14/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.2704 - masked_pearson_correlation_distance: 0.2233Epoch 00013: val_loss improved from 0.26125 to 0.22973\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.2704 - masked_pearson_correlation_distance: 0.2233 - val_loss: 0.2297 - val_masked_pearson_correlation_distance: 0.1938\n",
      "Epoch 15/10015\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.2562 - masked_pearson_correlation_distance: 0.2036 - val_loss: 0.2530 - val_masked_pearson_correlation_distance: 0.2001\n",
      "Epoch 16/10015\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.2307 - masked_pearson_correlation_distance: 0.1804Epoch 00015: val_loss improved from 0.22973 to 0.19798\n",
      "4194/4194 [==============================] - 132s 31ms/step - loss: 0.2307 - masked_pearson_correlation_distance: 0.1804 - val_loss: 0.1980 - val_masked_pearson_correlation_distance: 0.1695\n",
      "Epoch 17/10015\n",
      "3811/4194 [==========================>...] - ETA: 10s - loss: 0.2079 - masked_pearson_correlation_distance: 0.1677Epoch 00016: val_loss improved from 0.19798 to 0.17559\n",
      "4194/4194 [==============================] - 121s 29ms/step - loss: 0.2079 - masked_pearson_correlation_distance: 0.1677 - val_loss: 0.1756 - val_masked_pearson_correlation_distance: 0.1519\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.1756 - masked_pearson_correlation_distance: 0.1519\n",
      "validation loss: 0.175594300031662, pearson distance: 0.15185783803462982\n",
      "Epoch 1/10000\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.1683 - masked_pearson_correlation_distance: 0.1486Epoch 00000: val_loss improved from 0.17559 to 0.13715\n",
      "4194/4194 [==============================] - 139s 32ms/step - loss: 0.1683 - masked_pearson_correlation_distance: 0.1486 - val_loss: 0.1372 - val_masked_pearson_correlation_distance: 0.1395\n",
      "Epoch 2/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1574 - masked_pearson_correlation_distance: 0.1455Epoch 00001: val_loss improved from 0.13715 to 0.13063\n",
      "4194/4194 [==============================] - 131s 31ms/step - loss: 0.1574 - masked_pearson_correlation_distance: 0.1455 - val_loss: 0.1306 - val_masked_pearson_correlation_distance: 0.1379\n",
      "Epoch 3/10000\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.1523 - masked_pearson_correlation_distance: 0.1442Epoch 00002: val_loss improved from 0.13063 to 0.12609\n",
      "4194/4194 [==============================] - 131s 31ms/step - loss: 0.1523 - masked_pearson_correlation_distance: 0.1442 - val_loss: 0.1261 - val_masked_pearson_correlation_distance: 0.1366\n",
      "Epoch 4/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1484 - masked_pearson_correlation_distance: 0.1432Epoch 00003: val_loss improved from 0.12609 to 0.12273\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.1484 - masked_pearson_correlation_distance: 0.1432 - val_loss: 0.1227 - val_masked_pearson_correlation_distance: 0.1363\n",
      "Epoch 5/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1453 - masked_pearson_correlation_distance: 0.1420Epoch 00004: val_loss improved from 0.12273 to 0.11927\n",
      "4194/4194 [==============================] - 132s 32ms/step - loss: 0.1453 - masked_pearson_correlation_distance: 0.1420 - val_loss: 0.1193 - val_masked_pearson_correlation_distance: 0.1333\n",
      "Epoch 6/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1419 - masked_pearson_correlation_distance: 0.1407Epoch 00005: val_loss improved from 0.11927 to 0.11688\n",
      "4194/4194 [==============================] - 131s 31ms/step - loss: 0.1419 - masked_pearson_correlation_distance: 0.1407 - val_loss: 0.1169 - val_masked_pearson_correlation_distance: 0.1336\n",
      "Epoch 7/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1404 - masked_pearson_correlation_distance: 0.1404Epoch 00006: val_loss improved from 0.11688 to 0.11530\n",
      "4194/4194 [==============================] - 132s 31ms/step - loss: 0.1404 - masked_pearson_correlation_distance: 0.1404 - val_loss: 0.1153 - val_masked_pearson_correlation_distance: 0.1342\n",
      "Epoch 8/10000\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.1392 - masked_pearson_correlation_distance: 0.1398Epoch 00007: val_loss improved from 0.11530 to 0.11414\n",
      "4194/4194 [==============================] - 131s 31ms/step - loss: 0.1392 - masked_pearson_correlation_distance: 0.1398 - val_loss: 0.1141 - val_masked_pearson_correlation_distance: 0.1328\n",
      "Epoch 9/10000\n",
      "4193/4194 [============================>.] - ETA: 0s - loss: 0.1381 - masked_pearson_correlation_distance: 0.1392Epoch 00008: val_loss improved from 0.11414 to 0.11368\n",
      "4194/4194 [==============================] - 132s 31ms/step - loss: 0.1381 - masked_pearson_correlation_distance: 0.1392 - val_loss: 0.1137 - val_masked_pearson_correlation_distance: 0.1325\n",
      "Epoch 10/10000\n",
      "4194/4194 [==============================] - ETA: 0s - loss: 0.1371 - masked_pearson_correlation_distance: 0.1390Epoch 00009: val_loss improved from 0.11368 to 0.11224\n",
      "4194/4194 [==============================] - 133s 32ms/step - loss: 0.1371 - masked_pearson_correlation_distance: 0.1390 - val_loss: 0.1122 - val_masked_pearson_correlation_distance: 0.1321\n",
      "Epoch 11/10000\n",
      "2710/4194 [==================>...........] - ETA: 41s - loss: 0.1365 - masked_pearson_correlation_distance: 0.1391Epoch 00010: val_loss improved from 0.11224 to 0.11176\n",
      "4194/4194 [==============================] - 90s 21ms/step - loss: 0.1365 - masked_pearson_correlation_distance: 0.1391 - val_loss: 0.1118 - val_masked_pearson_correlation_distance: 0.1326\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.1118 - masked_pearson_correlation_distance: 0.1326\n",
      "validation loss: 0.11175863444805145, pearson distance: 0.13255575299263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▇▇██▁▁▂▂▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▆</td></tr><tr><td>epoch_total</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>es_avg_change</td><td>████▇▇▆▅▃▂▂▁█████▇▇▆▅▃▁▁████████████████</td></tr><tr><td>es_curr_change</td><td>████▇▆▅▅▄▄▄▅█████▇▇▅▄▂▁▆████████████████</td></tr><tr><td>freeze_embedding_layer</td><td>▁▁▁</td></tr><tr><td>freeze_inner_layers</td><td>█▁▁</td></tr><tr><td>freeze_old_embedding_weights</td><td>█▁▁</td></tr><tr><td>freeze_old_regressor_weights</td><td>▁▁▁</td></tr><tr><td>freeze_regressor_layer</td><td>█▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▇█▁▁▁▁▁▁▁▁▁▁▂▃▄███▂▂▂▂▁▁▁▁▁</td></tr><tr><td>loss</td><td>██████▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█████▇▇▆▆▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>patience_counter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▃▅█▁▁▁▁▁▁▁▃</td></tr><tr><td>val_loss</td><td>███▇▇▇▆▆▆▆▅▅▅▅▅▆▆▆▆▆▆▅▅▄▄▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█▇▇▇▇▆▅▅▄▄▄▄▄▄▄▅▅▅▅▅▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.11176</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_total</td><td>43</td></tr><tr><td>es_avg_change</td><td>-0.0</td></tr><tr><td>es_curr_change</td><td>-0.0</td></tr><tr><td>freeze_embedding_layer</td><td>0</td></tr><tr><td>freeze_inner_layers</td><td>0</td></tr><tr><td>freeze_old_embedding_weights</td><td>0</td></tr><tr><td>freeze_old_regressor_weights</td><td>0</td></tr><tr><td>freeze_regressor_layer</td><td>0</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>0.13649</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.13913</td></tr><tr><td>patience_counter</td><td>2671</td></tr><tr><td>val_loss</td><td>0.11176</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.13256</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-pyramid-52</strong> at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/12o0559k' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/12o0559k</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240724_160303-12o0559k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>epoch_total</td><td>▁▃▆█</td></tr><tr><td>es_avg_change</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>es_curr_change</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>freeze_embedding_layer</td><td>▁</td></tr><tr><td>freeze_inner_layers</td><td>▁</td></tr><tr><td>freeze_old_embedding_weights</td><td>▁</td></tr><tr><td>freeze_old_regressor_weights</td><td>▁</td></tr><tr><td>freeze_regressor_layer</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▄▆▇▇▇▇█▇▇▇▇▆▇▇▇███▇▇▇▄▆▇████▇▇▆▁▅▆▇▇▇█▇▇</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█▅▇▇▆▆▆▅▅▅▅▇▆▇▇▇▇▆▅▅▅▄▁▄▆▆▇▆▅▅▄▃▁▄▅▆▆▆▆▄</td></tr><tr><td>patience_counter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.65546</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>epoch_total</td><td>3</td></tr><tr><td>es_avg_change</td><td>0.03264</td></tr><tr><td>es_curr_change</td><td>0.03264</td></tr><tr><td>freeze_embedding_layer</td><td>0</td></tr><tr><td>freeze_inner_layers</td><td>1</td></tr><tr><td>freeze_old_embedding_weights</td><td>1</td></tr><tr><td>freeze_old_regressor_weights</td><td>0</td></tr><tr><td>freeze_regressor_layer</td><td>1</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.66359</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.54234</td></tr><tr><td>patience_counter</td><td>0</td></tr><tr><td>val_loss</td><td>0.65546</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.52869</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-universe-51</strong> at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/0ipnbmhu' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/0ipnbmhu</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL' target=\"_blank\">https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240724_155636-0ipnbmhu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

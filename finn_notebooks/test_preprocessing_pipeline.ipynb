{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bmpc_shared_scripts/prepare_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data.fragment_ion_intensity import FragmentIonIntensityDataset\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "import tensorflow as tf\n",
    "from pyarrow import parquet as pq\n",
    "from dlomix.losses import masked_spectral_distance\n",
    "from get_updated_alphabet import get_modification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:29:07.634554: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
      "2024-07-18 14:29:07.634624: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: minotaur.exbio.wzw.tum.de\n",
      "2024-07-18 14:29:07.634655: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: minotaur.exbio.wzw.tum.de\n",
      "2024-07-18 14:29:07.634846: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 550.90.7\n",
      "2024-07-18 14:29:07.634932: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.90.7\n",
      "2024-07-18 14:29:07.634948: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "MODEL_DIR = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/'\n",
    "RUN_NAME = '7ef3360f-2349-46c0-a905-01187d4899e2'\n",
    "model = tf.keras.models.load_model(MODEL_DIR + RUN_NAME + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_parquet_path = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet'\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if intensities column in parquet file\n",
    "inference_only = True\n",
    "col_names = pq.read_schema(small_parquet_path).names\n",
    "if 'intensities_raw' in col_names:\n",
    "    inference_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all tokens present in the dataset\n",
    "file = pq.ParquetFile(small_parquet_path)\n",
    "dataset_tokens = set()\n",
    "for batch in tqdm(file.iter_batches()):\n",
    "    for cur_seq in batch['modified_sequence']:\n",
    "        cur_mods = get_modification(str(cur_seq))\n",
    "        dataset_tokens |= set(cur_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens unknown to the model appear in the dataset!\n"
     ]
    }
   ],
   "source": [
    "# get the model alphabet and compare with the tokens from the dataset\n",
    "# if new modifications are present -> need new embedding layer\n",
    "model_tokens = set(model.alphabet.keys())\n",
    "difference = dataset_tokens - model_tokens\n",
    "if not difference:\n",
    "    print('No tokens unknown to the model appear in the dataset!')\n",
    "    new_alphabet = model.alphabet\n",
    "else:\n",
    "    print(f'These tokens appear in the dataset, but are not known to the model {difference}')\n",
    "    print('A new embedding layer is necessary.')\n",
    "    old_alphabet = model.alphabet\n",
    "    new_alphabet = old_alphabet.update({k: i for i, k in enumerate(difference, start=len(model.alphabet) + 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new ion types detected. Output layer can stay the same.\n"
     ]
    }
   ],
   "source": [
    "# check for the ion types -> if ion types contain other than the b and y ions -> new output layer is necessary\n",
    "number_of_ions = len(ion_types)\n",
    "if any([ion_type in ['c', 'z', 'a', 'x'] for ion_type in ion_types]):\n",
    "    if len(number_of_ions) == 2:\n",
    "        print(f'New ion types detected, but only 2 ion types present. -> reinitialize the output layer')\n",
    "    if len(number_of_ions) > 2:\n",
    "        if 'y' in ion_types and 'b' in ion_types:\n",
    "            print('New Ion types in addition to y and b ions detected -> new output layer, but can keep trained weights for y and b ions')\n",
    "else:\n",
    "    print('No new ion types detected. Output layer can stay the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 10466.55 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 11558.86 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 14808.49 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 10428.44 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 17322.69 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 13969.63 examples/s]\n",
      "Filter: 100%|██████████| 6400/6400 [00:00<00:00, 232607.00 examples/s]\n",
      "Filter: 100%|██████████| 1600/1600 [00:00<00:00, 102744.91 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6390/6390 [00:01<00:00, 4667.35 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1598/1598 [00:00<00:00, 4888.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = FragmentIonIntensityDataset(\n",
    "    data_source=small_parquet_path,\n",
    "    data_format='parquet',\n",
    "    inference_only=inference_only,\n",
    "    alphabet=new_alphabet,\n",
    "    encoding_scheme='naive-mods',\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\", \"method_nbr\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 6390\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 1598\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the process_dataset function on different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bmpc_shared_scripts/oktoberfest_interface')\n",
    "from oktoberfest_interface import process_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different dataset paths\n",
    "etd_dataset = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/new_ion_types_ETD_support_edited.parquet'\n",
    "inference_only_ds = 'test_inference_only.parquet'\n",
    "single_ptm = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path\n",
    "model_path = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/dlomix/finn_notebooks/../bmpc_shared_scripts/oktoberfest_interface/oktoberfest_interface.py:69: UserWarning: \n",
      "                Number of ions is the same as the loaded model supports, but the ion types are different.\n",
      "                The model probably needs to be refined to achieve a better performance on these new ion types.\n",
      "                \n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 10704.92 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 10473.91 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 18409.15 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 12151.23 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 19121.32 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 13252.84 examples/s]\n",
      "Filter: 100%|██████████| 6302/6302 [00:00<00:00, 354984.54 examples/s]\n",
      "Filter: 100%|██████████| 1576/1576 [00:00<00:00, 155574.93 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6300/6300 [00:01<00:00, 4535.82 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1575/1575 [00:00<00:00, 2930.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: train, val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds, model = process_dataset(etd_dataset, model_path, ion_types=['z', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7878 examples [00:00, 190705.20 examples/s]\n",
      "/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:366: UserWarning: \n",
      "                This is a inference only dataset! You can only make predictions with this dataset! Attempting to\n",
      "                train a model with this dataset will result in an error!\n",
      "                \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 61882.63 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 52750.79 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 48444.92 examples/s]\n",
      "Filter: 100%|██████████| 7878/7878 [00:00<00:00, 484888.50 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7875/7875 [00:01<00:00, 6177.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds, model = process_dataset(inference_only_ds, 'unmod_ext', ion_types=['y', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/dlomix/finn_notebooks/../bmpc_shared_scripts/oktoberfest_interface/oktoberfest_interface.py:58: UserWarning: \n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6839/6839 [00:01<00:00, 6166.51 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1710/1710 [00:00<00:00, 7753.03 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6839/6839 [00:00<00:00, 17776.25 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1710/1710 [00:00<00:00, 10453.15 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6839/6839 [00:00<00:00, 15557.11 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1710/1710 [00:01<00:00, 1686.71 examples/s]\n",
      "Filter: 100%|██████████| 6839/6839 [00:00<00:00, 136080.63 examples/s]\n",
      "Filter: 100%|██████████| 1710/1710 [00:02<00:00, 676.75 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6839/6839 [00:02<00:00, 3092.24 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1710/1710 [00:00<00:00, 3025.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: train, val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds, model = process_dataset(single_ptm, 'unmod_ext', modifications=['K[UNIMOD:1]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

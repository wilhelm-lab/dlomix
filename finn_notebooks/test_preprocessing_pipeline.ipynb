{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:05:53.449658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 18:05:53.449710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 18:05:53.451260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 18:05:53.460252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 18:05:55.313204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dlomix.data.fragment_ion_intensity import FragmentIonIntensityDataset\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "import tensorflow as tf\n",
    "from pyarrow import parquet as pq\n",
    "from dlomix.losses import masked_spectral_distance\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.extend(['../bmpc_shared_scripts/oktoberfest_interface', '../bmpc_shared_scripts/prepare_dataset'])\n",
    "from get_updated_alphabet import get_modification\n",
    "from dlomix.interface.oktoberfest_interface import process_dataset, download_model_from_github, load_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:05:58.860918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7505 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-07-28 18:05:58.861532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7505 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-07-28 18:05:58.862098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 136 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1\n",
      "2024-07-28 18:05:58.862699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 136 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "MODEL_DIR = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/'\n",
    "RUN_NAME = '7ef3360f-2349-46c0-a905-01187d4899e2'\n",
    "model = tf.keras.models.load_model(MODEL_DIR + RUN_NAME + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_parquet_path = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet'\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if intensities column in parquet file\n",
    "inference_only = True\n",
    "col_names = pq.read_schema(small_parquet_path).names\n",
    "if 'intensities_raw' in col_names:\n",
    "    inference_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all tokens present in the dataset\n",
    "file = pq.ParquetFile(small_parquet_path)\n",
    "dataset_tokens = set()\n",
    "for batch in tqdm(file.iter_batches()):\n",
    "    for cur_seq in batch['modified_sequence']:\n",
    "        cur_mods = get_modification(str(cur_seq))\n",
    "        dataset_tokens |= set(cur_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens unknown to the model appear in the dataset!\n"
     ]
    }
   ],
   "source": [
    "# get the model alphabet and compare with the tokens from the dataset\n",
    "# if new modifications are present -> need new embedding layer\n",
    "model_tokens = set(model.alphabet.keys())\n",
    "difference = dataset_tokens - model_tokens\n",
    "if not difference:\n",
    "    print('No tokens unknown to the model appear in the dataset!')\n",
    "    new_alphabet = model.alphabet\n",
    "else:\n",
    "    print(f'These tokens appear in the dataset, but are not known to the model {difference}')\n",
    "    print('A new embedding layer is necessary.')\n",
    "    old_alphabet = model.alphabet\n",
    "    new_alphabet = old_alphabet.update({k: i for i, k in enumerate(difference, start=len(model.alphabet) + 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new ion types detected. Output layer can stay the same.\n"
     ]
    }
   ],
   "source": [
    "# check for the ion types -> if ion types contain other than the b and y ions -> new output layer is necessary\n",
    "number_of_ions = len(ion_types)\n",
    "if any([ion_type in ['c', 'z', 'a', 'x'] for ion_type in ion_types]):\n",
    "    if len(number_of_ions) == 2:\n",
    "        print(f'New ion types detected, but only 2 ion types present. -> reinitialize the output layer')\n",
    "    if len(number_of_ions) > 2:\n",
    "        if 'y' in ion_types and 'b' in ion_types:\n",
    "            print('New Ion types in addition to y and b ions detected -> new output layer, but can keep trained weights for y and b ions')\n",
    "else:\n",
    "    print('No new ion types detected. Output layer can stay the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 9306.93 examples/s] \n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 9894.40 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 17163.32 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 11316.78 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 18463.89 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 12405.90 examples/s]\n",
      "Filter: 100%|██████████| 6400/6400 [00:00<00:00, 229183.24 examples/s]\n",
      "Filter: 100%|██████████| 1600/1600 [00:00<00:00, 107800.21 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6391/6391 [00:01<00:00, 4534.39 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1597/1597 [00:00<00:00, 4795.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = FragmentIonIntensityDataset(\n",
    "    data_source=small_parquet_path,\n",
    "    data_format='parquet',\n",
    "    inference_only=inference_only,\n",
    "    alphabet=new_alphabet,\n",
    "    encoding_scheme='naive-mods',\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\", \"method_nbr\"],\n",
    "    ion_types=['y', 'b', 'z', 'c']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 6391\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 1597\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'b', 'z', 'c']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ion_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model downloading from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = download_model_from_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  464       \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582298 (13.67 MB)\n",
      "Trainable params: 3582298 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the process_dataset function on different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different dataset paths\n",
    "etd_dataset = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/new_ion_types_ETD_support_edited.parquet'\n",
    "inference_only_ds = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/test_inference_only.parquet'\n",
    "single_ptm = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'\n",
    "split_dataset = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path\n",
    "model_path = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = load_keras_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                Number of ions is the same as the loaded model supports, but the ion types are different.\n",
      "                The model probably needs to be refined to achieve a better performance on these new ion types.\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 8520.55 examples/s] \n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 9532.14 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 18415.41 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 12194.79 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 18189.61 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 13307.80 examples/s]\n",
      "Filter: 100%|██████████| 6302/6302 [00:00<00:00, 227136.05 examples/s]\n",
      "Filter: 100%|██████████| 1576/1576 [00:00<00:00, 58903.62 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6299/6299 [00:01<00:00, 4072.55 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1576/1576 [00:00<00:00, 4183.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test the function with a dataset containing new ion types\n",
    "ds = process_dataset(etd_dataset, baseline_model, ion_types=['z', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7878 examples [00:00, 460089.77 examples/s]\n",
      "/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:369: UserWarning: \n",
      "                This is a inference only dataset! You can only make predictions with this dataset! Attempting to\n",
      "                train a model with this dataset will result in an error!\n",
      "                \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 46523.98 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 41791.38 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 43249.36 examples/s]\n",
      "Filter: 100%|██████████| 7878/7878 [00:00<00:00, 375840.02 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7875/7875 [00:01<00:00, 5664.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with inference only dataset, and no model is specified\n",
    "ds = process_dataset(inference_only_ds, ion_types=['y', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 12323.67 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 10589.28 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 16867.73 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 11372.83 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 18929.60 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 13862.03 examples/s]\n",
      "Filter: 100%|██████████| 7169/7169 [00:00<00:00, 237221.14 examples/s]\n",
      "Filter: 100%|██████████| 1793/1793 [00:00<00:00, 119564.80 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7169/7169 [00:01<00:00, 4589.33 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1793/1793 [00:00<00:00, 4708.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with a dataset containing new modifications\n",
    "ds = process_dataset(single_ptm, baseline_model, modifications=['K[UNIMOD:122]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:359: UserWarning: \n",
      "                Multiple data sources or a single non-train data source provided {'train': '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet', 'val': '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_val.parquet', 'test': '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_test.parquet'}, please ensure that the data sources are already split into train, val and test sets\n",
      "                since no splitting will happen. If not, please provide only one data_source and set the val_ratio to split the data into train and val sets.\"\n",
      "                \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 21471.73 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 4000/4000 [00:00<00:00, 19059.69 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 2000/2000 [00:00<00:00, 15881.56 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 16473.45 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 4000/4000 [00:00<00:00, 14771.56 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 2000/2000 [00:00<00:00, 16002.14 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 18077.31 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 4000/4000 [00:00<00:00, 15191.98 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 2000/2000 [00:00<00:00, 13935.44 examples/s]\n",
      "Filter: 100%|██████████| 8000/8000 [00:00<00:00, 247583.02 examples/s]\n",
      "Filter: 100%|██████████| 4000/4000 [00:00<00:00, 177758.64 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7988/7988 [00:01<00:00, 4618.83 examples/s]\n",
      "Casting the dataset: 100%|██████████| 3993/3993 [00:00<00:00, 5003.27 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2000/2000 [00:00<00:00, 23417.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with dataset which is already split\n",
    "ds = process_dataset(split_dataset, baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6272/6272 [00:00<00:00, 10330.99 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 4215.72 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 897/897 [00:00<00:00, 13546.43 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6272/6272 [00:00<00:00, 17262.62 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 14091.51 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 897/897 [00:00<00:00, 11015.21 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6272/6272 [00:00<00:00, 16149.35 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 12923.23 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 897/897 [00:00<00:00, 11724.15 examples/s]\n",
      "Filter: 100%|██████████| 6272/6272 [00:00<00:00, 361038.03 examples/s]\n",
      "Filter: 100%|██████████| 1793/1793 [00:00<00:00, 168660.14 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6272/6272 [00:01<00:00, 4600.38 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1793/1793 [00:00<00:00, 4740.74 examples/s]\n",
      "Casting the dataset: 100%|██████████| 897/897 [00:00<00:00, 18035.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with a given test_ratio\n",
    "ds = process_dataset(single_ptm, baseline_model, modifications=['K[UNIMOD:122]'], test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

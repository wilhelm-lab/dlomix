{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data.fragment_ion_intensity import FragmentIonIntensityDataset\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "import tensorflow as tf\n",
    "from pyarrow import parquet as pq\n",
    "from dlomix.losses import masked_spectral_distance\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.extend(['../bmpc_shared_scripts/oktoberfest_interface', '../bmpc_shared_scripts/prepare_dataset'])\n",
    "from get_updated_alphabet import get_modification\n",
    "from oktoberfest_interface import process_dataset, download_model_from_github, load_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 08:22:29.155748: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
      "2024-07-23 08:22:29.155796: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: minotaur.exbio.wzw.tum.de\n",
      "2024-07-23 08:22:29.155806: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: minotaur.exbio.wzw.tum.de\n",
      "2024-07-23 08:22:29.155965: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 550.90.7\n",
      "2024-07-23 08:22:29.156000: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.90.7\n",
      "2024-07-23 08:22:29.156007: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "MODEL_DIR = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/'\n",
    "RUN_NAME = '7ef3360f-2349-46c0-a905-01187d4899e2'\n",
    "model = tf.keras.models.load_model(MODEL_DIR + RUN_NAME + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_parquet_path = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet'\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if intensities column in parquet file\n",
    "inference_only = True\n",
    "col_names = pq.read_schema(small_parquet_path).names\n",
    "if 'intensities_raw' in col_names:\n",
    "    inference_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all tokens present in the dataset\n",
    "file = pq.ParquetFile(small_parquet_path)\n",
    "dataset_tokens = set()\n",
    "for batch in tqdm(file.iter_batches()):\n",
    "    for cur_seq in batch['modified_sequence']:\n",
    "        cur_mods = get_modification(str(cur_seq))\n",
    "        dataset_tokens |= set(cur_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens unknown to the model appear in the dataset!\n"
     ]
    }
   ],
   "source": [
    "# get the model alphabet and compare with the tokens from the dataset\n",
    "# if new modifications are present -> need new embedding layer\n",
    "model_tokens = set(model.alphabet.keys())\n",
    "difference = dataset_tokens - model_tokens\n",
    "if not difference:\n",
    "    print('No tokens unknown to the model appear in the dataset!')\n",
    "    new_alphabet = model.alphabet\n",
    "else:\n",
    "    print(f'These tokens appear in the dataset, but are not known to the model {difference}')\n",
    "    print('A new embedding layer is necessary.')\n",
    "    old_alphabet = model.alphabet\n",
    "    new_alphabet = old_alphabet.update({k: i for i, k in enumerate(difference, start=len(model.alphabet) + 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new ion types detected. Output layer can stay the same.\n"
     ]
    }
   ],
   "source": [
    "# check for the ion types -> if ion types contain other than the b and y ions -> new output layer is necessary\n",
    "number_of_ions = len(ion_types)\n",
    "if any([ion_type in ['c', 'z', 'a', 'x'] for ion_type in ion_types]):\n",
    "    if len(number_of_ions) == 2:\n",
    "        print(f'New ion types detected, but only 2 ion types present. -> reinitialize the output layer')\n",
    "    if len(number_of_ions) > 2:\n",
    "        if 'y' in ion_types and 'b' in ion_types:\n",
    "            print('New Ion types in addition to y and b ions detected -> new output layer, but can keep trained weights for y and b ions')\n",
    "else:\n",
    "    print('No new ion types detected. Output layer can stay the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 10424.86 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 9912.83 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 15803.83 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 10502.73 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 17052.17 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 12661.72 examples/s]\n",
      "Filter: 100%|██████████| 6400/6400 [00:00<00:00, 326997.42 examples/s]\n",
      "Filter: 100%|██████████| 1600/1600 [00:00<00:00, 71847.19 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6390/6390 [00:01<00:00, 4419.79 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1598/1598 [00:00<00:00, 4469.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = FragmentIonIntensityDataset(\n",
    "    data_source=small_parquet_path,\n",
    "    data_format='parquet',\n",
    "    inference_only=inference_only,\n",
    "    alphabet=new_alphabet,\n",
    "    encoding_scheme='naive-mods',\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\", \"method_nbr\"],\n",
    "    ion_types=['y', 'b', 'z', 'c']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 6391\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 1597\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'b', 'z', 'c']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ion_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model downloading from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = download_model_from_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:30:10.668582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7505 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-07-26 22:30:10.669222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7505 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-07-26 22:30:10.669708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 7505 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1\n",
      "2024-07-26 22:30:10.670238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 7505 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  464       \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582298 (13.67 MB)\n",
      "Trainable params: 3582298 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the process_dataset function on different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different dataset paths\n",
    "etd_dataset = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/new_ion_types_ETD_support_edited.parquet'\n",
    "inference_only_ds = 'test_inference_only.parquet'\n",
    "single_ptm = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path\n",
    "model_path = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = load_keras_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                Number of ions is the same as the loaded model supports, but the ion types are different.\n",
      "                The model probably needs to be refined to achieve a better performance on these new ion types.\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 10133.36 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 9838.35 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 17380.27 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 12146.08 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 15803.34 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 12684.48 examples/s]\n",
      "Filter: 100%|██████████| 6302/6302 [00:00<00:00, 194455.30 examples/s]\n",
      "Filter: 100%|██████████| 1576/1576 [00:00<00:00, 86325.77 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6299/6299 [00:01<00:00, 4333.77 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1576/1576 [00:00<00:00, 4500.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test the function with a dataset containing new ion types\n",
    "ds = process_dataset(etd_dataset, baseline_model, ion_types=['z', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 51696.55 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 53523.23 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 46877.36 examples/s]\n",
      "Filter: 100%|██████████| 7878/7878 [00:00<00:00, 426313.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with inference only dataset, and no model is specified\n",
    "ds = process_dataset(inference_only_ds, ion_types=['y', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are new tokens in the dataset, which are not supported by the loaded model.\n",
      "            Either load a different model or transfer learning needs to be done.\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 10214.75 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 9567.03 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 16383.90 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 11530.55 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 17180.41 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 12458.73 examples/s]\n",
      "Filter: 100%|██████████| 7169/7169 [00:00<00:00, 218047.48 examples/s]\n",
      "Filter: 100%|██████████| 1793/1793 [00:00<00:00, 96552.62 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7169/7169 [00:01<00:00, 4827.79 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1793/1793 [00:00<00:00, 4882.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test function with a dataset containing new modifications\n",
    "ds = process_dataset(single_ptm, baseline_model, modifications=['K[UNIMOD:122]'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

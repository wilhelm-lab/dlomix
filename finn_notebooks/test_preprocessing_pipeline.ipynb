{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data.fragment_ion_intensity import FragmentIonIntensityDataset\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "import tensorflow as tf\n",
    "from pyarrow import parquet as pq\n",
    "from dlomix.losses import masked_spectral_distance\n",
    "from get_updated_alphabet import get_modification\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.extend(['../bmpc_shared_scripts/oktoberfest_interface', '../bmpc_shared_scripts/prepare_dataset'])\n",
    "from oktoberfest_interface import process_dataset, download_model_from_github, load_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 08:22:29.155748: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
      "2024-07-23 08:22:29.155796: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: minotaur.exbio.wzw.tum.de\n",
      "2024-07-23 08:22:29.155806: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: minotaur.exbio.wzw.tum.de\n",
      "2024-07-23 08:22:29.155965: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 550.90.7\n",
      "2024-07-23 08:22:29.156000: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.90.7\n",
      "2024-07-23 08:22:29.156007: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "MODEL_DIR = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/'\n",
    "RUN_NAME = '7ef3360f-2349-46c0-a905-01187d4899e2'\n",
    "model = tf.keras.models.load_model(MODEL_DIR + RUN_NAME + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_parquet_path = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet'\n",
    "ion_types = ['y', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if intensities column in parquet file\n",
    "inference_only = True\n",
    "col_names = pq.read_schema(small_parquet_path).names\n",
    "if 'intensities_raw' in col_names:\n",
    "    inference_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all tokens present in the dataset\n",
    "file = pq.ParquetFile(small_parquet_path)\n",
    "dataset_tokens = set()\n",
    "for batch in tqdm(file.iter_batches()):\n",
    "    for cur_seq in batch['modified_sequence']:\n",
    "        cur_mods = get_modification(str(cur_seq))\n",
    "        dataset_tokens |= set(cur_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens unknown to the model appear in the dataset!\n"
     ]
    }
   ],
   "source": [
    "# get the model alphabet and compare with the tokens from the dataset\n",
    "# if new modifications are present -> need new embedding layer\n",
    "model_tokens = set(model.alphabet.keys())\n",
    "difference = dataset_tokens - model_tokens\n",
    "if not difference:\n",
    "    print('No tokens unknown to the model appear in the dataset!')\n",
    "    new_alphabet = model.alphabet\n",
    "else:\n",
    "    print(f'These tokens appear in the dataset, but are not known to the model {difference}')\n",
    "    print('A new embedding layer is necessary.')\n",
    "    old_alphabet = model.alphabet\n",
    "    new_alphabet = old_alphabet.update({k: i for i, k in enumerate(difference, start=len(model.alphabet) + 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new ion types detected. Output layer can stay the same.\n"
     ]
    }
   ],
   "source": [
    "# check for the ion types -> if ion types contain other than the b and y ions -> new output layer is necessary\n",
    "number_of_ions = len(ion_types)\n",
    "if any([ion_type in ['c', 'z', 'a', 'x'] for ion_type in ion_types]):\n",
    "    if len(number_of_ions) == 2:\n",
    "        print(f'New ion types detected, but only 2 ion types present. -> reinitialize the output layer')\n",
    "    if len(number_of_ions) > 2:\n",
    "        if 'y' in ion_types and 'b' in ion_types:\n",
    "            print('New Ion types in addition to y and b ions detected -> new output layer, but can keep trained weights for y and b ions')\n",
    "else:\n",
    "    print('No new ion types detected. Output layer can stay the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 10424.86 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 9912.83 examples/s] \n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 15803.83 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 10502.73 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6400/6400 [00:00<00:00, 17052.17 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1600/1600 [00:00<00:00, 12661.72 examples/s]\n",
      "Filter: 100%|██████████| 6400/6400 [00:00<00:00, 326997.42 examples/s]\n",
      "Filter: 100%|██████████| 1600/1600 [00:00<00:00, 71847.19 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6390/6390 [00:01<00:00, 4419.79 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1598/1598 [00:00<00:00, 4469.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = FragmentIonIntensityDataset(\n",
    "    data_source=small_parquet_path,\n",
    "    data_format='parquet',\n",
    "    inference_only=inference_only,\n",
    "    alphabet=new_alphabet,\n",
    "    encoding_scheme='naive-mods',\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\", \"method_nbr\"],\n",
    "    ion_types=['y', 'b', 'z', 'c']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 6391\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 1597\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'b', 'z', 'c']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ion_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model downloading from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached model: /cmnfs/home/f.kapitza/.dlomix/models/prosit_baseline_model.keras\n"
     ]
    }
   ],
   "source": [
    "model_path = download_model_from_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prosit_intensity_predictor_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  464       \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 30, 512)           1996800   \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   multiple                  4608      \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (None, 29, 512)           1576806   \n",
      "                                                                 \n",
      " encoder_att (AttentionLaye  multiple                  542       \n",
      " r)                                                              \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   multiple                  0         \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 174)               3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3582298 (13.67 MB)\n",
      "Trainable params: 3582298 (13.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the process_dataset function on different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different dataset paths\n",
    "etd_dataset = '/cmnfs/proj/bmpc_dlomix/datasets/parquet/new_ion_types_ETD_support_edited.parquet'\n",
    "inference_only_ds = 'test_inference_only.parquet'\n",
    "single_ptm = '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path\n",
    "model_path = '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = load_keras_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/f.kapitza/dlomix/finn_notebooks/../bmpc_shared_scripts/oktoberfest_interface/oktoberfest_interface.py:146: UserWarning: \n",
      "                Number of ions is the same as the loaded model supports, but the ion types are different.\n",
      "                The model probably needs to be refined to achieve a better performance on these new ion types.\n",
      "                \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 10629.34 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 10483.45 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 18676.26 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 12869.19 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 6302/6302 [00:00<00:00, 19678.77 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1576/1576 [00:00<00:00, 13256.93 examples/s]\n",
      "Filter: 100%|██████████| 6302/6302 [00:00<00:00, 225591.05 examples/s]\n",
      "Filter: 100%|██████████| 1576/1576 [00:00<00:00, 110007.21 examples/s]\n",
      "Casting the dataset: 100%|██████████| 6299/6299 [00:01<00:00, 4537.23 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1576/1576 [00:00<00:00, 3819.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: train, val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the function with a dataset containing new ion types\n",
    "ds, model = process_dataset(etd_dataset, baseline_model, ion_types=['z', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached model: /cmnfs/home/f.kapitza/.dlomix/models/prosit_baseline_model.keras\n",
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7878 examples [00:00, 252442.68 examples/s]\n",
      "/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:369: UserWarning: \n",
      "                This is a inference only dataset! You can only make predictions with this dataset! Attempting to\n",
      "                train a model with this dataset will result in an error!\n",
      "                \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 53786.87 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 52111.78 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7878/7878 [00:00<00:00, 51173.66 examples/s]\n",
      "Filter: 100%|██████████| 7878/7878 [00:00<00:00, 489507.38 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7875/7875 [00:01<00:00, 6364.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test function with inference only dataset, and no model is specified\n",
    "ds, model = process_dataset(inference_only_ds, ion_types=['y', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping SequenceParsingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 10365.77 examples/s]\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 10143.96 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 17843.58 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 12396.58 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 7169/7169 [00:00<00:00, 17675.83 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 1793/1793 [00:00<00:00, 13033.24 examples/s]\n",
      "Filter: 100%|██████████| 7169/7169 [00:00<00:00, 242667.79 examples/s]\n",
      "Filter: 100%|██████████| 1793/1793 [00:00<00:00, 188051.99 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7169/7169 [00:01<00:00, 4856.73 examples/s]\n",
      "Casting the dataset: 100%|██████████| 1793/1793 [00:00<00:00, 4664.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available data splits are: train, val\n"
     ]
    }
   ],
   "source": [
    "# test function with a dataset containing new modifications\n",
    "ds, model = process_dataset(single_ptm, baseline_model, modifications=['K[UNIMOD:122]'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

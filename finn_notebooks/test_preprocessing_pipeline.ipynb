{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data.fragment_ion_intensity import FragmentIonIntensityDataset\n",
    "from dlomix.constants import PTMS_ALPHABET\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8000 examples [00:00, 48347.93 examples/s]\n",
      "/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:364: UserWarning: \n",
      "                This is a inference only dataset! You can only make predictions with this dataset! Attempting to\n",
      "                train a model with this dataset will result in an error!\n",
      "                \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 47289.40 examples/s]\n",
      "Mapping SequenceEncodingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 35476.85 examples/s]\n",
      "Mapping SequencePaddingProcessor: 100%|██████████| 8000/8000 [00:00<00:00, 37421.12 examples/s]\n",
      "Filter: 100%|██████████| 8000/8000 [00:00<00:00, 444153.07 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7988/7988 [00:01<00:00, 5905.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = FragmentIonIntensityDataset(\n",
    "    '/cmnfs/proj/bmpc_dlomix/datasets/parquet/noptm_baseline_small_train.parquet',\n",
    "    inference_only=True,\n",
    "    alphabet=PTMS_ALPHABET,\n",
    "    encoding_scheme='naive-mods',\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\",\"method_nbr\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    inference: Dataset({\n",
       "        features: ['modified_sequence', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "        num_rows: 7988\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ds.tensor_inference_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modified_sequence': <tf.Tensor: shape=(64, 30), dtype=int64, numpy=\n",
       " array([[21, 17,  6, ...,  0,  0,  0],\n",
       "        [21,  1,  7, ...,  0,  0,  0],\n",
       "        [21, 12,  4, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [21,  5, 10, ...,  0,  0,  0],\n",
       "        [21,  5,  3, ...,  0,  0,  0],\n",
       "        [21,  6, 12, ...,  0,  0,  0]])>,\n",
       " 'precursor_charge_onehot': <tf.Tensor: shape=(64, 6), dtype=float32, numpy=\n",
       " array([[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " 'collision_energy_aligned_normed': <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([0.22366957, 0.31246135, 0.21690454, 0.23119135, 0.27326065,\n",
       "        0.28179693, 0.34295037, 0.21536441, 0.29070818, 0.26568195,\n",
       "        0.35      , 0.35      , 0.3808744 , 0.23284255, 0.27445298,\n",
       "        0.38712904, 0.21804687, 0.36519817, 0.3014984 , 0.43208858,\n",
       "        0.38754806, 0.35      , 0.27574265, 0.26566762, 0.27208096,\n",
       "        0.3365882 , 0.32382253, 0.24037503, 0.32699102, 0.2225332 ,\n",
       "        0.30466658, 0.24506447, 0.25081056, 0.24742638, 0.31442294,\n",
       "        0.36725262, 0.35      , 0.32528755, 0.21139948, 0.35      ,\n",
       "        0.21397674, 0.35      , 0.22823973, 0.37456873, 0.35      ,\n",
       "        0.25294113, 0.32178414, 0.32134387, 0.2562581 , 0.32025096,\n",
       "        0.29133743, 0.31212443, 0.39674354, 0.35      , 0.21397111,\n",
       "        0.32051176, 0.25971907, 0.35      , 0.35      , 0.35      ,\n",
       "        0.21241851, 0.37445298, 0.28210002, 0.26527247], dtype=float32)>,\n",
       " 'method_nbr': <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 1., 2., 2., 2., 1., 1., 1., 2., 2., 2., 2.], dtype=float32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.losses import masked_spectral_distance\n",
    "model = tf.keras.models.load_model('/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024/4bc7bc69-bbf4-4366-90fc-474b1946c588.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 83ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22409272"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

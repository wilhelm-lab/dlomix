{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 17:14:52.114343: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-04 17:14:52.114387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-04 17:14:52.116031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-04 17:14:52.125264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 17:14:54.170493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/cmnfs/home/f.kapitza/miniconda3/envs/dlomix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\",\n",
      "   \"red_smiles\": \"Reduced SMILES representation of PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: parsed sequence\n",
      "    - _n_term_mods: N-terminal modifications\n",
      "    - _c_term_mods: C-terminal modifications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "sys.path.append('../bmpc_shared_scripts/refinement_transfer_learning')\n",
    "from impl_model_training import RlTlTraining, load_config, combine_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../bmpc_shared_scripts/refinement_transfer_learning/config_files/baseline_no_ptm_to_ptm_kmod_formyl.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'alphabet': {'-[]': 22,\n",
      "                          'A': 1,\n",
      "                          'C': 2,\n",
      "                          'C[UNIMOD:4]': 25,\n",
      "                          'D': 3,\n",
      "                          'E': 4,\n",
      "                          'F': 5,\n",
      "                          'G': 6,\n",
      "                          'H': 7,\n",
      "                          'I': 8,\n",
      "                          'K': 9,\n",
      "                          'K[UNIMOD:737]': 24,\n",
      "                          'L': 10,\n",
      "                          'M': 11,\n",
      "                          'M[UNIMOD:35]': 23,\n",
      "                          'N': 12,\n",
      "                          'P': 13,\n",
      "                          'Q': 14,\n",
      "                          'R': 15,\n",
      "                          'S': 16,\n",
      "                          'T': 17,\n",
      "                          'V': 18,\n",
      "                          'W': 19,\n",
      "                          'Y': 20,\n",
      "                          '[UNIMOD:1]-': 26,\n",
      "                          '[UNIMOD:737]-': 21,\n",
      "                          '[]-': 21},\n",
      "             'batch_size': 1024,\n",
      "             'hf_cache': '/cmnfs/proj/bmpc_dlomix/datasets/hf_cache',\n",
      "             'hf_home': '/cmnfs/proj/bmpc_dlomix/datasets',\n",
      "             'name': 'kmod_formyl',\n",
      "             'parquet_path': '/cmnfs/data/proteomics/Prosit_PTMs/21PTMs/Kmod_Formyl.parquet',\n",
      "             'processed_path': '/cmnfs/proj/bmpc_dlomix/datasets/processed/kmod_formyl',\n",
      "             'seq_length': 30},\n",
      " 'model': {'load_path': '/cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras'},\n",
      " 'processing': {'cuda_device_nr': '0', 'num_proc': 4},\n",
      " 'project': 'single ptm',\n",
      " 'refinement_transfer_learning': {'freeze_layers': {'is_first_layer_trainable': True,\n",
      "                                                    'is_last_layer_trainable': False,\n",
      "                                                    'release_after_epochs': 5},\n",
      "                                  'new_input_layer': {'freeze_old_weights': True,\n",
      "                                                      'new_mods': ['K[UNIMOD:122]'],\n",
      "                                                      'release_after_epochs': 10}},\n",
      " 'training': {'early_stopping': {'min_delta': 1e-05, 'patience': 5},\n",
      "              'learning_rate': 0.0001,\n",
      "              'lr_scheduler_plateau': {'cooldown': 1,\n",
      "                                       'factor': 0.5,\n",
      "                                       'min_delta': 0.0001,\n",
      "                                       'patience': 2},\n",
      "              'lr_warmup_linear': {'end_lr': 0.0001,\n",
      "                                   'num_epochs': 1,\n",
      "                                   'start_lr': 1e-06},\n",
      "              'num_epochs': 20}}\n"
     ]
    }
   ],
   "source": [
    "if \"project\" not in config:\n",
    "    config[\"project\"] = \"refinement transfer learning\"\n",
    "\n",
    "overwritten_params = {\n",
    "    \"processing\": {}\n",
    "}\n",
    "\n",
    "overwritten_params['processing']['cuda_device_nr'] = '0'\n",
    "overwritten_params['processing']['num_proc'] = 4\n",
    "\n",
    "# start run\n",
    "combine_into(overwritten_params, config)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cmnfs/home/f.kapitza/dlomix/finn_notebooks/wandb/run-20240704_171852-3y2bnbxy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapra_dlomix/single%20ptm/runs/3y2bnbxy' target=\"_blank\">devoted-glitter-19</a></strong> to <a href='https://wandb.ai/mapra_dlomix/single%20ptm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapra_dlomix/single%20ptm' target=\"_blank\">https://wandb.ai/mapra_dlomix/single%20ptm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapra_dlomix/single%20ptm/runs/3y2bnbxy' target=\"_blank\">https://wandb.ai/mapra_dlomix/single%20ptm/runs/3y2bnbxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n",
      "loading model from file /cmnfs/proj/bmpc_dlomix/models/baseline_models/noptm_baseline_full_bs1024_unmod_extended/7ef3360f-2349-46c0-a905-01187d4899e2.keras\n",
      "freezing active\n",
      "using early stopping\n",
      "using lr scheduler plateau\n",
      "using lr warmup linear\n",
      "warmup step\n",
      "Epoch 1/5\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.4566 - masked_pearson_correlation_distance: 0.3725WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0756s vs `on_train_batch_end` time: 0.0978s). Check your callbacks.\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4523 - masked_pearson_correlation_distance: 0.3804Epoch 00000: val_loss improved from inf to 0.40850\n",
      "8/8 [==============================] - 8s 402ms/step - loss: 0.4523 - masked_pearson_correlation_distance: 0.3804 - val_loss: 0.4085 - val_masked_pearson_correlation_distance: 0.3300 - lr: 1.0000e-06\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.4125 - masked_pearson_correlation_distance: 0.3438 - val_loss: 0.4085 - val_masked_pearson_correlation_distance: 0.3300 - lr: 1.0000e-06\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.4132 - masked_pearson_correlation_distance: 0.3511 - val_loss: 0.4085 - val_masked_pearson_correlation_distance: 0.3300 - lr: 5.0000e-07\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.4129 - masked_pearson_correlation_distance: 0.3436 - val_loss: 0.4085 - val_masked_pearson_correlation_distance: 0.3300 - lr: 5.0000e-07\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.4123 - masked_pearson_correlation_distance: 0.3552 - val_loss: 0.4085 - val_masked_pearson_correlation_distance: 0.3300 - lr: 2.5000e-07\n",
      "Epoch 1/5\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.4111 - masked_pearson_correlation_distance: 0.3355WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0772s vs `on_train_batch_end` time: 0.0984s). Check your callbacks.\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4128 - masked_pearson_correlation_distance: 0.3513Epoch 00000: val_loss improved from 0.40850 to 0.40777\n",
      "8/8 [==============================] - 9s 389ms/step - loss: 0.4128 - masked_pearson_correlation_distance: 0.3513 - val_loss: 0.4078 - val_masked_pearson_correlation_distance: 0.3293 - lr: 2.5000e-07\n",
      "Epoch 2/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4119 - masked_pearson_correlation_distance: 0.3358Epoch 00001: val_loss improved from 0.40777 to 0.40720\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.4120 - masked_pearson_correlation_distance: 0.3483 - val_loss: 0.4072 - val_masked_pearson_correlation_distance: 0.3287 - lr: 2.5000e-07\n",
      "Epoch 3/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4118 - masked_pearson_correlation_distance: 0.3366Epoch 00002: val_loss improved from 0.40720 to 0.40665\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.4119 - masked_pearson_correlation_distance: 0.3440 - val_loss: 0.4067 - val_masked_pearson_correlation_distance: 0.3281 - lr: 2.5000e-07\n",
      "Epoch 4/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4111 - masked_pearson_correlation_distance: 0.3370Epoch 00003: val_loss improved from 0.40665 to 0.40616\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.4112 - masked_pearson_correlation_distance: 0.3476 - val_loss: 0.4062 - val_masked_pearson_correlation_distance: 0.3276 - lr: 2.5000e-07\n",
      "Epoch 5/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4109 - masked_pearson_correlation_distance: 0.3359Epoch 00004: val_loss improved from 0.40616 to 0.40566\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.4109 - masked_pearson_correlation_distance: 0.3509 - val_loss: 0.4057 - val_masked_pearson_correlation_distance: 0.3271 - lr: 2.5000e-07\n",
      "Epoch 1/10\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.4074 - masked_pearson_correlation_distance: 0.3305WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0776s vs `on_train_batch_end` time: 0.0992s). Check your callbacks.\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4095 - masked_pearson_correlation_distance: 0.3415Epoch 00000: val_loss improved from 0.40566 to 0.40496\n",
      "8/8 [==============================] - 9s 385ms/step - loss: 0.4095 - masked_pearson_correlation_distance: 0.3415 - val_loss: 0.4050 - val_masked_pearson_correlation_distance: 0.3264 - lr: 2.5000e-07\n",
      "Epoch 2/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4087 - masked_pearson_correlation_distance: 0.3327Epoch 00001: val_loss improved from 0.40496 to 0.40442\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.4088 - masked_pearson_correlation_distance: 0.3431 - val_loss: 0.4044 - val_masked_pearson_correlation_distance: 0.3258 - lr: 2.5000e-07\n",
      "Epoch 3/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4090 - masked_pearson_correlation_distance: 0.3336Epoch 00002: val_loss improved from 0.40442 to 0.40387\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.4090 - masked_pearson_correlation_distance: 0.3407 - val_loss: 0.4039 - val_masked_pearson_correlation_distance: 0.3252 - lr: 2.5000e-07\n",
      "Epoch 4/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4084 - masked_pearson_correlation_distance: 0.3337Epoch 00003: val_loss improved from 0.40387 to 0.40333\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.4084 - masked_pearson_correlation_distance: 0.3422 - val_loss: 0.4033 - val_masked_pearson_correlation_distance: 0.3247 - lr: 2.5000e-07\n",
      "Epoch 5/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4080 - masked_pearson_correlation_distance: 0.3319Epoch 00004: val_loss improved from 0.40333 to 0.40276\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.4080 - masked_pearson_correlation_distance: 0.3459 - val_loss: 0.4028 - val_masked_pearson_correlation_distance: 0.3241 - lr: 2.5000e-07\n",
      "Epoch 6/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4067 - masked_pearson_correlation_distance: 0.3301Epoch 00005: val_loss improved from 0.40276 to 0.40219\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.4068 - masked_pearson_correlation_distance: 0.3406 - val_loss: 0.4022 - val_masked_pearson_correlation_distance: 0.3235 - lr: 2.5000e-07\n",
      "Epoch 7/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4055 - masked_pearson_correlation_distance: 0.3292Epoch 00006: val_loss improved from 0.40219 to 0.40163\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.4055 - masked_pearson_correlation_distance: 0.3398 - val_loss: 0.4016 - val_masked_pearson_correlation_distance: 0.3230 - lr: 2.5000e-07\n",
      "Epoch 8/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4060 - masked_pearson_correlation_distance: 0.3299Epoch 00007: val_loss improved from 0.40163 to 0.40110\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.4060 - masked_pearson_correlation_distance: 0.3304 - val_loss: 0.4011 - val_masked_pearson_correlation_distance: 0.3224 - lr: 2.5000e-07\n",
      "Epoch 9/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4048 - masked_pearson_correlation_distance: 0.3289Epoch 00008: val_loss improved from 0.40110 to 0.40062\n",
      "8/8 [==============================] - 2s 211ms/step - loss: 0.4048 - masked_pearson_correlation_distance: 0.3404 - val_loss: 0.4006 - val_masked_pearson_correlation_distance: 0.3219 - lr: 2.5000e-07\n",
      "Epoch 10/10\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.4045 - masked_pearson_correlation_distance: 0.3280Epoch 00009: val_loss improved from 0.40062 to 0.40016\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 0.4045 - masked_pearson_correlation_distance: 0.3350 - val_loss: 0.4002 - val_masked_pearson_correlation_distance: 0.3213 - lr: 2.5000e-07\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▁▂▃▃▄▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_total</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>freeze_layers</td><td>█▁</td></tr><tr><td>freeze_old_embedding_weights</td><td>█▁</td></tr><tr><td>learning_rate</td><td>██████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>masked_pearson_correlation_distance</td><td>█▄▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▂▁▂▁▂▁▂▁▁▁▂▁▁▁▂▁▂</td></tr><tr><td>val_loss</td><td>█████▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>█████▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.40016</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>epoch_total</td><td>19</td></tr><tr><td>freeze_layers</td><td>0</td></tr><tr><td>freeze_old_embedding_weights</td><td>0</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.40452</td></tr><tr><td>masked_pearson_correlation_distance</td><td>0.33495</td></tr><tr><td>val_loss</td><td>0.40016</td></tr><tr><td>val_masked_pearson_correlation_distance</td><td>0.32135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-glitter-19</strong> at: <a href='https://wandb.ai/mapra_dlomix/single%20ptm/runs/3y2bnbxy' target=\"_blank\">https://wandb.ai/mapra_dlomix/single%20ptm/runs/3y2bnbxy</a><br/> View project at: <a href='https://wandb.ai/mapra_dlomix/single%20ptm' target=\"_blank\">https://wandb.ai/mapra_dlomix/single%20ptm</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240704_171852-3y2bnbxy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_run = RlTlTraining(config)\n",
    "try:\n",
    "    rl_run()\n",
    "finally:    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

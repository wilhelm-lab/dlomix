2024-08-05 15:26:51.808946: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-05 15:26:51.809015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-05 15:26:51.810535: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-05 15:26:51.816693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-05 15:26:52.665951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-05 15:26:55.763440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43607 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:21:00.0, compute capability: 8.6

                Number of ions is the same as the loaded model supports, but the ion types are different.
                The model probably needs to be refined to achieve a better performance on these new ion types.
                

Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):
{
   "atom_count": "Atom count of PTM.",
   "delta_mass": "Delta mass of PTM.",
   "mod_gain": "Gain of atoms due to PTM.",
   "mod_loss": "Loss of atoms due to PTM.",
   "red_smiles": "Reduced SMILES representation of PTM."
}.
When writing your own feature extractor, you can either
    (1) use the FeatureExtractor class or
    (2) write a function that can be mapped to the Hugging Face dataset.
In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:
    - _parsed_sequence: parsed sequence
    - _n_term_mods: N-terminal modifications
    - _c_term_mods: C-terminal modifications

Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 140 examples [00:00, 19500.62 examples/s]
Generating val split: 0 examples [00:00, ? examples/s]Generating val split: 40 examples [00:00, 12031.85 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 20 examples [00:00, 6901.36 examples/s]
/cmnfs/home/f.kapitza/dlomix/src/dlomix/data/dataset.py:359: UserWarning: 
                Multiple data sources or a single non-train data source provided {'train': '/cmnfs/data/proteomics/ProteomeTools/ETD/parquet/etd_data_train.parquet', 'val': '/cmnfs/data/proteomics/ProteomeTools/ETD/parquet/etd_data_val.parquet', 'test': '/cmnfs/data/proteomics/ProteomeTools/ETD/parquet/etd_data_test.parquet'}, please ensure that the data sources are already split into train, val and test sets
                since no splitting will happen. If not, please provide only one data_source and set the val_ratio to split the data into train and val sets."
                
  warnings.warn(
Mapping SequenceParsingProcessor:   0%|          | 0/140 [00:00<?, ? examples/s]Mapping SequenceParsingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:00<00:00, 12693.25 examples/s]
Mapping SequenceParsingProcessor:   0%|          | 0/40 [00:00<?, ? examples/s]Mapping SequenceParsingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 6515.68 examples/s]
Mapping SequenceParsingProcessor:   0%|          | 0/20 [00:00<?, ? examples/s]Mapping SequenceParsingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 3618.59 examples/s]
Mapping SequenceEncodingProcessor:   0%|          | 0/140 [00:00<?, ? examples/s]Mapping SequenceEncodingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:00<00:00, 18076.11 examples/s]
Mapping SequenceEncodingProcessor:   0%|          | 0/40 [00:00<?, ? examples/s]Mapping SequenceEncodingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 6532.93 examples/s]
Mapping SequenceEncodingProcessor:   0%|          | 0/20 [00:00<?, ? examples/s]Mapping SequenceEncodingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 3560.38 examples/s]
Mapping SequencePaddingProcessor:   0%|          | 0/140 [00:00<?, ? examples/s]Mapping SequencePaddingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:00<00:00, 19268.34 examples/s]
Mapping SequencePaddingProcessor:   0%|          | 0/40 [00:00<?, ? examples/s]Mapping SequencePaddingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 6628.43 examples/s]
Mapping SequencePaddingProcessor:   0%|          | 0/20 [00:00<?, ? examples/s]Mapping SequencePaddingProcessor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 3528.48 examples/s]
Filter:   0%|          | 0/140 [00:00<?, ? examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:00<00:00, 31769.87 examples/s]
Filter:   0%|          | 0/40 [00:00<?, ? examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 9913.85 examples/s]
Casting the dataset:   0%|          | 0/140 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:00<00:00, 3724.18 examples/s]
Casting the dataset:   0%|          | 0/40 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 4617.24 examples/s]
Casting the dataset:   0%|          | 0/20 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 3868.04 examples/s]
wandb: Currently logged in as: finnkap (mapra_dlomix). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /cmnfs/home/f.kapitza/dlomix/finn_notebooks/scripts/wandb/run-20240805_152704-ptpyreeb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-lake-78
wandb: â­ï¸ View project at https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL
wandb: ðŸš€ View run at https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/ptpyreeb
2024-08-05 15:27:16.382832: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2024-08-05 15:27:21.711106: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fdd186f4a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-08-05 15:27:21.711180: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
2024-08-05 15:27:21.724020: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1722871641.836604 3391608 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
wandb: - 0.013 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.117 MB of 0.122 MB uploadedwandb: - 0.117 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.117 MB of 0.122 MB uploadedwandb: - 0.117 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.117 MB of 0.122 MB uploadedwandb: - 0.117 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.117 MB of 0.122 MB uploadedwandb: - 0.117 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.117 MB of 0.122 MB uploadedwandb: - 0.117 MB of 0.122 MB uploadedwandb: \ 0.117 MB of 0.122 MB uploadedwandb: | 0.117 MB of 0.122 MB uploadedwandb: / 0.122 MB of 0.122 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       InflectionPointEarlyStopping_avg_change â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‡â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   InflectionPointEarlyStopping_current_change â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: InflectionPointEarlyStopping_current_patience â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           InflectionPointLRReducer_avg_change â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       InflectionPointLRReducer_current_change â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:     InflectionPointLRReducer_current_patience â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                         epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†
wandb:                                   epoch_total â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                        freeze_embedding_layer â–ˆâ–â–
wandb:                           freeze_inner_layers â–ˆâ–â–
wandb:                  freeze_old_embedding_weights â–â–â–
wandb:                  freeze_old_regressor_weights â–â–â–
wandb:                        freeze_regressor_layer â–â–â–
wandb:                                 learning_rate â–â–â–â–â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                          loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           masked_pearson_correlation_distance â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–…â–…â–…â–„â–…â–†â–†â–†â–ƒâ–ƒâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                      val_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       val_masked_pearson_correlation_distance â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–…â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:       InflectionPointEarlyStopping_avg_change 0.0
wandb:   InflectionPointEarlyStopping_current_change 0.0
wandb: InflectionPointEarlyStopping_current_patience 0.0
wandb:           InflectionPointLRReducer_avg_change 0.0
wandb:       InflectionPointLRReducer_current_change 0.0
wandb:     InflectionPointLRReducer_current_patience 0.0
wandb:                                    best_epoch 6665
wandb:                                 best_val_loss 0.42078
wandb:                                         epoch 9999
wandb:                                   epoch_total 31762
wandb:                        freeze_embedding_layer 0
wandb:                           freeze_inner_layers 0
wandb:                  freeze_old_embedding_weights 0
wandb:                  freeze_old_regressor_weights 0
wandb:                        freeze_regressor_layer 0
wandb:                                 learning_rate 0.001
wandb:                                          loss 0.05809
wandb:           masked_pearson_correlation_distance 0.19639
wandb:                                      val_loss 0.4537
wandb:       val_masked_pearson_correlation_distance 0.37237
wandb: 
wandb: ðŸš€ View run dauntless-lake-78 at: https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL/runs/ptpyreeb
wandb: â­ï¸ View project at: https://wandb.ai/mapra_dlomix/DLOmix_auto_RL_TL
wandb: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240805_152704-ptpyreeb/logs
{'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, '[]-': 21, '-[]': 22, '[UNIMOD:737]-': 21, 'M[UNIMOD:35]': 23, 'K[UNIMOD:737]': 24, 'C[UNIMOD:4]': 25, '[UNIMOD:1]-': 26}
{'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, '[]-': 21, '-[]': 22, '[UNIMOD:737]-': 21, 'M[UNIMOD:35]': 23, 'K[UNIMOD:737]': 24, 'C[UNIMOD:4]': 25, '[UNIMOD:1]-': 26}
[embedding layer]  model and dataset modifications match
[regressor layer]  ion types not matching
[regressor layer]  old regressor weights cannot be reused (mismatch in the ion ordering / num. ions)
validation loss: 1.004319190979004, pearson distance: 1.0146753787994385
[training]  masked spectral distance: 1.0018986463546753, approx. progress: 0.27%
Epoch 00000: val_loss improved from inf to 1.00432
Epoch 00001: val_loss improved from 1.00432 to 1.00432
Epoch 00003: val_loss improved from 1.00432 to 1.00432
Epoch 00010: val_loss improved from 1.00432 to 1.00432
Epoch 00018: val_loss improved from 1.00432 to 1.00432
Epoch 00068: val_loss improved from 1.00432 to 1.00432
Epoch 00325: val_loss improved from 1.00432 to 1.00432
Epoch 00740: val_loss improved from 1.00432 to 1.00432
Epoch 00821: val_loss improved from 1.00432 to 1.00432
Epoch 00850: val_loss improved from 1.00432 to 1.00432
Epoch 00908: val_loss improved from 1.00432 to 1.00432
[training]  masked spectral distance: 1.0012967586517334, approx. progress: 0.33%
Epoch 01365: val_loss improved from 1.00432 to 1.00431
Epoch 01389: val_loss improved from 1.00431 to 1.00431
Epoch 01430: val_loss improved from 1.00431 to 1.00431
Epoch 01432: val_loss improved from 1.00431 to 1.00431
Epoch 01489: val_loss improved from 1.00431 to 1.00431
Epoch 01493: val_loss improved from 1.00431 to 1.00431
Epoch 01539: val_loss improved from 1.00431 to 1.00431
Epoch 01540: val_loss improved from 1.00431 to 1.00431
Epoch 01572: val_loss improved from 1.00431 to 1.00431
Epoch 01585: val_loss improved from 1.00431 to 1.00431
Epoch 01617: val_loss improved from 1.00431 to 1.00431
Epoch 01618: val_loss improved from 1.00431 to 1.00431
Epoch 01637: val_loss improved from 1.00431 to 1.00431
Epoch 01642: val_loss improved from 1.00431 to 1.00431
Epoch 01695: val_loss improved from 1.00431 to 1.00431
Epoch 01735: val_loss improved from 1.00431 to 1.00431
Epoch 01752: val_loss improved from 1.00431 to 1.00431
Epoch 01779: val_loss improved from 1.00431 to 1.00431
Epoch 01785: val_loss improved from 1.00431 to 1.00431
Epoch 01811: val_loss improved from 1.00431 to 1.00431
Epoch 01812: val_loss improved from 1.00431 to 1.00431
Epoch 01837: val_loss improved from 1.00431 to 1.00431
Epoch 01851: val_loss improved from 1.00431 to 1.00431
Epoch 01861: val_loss improved from 1.00431 to 1.00431
Epoch 01872: val_loss improved from 1.00431 to 1.00431
Epoch 01886: val_loss improved from 1.00431 to 1.00431
Epoch 01891: val_loss improved from 1.00431 to 1.00430
Epoch 01894: val_loss improved from 1.00430 to 1.00430
Epoch 01930: val_loss improved from 1.00430 to 1.00430
Epoch 01940: val_loss improved from 1.00430 to 1.00430
Epoch 01946: val_loss improved from 1.00430 to 1.00430
Epoch 01950: val_loss improved from 1.00430 to 1.00430
Epoch 01963: val_loss improved from 1.00430 to 1.00430
Epoch 01982: val_loss improved from 1.00430 to 1.00430
Epoch 01983: val_loss improved from 1.00430 to 1.00430
Epoch 01986: val_loss improved from 1.00430 to 1.00430
Epoch 01994: val_loss improved from 1.00430 to 1.00430
[training]  masked spectral distance: 1.0018986463546753, approx. progress: 0.27%
Epoch 02002: val_loss improved from 1.00430 to 1.00430
Epoch 02025: val_loss improved from 1.00430 to 1.00430
Epoch 02037: val_loss improved from 1.00430 to 1.00430
Epoch 02040: val_loss improved from 1.00430 to 1.00430
Epoch 02048: val_loss improved from 1.00430 to 1.00430
Epoch 02076: val_loss improved from 1.00430 to 1.00430
Epoch 02077: val_loss improved from 1.00430 to 1.00430
Epoch 02090: val_loss improved from 1.00430 to 1.00430
Epoch 02100: val_loss improved from 1.00430 to 1.00430
Epoch 02107: val_loss improved from 1.00430 to 1.00430
Epoch 02108: val_loss improved from 1.00430 to 1.00430
Epoch 02130: val_loss improved from 1.00430 to 1.00430
Epoch 02132: val_loss improved from 1.00430 to 1.00429
Epoch 02136: val_loss improved from 1.00429 to 1.00429
Epoch 02140: val_loss improved from 1.00429 to 1.00429
Epoch 02143: val_loss improved from 1.00429 to 1.00429
Epoch 02144: val_loss improved from 1.00429 to 1.00429
Epoch 02148: val_loss improved from 1.00429 to 1.00429
Epoch 02153: val_loss improved from 1.00429 to 1.00429
Epoch 02159: val_loss improved from 1.00429 to 1.00429
Epoch 02164: val_loss improved from 1.00429 to 1.00429
Epoch 02166: val_loss improved from 1.00429 to 1.00429
Epoch 02167: val_loss improved from 1.00429 to 1.00429
Epoch 02192: val_loss improved from 1.00429 to 1.00429
Epoch 02195: val_loss improved from 1.00429 to 1.00429
Epoch 02199: val_loss improved from 1.00429 to 1.00429
Epoch 02349: val_loss improved from 1.00429 to 1.00429
Epoch 02352: val_loss improved from 1.00429 to 1.00429
Epoch 02356: val_loss improved from 1.00429 to 1.00429
Epoch 02358: val_loss improved from 1.00429 to 1.00429
Epoch 02361: val_loss improved from 1.00429 to 1.00429
Epoch 02364: val_loss improved from 1.00429 to 1.00428
Epoch 02366: val_loss improved from 1.00428 to 1.00428
Epoch 02367: val_loss improved from 1.00428 to 1.00428
Epoch 02369: val_loss improved from 1.00428 to 1.00428
Epoch 02375: val_loss improved from 1.00428 to 1.00428
Epoch 02379: val_loss improved from 1.00428 to 1.00428
Epoch 02385: val_loss improved from 1.00428 to 1.00428
Epoch 02386: val_loss improved from 1.00428 to 1.00428
Epoch 02388: val_loss improved from 1.00428 to 1.00428
Epoch 02391: val_loss improved from 1.00428 to 1.00428
Epoch 02399: val_loss improved from 1.00428 to 1.00428
Epoch 02413: val_loss improved from 1.00428 to 1.00428
Epoch 02414: val_loss improved from 1.00428 to 1.00428
Epoch 02417: val_loss improved from 1.00428 to 1.00428
Epoch 02418: val_loss improved from 1.00428 to 1.00428
Epoch 02419: val_loss improved from 1.00428 to 1.00427
Epoch 02420: val_loss improved from 1.00427 to 1.00427
Epoch 02421: val_loss improved from 1.00427 to 1.00427
Epoch 02422: val_loss improved from 1.00427 to 1.00427
Epoch 02424: val_loss improved from 1.00427 to 1.00427
Epoch 02427: val_loss improved from 1.00427 to 1.00427
Epoch 02428: val_loss improved from 1.00427 to 1.00427
Epoch 02430: val_loss improved from 1.00427 to 1.00427
Epoch 02431: val_loss improved from 1.00427 to 1.00427
Epoch 02436: val_loss improved from 1.00427 to 1.00427
Epoch 02438: val_loss improved from 1.00427 to 1.00427
Epoch 02439: val_loss improved from 1.00427 to 1.00427
Epoch 02440: val_loss improved from 1.00427 to 1.00427
Epoch 02444: val_loss improved from 1.00427 to 1.00427
Epoch 02445: val_loss improved from 1.00427 to 1.00427
Epoch 02453: val_loss improved from 1.00427 to 1.00427
Epoch 02455: val_loss improved from 1.00427 to 1.00427
Epoch 02458: val_loss improved from 1.00427 to 1.00427
Epoch 02459: val_loss improved from 1.00427 to 1.00427
Epoch 02460: val_loss improved from 1.00427 to 1.00426
Epoch 02461: val_loss improved from 1.00426 to 1.00426
Epoch 02465: val_loss improved from 1.00426 to 1.00426
Epoch 02467: val_loss improved from 1.00426 to 1.00426
Epoch 02468: val_loss improved from 1.00426 to 1.00426
Epoch 02472: val_loss improved from 1.00426 to 1.00426
Epoch 02473: val_loss improved from 1.00426 to 1.00426
Epoch 02480: val_loss improved from 1.00426 to 1.00426
Epoch 02481: val_loss improved from 1.00426 to 1.00426
Epoch 02484: val_loss improved from 1.00426 to 1.00426
Epoch 02486: val_loss improved from 1.00426 to 1.00426
Epoch 02487: val_loss improved from 1.00426 to 1.00426
Epoch 02488: val_loss improved from 1.00426 to 1.00426
Epoch 02489: val_loss improved from 1.00426 to 1.00426
Epoch 02492: val_loss improved from 1.00426 to 1.00426
Epoch 02493: val_loss improved from 1.00426 to 1.00426
Epoch 02494: val_loss improved from 1.00426 to 1.00426
Epoch 02496: val_loss improved from 1.00426 to 1.00426
Epoch 02497: val_loss improved from 1.00426 to 1.00425
Epoch 02500: val_loss improved from 1.00425 to 1.00425
Epoch 02502: val_loss improved from 1.00425 to 1.00425
Epoch 02504: val_loss improved from 1.00425 to 1.00425
Epoch 02505: val_loss improved from 1.00425 to 1.00425
Epoch 02506: val_loss improved from 1.00425 to 1.00425
Epoch 02507: val_loss improved from 1.00425 to 1.00425
Epoch 02509: val_loss improved from 1.00425 to 1.00425
Epoch 02511: val_loss improved from 1.00425 to 1.00425
Epoch 02513: val_loss improved from 1.00425 to 1.00425
Epoch 02514: val_loss improved from 1.00425 to 1.00425
Epoch 02515: val_loss improved from 1.00425 to 1.00425
Epoch 02517: val_loss improved from 1.00425 to 1.00425
Epoch 02519: val_loss improved from 1.00425 to 1.00425
Epoch 02520: val_loss improved from 1.00425 to 1.00425
Epoch 02521: val_loss improved from 1.00425 to 1.00425
Epoch 02522: val_loss improved from 1.00425 to 1.00424
Epoch 02523: val_loss improved from 1.00424 to 1.00424
Epoch 02524: val_loss improved from 1.00424 to 1.00424
Epoch 02525: val_loss improved from 1.00424 to 1.00424
Epoch 02526: val_loss improved from 1.00424 to 1.00424
Epoch 02531: val_loss improved from 1.00424 to 1.00424
Epoch 02533: val_loss improved from 1.00424 to 1.00424
Epoch 02534: val_loss improved from 1.00424 to 1.00424
Epoch 02535: val_loss improved from 1.00424 to 1.00424
Epoch 02538: val_loss improved from 1.00424 to 1.00424
Epoch 02539: val_loss improved from 1.00424 to 1.00424
Epoch 02541: val_loss improved from 1.00424 to 1.00424
Epoch 02542: val_loss improved from 1.00424 to 1.00423
Epoch 02544: val_loss improved from 1.00423 to 1.00423
Epoch 02547: val_loss improved from 1.00423 to 1.00423
Epoch 02548: val_loss improved from 1.00423 to 1.00423
Epoch 02549: val_loss improved from 1.00423 to 1.00423
Epoch 02550: val_loss improved from 1.00423 to 1.00423
Epoch 02552: val_loss improved from 1.00423 to 1.00423
Epoch 02553: val_loss improved from 1.00423 to 1.00423
Epoch 02557: val_loss improved from 1.00423 to 1.00423
Epoch 02558: val_loss improved from 1.00423 to 1.00423
Epoch 02560: val_loss improved from 1.00423 to 1.00423
Epoch 02562: val_loss improved from 1.00423 to 1.00423
Epoch 02565: val_loss improved from 1.00423 to 1.00422
Epoch 02567: val_loss improved from 1.00422 to 1.00422
Epoch 02568: val_loss improved from 1.00422 to 1.00422
Epoch 02576: val_loss improved from 1.00422 to 1.00422
Epoch 02577: val_loss improved from 1.00422 to 1.00422
Epoch 02579: val_loss improved from 1.00422 to 1.00422
Epoch 02583: val_loss improved from 1.00422 to 1.00422
Epoch 02586: val_loss improved from 1.00422 to 1.00422
Epoch 02587: val_loss improved from 1.00422 to 1.00422
Epoch 02588: val_loss improved from 1.00422 to 1.00422
Epoch 02589: val_loss improved from 1.00422 to 1.00422
Epoch 02591: val_loss improved from 1.00422 to 1.00422
Epoch 02593: val_loss improved from 1.00422 to 1.00422
Epoch 02594: val_loss improved from 1.00422 to 1.00422
Epoch 02595: val_loss improved from 1.00422 to 1.00422
Epoch 02596: val_loss improved from 1.00422 to 1.00422
Epoch 02598: val_loss improved from 1.00422 to 1.00422
Epoch 02606: val_loss improved from 1.00422 to 1.00422
Epoch 02609: val_loss improved from 1.00422 to 1.00421
Epoch 02611: val_loss improved from 1.00421 to 1.00421
Epoch 02612: val_loss improved from 1.00421 to 1.00421
Epoch 02619: val_loss improved from 1.00421 to 1.00421
Epoch 02634: val_loss improved from 1.00421 to 1.00421
Epoch 02636: val_loss improved from 1.00421 to 1.00421
Epoch 02639: val_loss improved from 1.00421 to 1.00421
Epoch 02646: val_loss improved from 1.00421 to 1.00421
Epoch 02647: val_loss improved from 1.00421 to 1.00421
Epoch 02648: val_loss improved from 1.00421 to 1.00421
Epoch 02650: val_loss improved from 1.00421 to 1.00421
Epoch 02651: val_loss improved from 1.00421 to 1.00421
Epoch 02653: val_loss improved from 1.00421 to 1.00421
Epoch 02655: val_loss improved from 1.00421 to 1.00420
Epoch 02657: val_loss improved from 1.00420 to 1.00420
Epoch 02659: val_loss improved from 1.00420 to 1.00420
Epoch 02661: val_loss improved from 1.00420 to 1.00420
Epoch 02668: val_loss improved from 1.00420 to 1.00420
Epoch 02670: val_loss improved from 1.00420 to 1.00420
Epoch 02671: val_loss improved from 1.00420 to 1.00420
Epoch 02906: val_loss improved from 1.00420 to 1.00420
Epoch 02907: val_loss improved from 1.00420 to 1.00420
Epoch 02908: val_loss improved from 1.00420 to 1.00420
Epoch 02909: val_loss improved from 1.00420 to 1.00420
Epoch 02911: val_loss improved from 1.00420 to 1.00420
Epoch 02912: val_loss improved from 1.00420 to 1.00420
Epoch 02913: val_loss improved from 1.00420 to 1.00419
Epoch 02916: val_loss improved from 1.00419 to 1.00419
Epoch 02917: val_loss improved from 1.00419 to 1.00419
Epoch 02918: val_loss improved from 1.00419 to 1.00419
Epoch 02919: val_loss improved from 1.00419 to 1.00419
Epoch 02920: val_loss improved from 1.00419 to 1.00419
Epoch 02921: val_loss improved from 1.00419 to 1.00419
Epoch 02922: val_loss improved from 1.00419 to 1.00419
Epoch 02923: val_loss improved from 1.00419 to 1.00419
Epoch 02924: val_loss improved from 1.00419 to 1.00419
Epoch 02926: val_loss improved from 1.00419 to 1.00419
Epoch 02927: val_loss improved from 1.00419 to 1.00418
Epoch 02928: val_loss improved from 1.00418 to 1.00418
Epoch 02929: val_loss improved from 1.00418 to 1.00418
Epoch 02931: val_loss improved from 1.00418 to 1.00418
Epoch 02932: val_loss improved from 1.00418 to 1.00418
Epoch 02933: val_loss improved from 1.00418 to 1.00418
Epoch 02935: val_loss improved from 1.00418 to 1.00418
Epoch 02942: val_loss improved from 1.00418 to 1.00418
Epoch 02948: val_loss improved from 1.00418 to 1.00418
Epoch 02949: val_loss improved from 1.00418 to 1.00418
Epoch 02952: val_loss improved from 1.00418 to 1.00417
Epoch 02953: val_loss improved from 1.00417 to 1.00417
Epoch 02954: val_loss improved from 1.00417 to 1.00417
Epoch 02955: val_loss improved from 1.00417 to 1.00417
Epoch 02956: val_loss improved from 1.00417 to 1.00417
Epoch 02957: val_loss improved from 1.00417 to 1.00417
Epoch 02959: val_loss improved from 1.00417 to 1.00417
Epoch 02960: val_loss improved from 1.00417 to 1.00416
Epoch 02961: val_loss improved from 1.00416 to 1.00416
Epoch 02962: val_loss improved from 1.00416 to 1.00416
Epoch 02963: val_loss improved from 1.00416 to 1.00416
Epoch 02965: val_loss improved from 1.00416 to 1.00416
Epoch 02966: val_loss improved from 1.00416 to 1.00416
Epoch 02967: val_loss improved from 1.00416 to 1.00416
Epoch 02969: val_loss improved from 1.00416 to 1.00416
Epoch 02970: val_loss improved from 1.00416 to 1.00416
Epoch 02971: val_loss improved from 1.00416 to 1.00416
Epoch 02972: val_loss improved from 1.00416 to 1.00415
Epoch 02973: val_loss improved from 1.00415 to 1.00415
Epoch 02974: val_loss improved from 1.00415 to 1.00415
Epoch 02975: val_loss improved from 1.00415 to 1.00415
Epoch 02976: val_loss improved from 1.00415 to 1.00415
Epoch 02977: val_loss improved from 1.00415 to 1.00415
Epoch 02978: val_loss improved from 1.00415 to 1.00415
Epoch 02980: val_loss improved from 1.00415 to 1.00415
Epoch 02981: val_loss improved from 1.00415 to 1.00415
Epoch 02982: val_loss improved from 1.00415 to 1.00415
Epoch 02984: val_loss improved from 1.00415 to 1.00415
Epoch 02985: val_loss improved from 1.00415 to 1.00415
Epoch 02986: val_loss improved from 1.00415 to 1.00415
Epoch 02987: val_loss improved from 1.00415 to 1.00414
Epoch 02988: val_loss improved from 1.00414 to 1.00414
Epoch 02990: val_loss improved from 1.00414 to 1.00414
Epoch 02992: val_loss improved from 1.00414 to 1.00414
Epoch 02993: val_loss improved from 1.00414 to 1.00414
Epoch 02996: val_loss improved from 1.00414 to 1.00414
Epoch 02998: val_loss improved from 1.00414 to 1.00414
Epoch 02999: val_loss improved from 1.00414 to 1.00414
[training]  masked spectral distance: 1.0018986463546753, approx. progress: 0.27%
Epoch 03000: val_loss improved from 1.00414 to 1.00414
Epoch 03002: val_loss improved from 1.00414 to 1.00414
Epoch 03003: val_loss improved from 1.00414 to 1.00414
Epoch 03005: val_loss improved from 1.00414 to 1.00413
Epoch 03007: val_loss improved from 1.00413 to 1.00413
Epoch 03008: val_loss improved from 1.00413 to 1.00413
Epoch 03009: val_loss improved from 1.00413 to 1.00413
Epoch 03010: val_loss improved from 1.00413 to 1.00413
Epoch 03011: val_loss improved from 1.00413 to 1.00413
Epoch 03012: val_loss improved from 1.00413 to 1.00413
Epoch 03015: val_loss improved from 1.00413 to 1.00413
Epoch 03016: val_loss improved from 1.00413 to 1.00412
Epoch 03021: val_loss improved from 1.00412 to 1.00412
Epoch 03022: val_loss improved from 1.00412 to 1.00412
Epoch 03023: val_loss improved from 1.00412 to 1.00412
Epoch 03024: val_loss improved from 1.00412 to 1.00412
Epoch 03025: val_loss improved from 1.00412 to 1.00412
Epoch 03026: val_loss improved from 1.00412 to 1.00412
Epoch 03027: val_loss improved from 1.00412 to 1.00411
Epoch 03028: val_loss improved from 1.00411 to 1.00411
Epoch 03029: val_loss improved from 1.00411 to 1.00411
Epoch 03030: val_loss improved from 1.00411 to 1.00411
Epoch 03031: val_loss improved from 1.00411 to 1.00410
Epoch 03032: val_loss improved from 1.00410 to 1.00410
Epoch 03033: val_loss improved from 1.00410 to 1.00410
Epoch 03034: val_loss improved from 1.00410 to 1.00410
Epoch 03035: val_loss improved from 1.00410 to 1.00410
Epoch 03036: val_loss improved from 1.00410 to 1.00409
Epoch 03037: val_loss improved from 1.00409 to 1.00409
Epoch 03038: val_loss improved from 1.00409 to 1.00409
Epoch 03039: val_loss improved from 1.00409 to 1.00409
Epoch 03040: val_loss improved from 1.00409 to 1.00409
Epoch 03042: val_loss improved from 1.00409 to 1.00409
Epoch 03043: val_loss improved from 1.00409 to 1.00408
Epoch 03044: val_loss improved from 1.00408 to 1.00408
Epoch 03045: val_loss improved from 1.00408 to 1.00408
Epoch 03046: val_loss improved from 1.00408 to 1.00408
Epoch 03048: val_loss improved from 1.00408 to 1.00408
Epoch 03049: val_loss improved from 1.00408 to 1.00408
Epoch 03050: val_loss improved from 1.00408 to 1.00407
Epoch 03051: val_loss improved from 1.00407 to 1.00407
Epoch 03052: val_loss improved from 1.00407 to 1.00407
Epoch 03053: val_loss improved from 1.00407 to 1.00407
Epoch 03054: val_loss improved from 1.00407 to 1.00407
Epoch 03055: val_loss improved from 1.00407 to 1.00407
Epoch 03056: val_loss improved from 1.00407 to 1.00406
Epoch 03057: val_loss improved from 1.00406 to 1.00406
Epoch 03058: val_loss improved from 1.00406 to 1.00406
Epoch 03059: val_loss improved from 1.00406 to 1.00406
Epoch 03060: val_loss improved from 1.00406 to 1.00406
Epoch 03061: val_loss improved from 1.00406 to 1.00406
Epoch 03062: val_loss improved from 1.00406 to 1.00406
Epoch 03063: val_loss improved from 1.00406 to 1.00406
Epoch 03065: val_loss improved from 1.00406 to 1.00405
Epoch 03066: val_loss improved from 1.00405 to 1.00405
Epoch 03067: val_loss improved from 1.00405 to 1.00405
Epoch 03068: val_loss improved from 1.00405 to 1.00405
Epoch 03070: val_loss improved from 1.00405 to 1.00404
Epoch 03072: val_loss improved from 1.00404 to 1.00404
Epoch 03073: val_loss improved from 1.00404 to 1.00404
Epoch 03074: val_loss improved from 1.00404 to 1.00404
Epoch 03075: val_loss improved from 1.00404 to 1.00404
Epoch 03076: val_loss improved from 1.00404 to 1.00403
Epoch 03077: val_loss improved from 1.00403 to 1.00403
Epoch 03078: val_loss improved from 1.00403 to 1.00403
Epoch 03079: val_loss improved from 1.00403 to 1.00403
Epoch 03080: val_loss improved from 1.00403 to 1.00402
Epoch 03081: val_loss improved from 1.00402 to 1.00402
Epoch 03082: val_loss improved from 1.00402 to 1.00402
Epoch 03083: val_loss improved from 1.00402 to 1.00402
Epoch 03084: val_loss improved from 1.00402 to 1.00402
Epoch 03085: val_loss improved from 1.00402 to 1.00402
Epoch 03086: val_loss improved from 1.00402 to 1.00401
Epoch 03087: val_loss improved from 1.00401 to 1.00401
Epoch 03088: val_loss improved from 1.00401 to 1.00401
Epoch 03089: val_loss improved from 1.00401 to 1.00400
Epoch 03090: val_loss improved from 1.00400 to 1.00400
Epoch 03091: val_loss improved from 1.00400 to 1.00399
Epoch 03092: val_loss improved from 1.00399 to 1.00399
Epoch 03093: val_loss improved from 1.00399 to 1.00399
Epoch 03094: val_loss improved from 1.00399 to 1.00399
Epoch 03095: val_loss improved from 1.00399 to 1.00399
Epoch 03096: val_loss improved from 1.00399 to 1.00398
Epoch 03097: val_loss improved from 1.00398 to 1.00398
Epoch 03098: val_loss improved from 1.00398 to 1.00398
Epoch 03099: val_loss improved from 1.00398 to 1.00397
Epoch 03100: val_loss improved from 1.00397 to 1.00397
Epoch 03101: val_loss improved from 1.00397 to 1.00397
Epoch 03103: val_loss improved from 1.00397 to 1.00396
Epoch 03104: val_loss improved from 1.00396 to 1.00396
Epoch 03105: val_loss improved from 1.00396 to 1.00396
Epoch 03106: val_loss improved from 1.00396 to 1.00396
Epoch 03108: val_loss improved from 1.00396 to 1.00395
Epoch 03109: val_loss improved from 1.00395 to 1.00395
Epoch 03110: val_loss improved from 1.00395 to 1.00395
Epoch 03111: val_loss improved from 1.00395 to 1.00395
Epoch 03112: val_loss improved from 1.00395 to 1.00394
Epoch 03113: val_loss improved from 1.00394 to 1.00394
Epoch 03114: val_loss improved from 1.00394 to 1.00393
Epoch 03115: val_loss improved from 1.00393 to 1.00393
Epoch 03116: val_loss improved from 1.00393 to 1.00392
Epoch 03117: val_loss improved from 1.00392 to 1.00392
Epoch 03118: val_loss improved from 1.00392 to 1.00392
Epoch 03119: val_loss improved from 1.00392 to 1.00392
Epoch 03120: val_loss improved from 1.00392 to 1.00391
Epoch 03121: val_loss improved from 1.00391 to 1.00391
Epoch 03122: val_loss improved from 1.00391 to 1.00391
Epoch 03123: val_loss improved from 1.00391 to 1.00390
Epoch 03124: val_loss improved from 1.00390 to 1.00390
Epoch 03125: val_loss improved from 1.00390 to 1.00389
Epoch 03126: val_loss improved from 1.00389 to 1.00389
Epoch 03127: val_loss improved from 1.00389 to 1.00388
Epoch 03128: val_loss improved from 1.00388 to 1.00388
Epoch 03129: val_loss improved from 1.00388 to 1.00388
Epoch 03131: val_loss improved from 1.00388 to 1.00388
Epoch 03132: val_loss improved from 1.00388 to 1.00388
Epoch 03133: val_loss improved from 1.00388 to 1.00387
Epoch 03134: val_loss improved from 1.00387 to 1.00387
Epoch 03135: val_loss improved from 1.00387 to 1.00387
Epoch 03139: val_loss improved from 1.00387 to 1.00387
Epoch 03168: val_loss improved from 1.00387 to 1.00387
Epoch 03169: val_loss improved from 1.00387 to 1.00387
Epoch 03170: val_loss improved from 1.00387 to 1.00387
Epoch 03171: val_loss improved from 1.00387 to 1.00386
Epoch 03172: val_loss improved from 1.00386 to 1.00386
Epoch 03173: val_loss improved from 1.00386 to 1.00386
Epoch 03174: val_loss improved from 1.00386 to 1.00386
Epoch 03175: val_loss improved from 1.00386 to 1.00386
Epoch 03176: val_loss improved from 1.00386 to 1.00386
Epoch 03177: val_loss improved from 1.00386 to 1.00385
Epoch 03178: val_loss improved from 1.00385 to 1.00385
Epoch 03179: val_loss improved from 1.00385 to 1.00385
Epoch 03181: val_loss improved from 1.00385 to 1.00385
Epoch 03185: val_loss improved from 1.00385 to 1.00385
Epoch 03186: val_loss improved from 1.00385 to 1.00385
Epoch 03187: val_loss improved from 1.00385 to 1.00384
Epoch 03188: val_loss improved from 1.00384 to 1.00384
Epoch 03190: val_loss improved from 1.00384 to 1.00384
Epoch 03191: val_loss improved from 1.00384 to 1.00384
Epoch 03197: val_loss improved from 1.00384 to 1.00384
Epoch 03198: val_loss improved from 1.00384 to 1.00384
Epoch 03199: val_loss improved from 1.00384 to 1.00383
Epoch 03200: val_loss improved from 1.00383 to 1.00383
Epoch 03201: val_loss improved from 1.00383 to 1.00383
Epoch 03202: val_loss improved from 1.00383 to 1.00383
Epoch 03203: val_loss improved from 1.00383 to 1.00383
Epoch 03204: val_loss improved from 1.00383 to 1.00382
Epoch 03205: val_loss improved from 1.00382 to 1.00382
Epoch 03206: val_loss improved from 1.00382 to 1.00382
Epoch 03207: val_loss improved from 1.00382 to 1.00382
Epoch 03208: val_loss improved from 1.00382 to 1.00382
Epoch 03225: val_loss improved from 1.00382 to 1.00382
Epoch 03227: val_loss improved from 1.00382 to 1.00381
Epoch 03228: val_loss improved from 1.00381 to 1.00381
Epoch 03230: val_loss improved from 1.00381 to 1.00381
Epoch 03232: val_loss improved from 1.00381 to 1.00381
Epoch 03238: val_loss improved from 1.00381 to 1.00381
Epoch 03239: val_loss improved from 1.00381 to 1.00381
Epoch 03240: val_loss improved from 1.00381 to 1.00381
Epoch 03254: val_loss improved from 1.00381 to 1.00381
Epoch 03255: val_loss improved from 1.00381 to 1.00380
Epoch 03256: val_loss improved from 1.00380 to 1.00380
Epoch 03257: val_loss improved from 1.00380 to 1.00380
Epoch 03258: val_loss improved from 1.00380 to 1.00379
Epoch 03259: val_loss improved from 1.00379 to 1.00379
Epoch 03260: val_loss improved from 1.00379 to 1.00379
Epoch 03261: val_loss improved from 1.00379 to 1.00379
Epoch 03262: val_loss improved from 1.00379 to 1.00379
Epoch 03263: val_loss improved from 1.00379 to 1.00378
Epoch 03264: val_loss improved from 1.00378 to 1.00378
Epoch 03265: val_loss improved from 1.00378 to 1.00378
Epoch 03266: val_loss improved from 1.00378 to 1.00378
Epoch 03267: val_loss improved from 1.00378 to 1.00377
Epoch 03268: val_loss improved from 1.00377 to 1.00377
Epoch 03269: val_loss improved from 1.00377 to 1.00377
Epoch 03270: val_loss improved from 1.00377 to 1.00377
Epoch 03271: val_loss improved from 1.00377 to 1.00376
Epoch 03272: val_loss improved from 1.00376 to 1.00376
Epoch 03273: val_loss improved from 1.00376 to 1.00376
Epoch 03274: val_loss improved from 1.00376 to 1.00375
Epoch 03275: val_loss improved from 1.00375 to 1.00375
Epoch 03276: val_loss improved from 1.00375 to 1.00375
Epoch 03277: val_loss improved from 1.00375 to 1.00375
Epoch 03278: val_loss improved from 1.00375 to 1.00375
Epoch 03279: val_loss improved from 1.00375 to 1.00375
Epoch 03280: val_loss improved from 1.00375 to 1.00375
Epoch 03281: val_loss improved from 1.00375 to 1.00374
Epoch 03284: val_loss improved from 1.00374 to 1.00374
Epoch 03285: val_loss improved from 1.00374 to 1.00374
Epoch 03286: val_loss improved from 1.00374 to 1.00374
Epoch 03287: val_loss improved from 1.00374 to 1.00374
Epoch 03288: val_loss improved from 1.00374 to 1.00373
Epoch 03289: val_loss improved from 1.00373 to 1.00372
Epoch 03290: val_loss improved from 1.00372 to 1.00371
Epoch 03291: val_loss improved from 1.00371 to 1.00370
Epoch 03292: val_loss improved from 1.00370 to 1.00370
Epoch 03293: val_loss improved from 1.00370 to 1.00370
Epoch 03294: val_loss improved from 1.00370 to 1.00370
Epoch 03295: val_loss improved from 1.00370 to 1.00370
Epoch 03586: val_loss improved from 1.00370 to 1.00369
Epoch 03587: val_loss improved from 1.00369 to 1.00364
Epoch 03588: val_loss improved from 1.00364 to 1.00352
Epoch 03589: val_loss improved from 1.00352 to 1.00342
Epoch 03590: val_loss improved from 1.00342 to 1.00340
Epoch 03591: val_loss improved from 1.00340 to 1.00338
Epoch 03592: val_loss improved from 1.00338 to 1.00334
Epoch 03593: val_loss improved from 1.00334 to 1.00332
Epoch 03594: val_loss improved from 1.00332 to 1.00330
Epoch 03595: val_loss improved from 1.00330 to 1.00328
Epoch 03626: val_loss improved from 1.00328 to 1.00327
Epoch 03627: val_loss improved from 1.00327 to 1.00326
Epoch 03628: val_loss improved from 1.00326 to 1.00326
Epoch 03629: val_loss improved from 1.00326 to 1.00325
Epoch 03630: val_loss improved from 1.00325 to 1.00324
Epoch 03631: val_loss improved from 1.00324 to 1.00324
Epoch 03632: val_loss improved from 1.00324 to 1.00323
Epoch 03633: val_loss improved from 1.00323 to 1.00323
Epoch 03634: val_loss improved from 1.00323 to 1.00322
Epoch 03635: val_loss improved from 1.00322 to 1.00322
Epoch 03636: val_loss improved from 1.00322 to 1.00321
Epoch 03637: val_loss improved from 1.00321 to 1.00320
Epoch 03638: val_loss improved from 1.00320 to 1.00319
Epoch 03639: val_loss improved from 1.00319 to 1.00319
Epoch 03640: val_loss improved from 1.00319 to 1.00318
Epoch 03641: val_loss improved from 1.00318 to 1.00318
Epoch 03642: val_loss improved from 1.00318 to 1.00317
Epoch 03643: val_loss improved from 1.00317 to 1.00317
Epoch 03644: val_loss improved from 1.00317 to 1.00317
Epoch 03645: val_loss improved from 1.00317 to 1.00317
Epoch 03646: val_loss improved from 1.00317 to 1.00316
Epoch 03647: val_loss improved from 1.00316 to 1.00315
Epoch 03648: val_loss improved from 1.00315 to 1.00313
Epoch 03649: val_loss improved from 1.00313 to 1.00312
Epoch 03650: val_loss improved from 1.00312 to 1.00312
Epoch 03651: val_loss improved from 1.00312 to 1.00310
Epoch 03652: val_loss improved from 1.00310 to 1.00309
Epoch 03653: val_loss improved from 1.00309 to 1.00308
Epoch 03654: val_loss improved from 1.00308 to 1.00305
Epoch 03655: val_loss improved from 1.00305 to 1.00305
Epoch 03656: val_loss improved from 1.00305 to 1.00302
Epoch 03657: val_loss improved from 1.00302 to 1.00299
Epoch 03658: val_loss improved from 1.00299 to 1.00297
Epoch 03659: val_loss improved from 1.00297 to 1.00294
Epoch 03660: val_loss improved from 1.00294 to 1.00291
Epoch 03661: val_loss improved from 1.00291 to 1.00291
Epoch 03662: val_loss improved from 1.00291 to 1.00290
Epoch 03663: val_loss improved from 1.00290 to 1.00290
Epoch 03664: val_loss improved from 1.00290 to 1.00290
Epoch 03681: val_loss improved from 1.00290 to 1.00289
Epoch 03682: val_loss improved from 1.00289 to 1.00286
Epoch 03683: val_loss improved from 1.00286 to 1.00282
Epoch 03684: val_loss improved from 1.00282 to 1.00279
Epoch 03685: val_loss improved from 1.00279 to 1.00275
Epoch 03686: val_loss improved from 1.00275 to 1.00272
Epoch 03687: val_loss improved from 1.00272 to 1.00269
Epoch 03688: val_loss improved from 1.00269 to 1.00265
Epoch 03689: val_loss improved from 1.00265 to 1.00263
Epoch 03690: val_loss improved from 1.00263 to 1.00262
Epoch 03717: val_loss improved from 1.00262 to 1.00262
Epoch 03718: val_loss improved from 1.00262 to 1.00262
Epoch 03719: val_loss improved from 1.00262 to 1.00262
Epoch 03724: val_loss improved from 1.00262 to 1.00261
Epoch 03726: val_loss improved from 1.00261 to 1.00253
Epoch 03727: val_loss improved from 1.00253 to 1.00247
Epoch 03728: val_loss improved from 1.00247 to 1.00242
Epoch 03729: val_loss improved from 1.00242 to 1.00239
Epoch 03730: val_loss improved from 1.00239 to 1.00239
Epoch 03745: val_loss improved from 1.00239 to 1.00231
Epoch 03746: val_loss improved from 1.00231 to 1.00223
Epoch 03747: val_loss improved from 1.00223 to 1.00221
Epoch 03748: val_loss improved from 1.00221 to 1.00219
Epoch 03749: val_loss improved from 1.00219 to 1.00217
Epoch 03767: val_loss improved from 1.00217 to 1.00207
Epoch 03768: val_loss improved from 1.00207 to 1.00205
Epoch 03785: val_loss improved from 1.00205 to 1.00204
Epoch 03786: val_loss improved from 1.00204 to 1.00198
Epoch 03787: val_loss improved from 1.00198 to 1.00194
Epoch 03788: val_loss improved from 1.00194 to 1.00189
Epoch 03789: val_loss improved from 1.00189 to 1.00184
Epoch 03790: val_loss improved from 1.00184 to 1.00177
Epoch 03791: val_loss improved from 1.00177 to 1.00163
Epoch 03792: val_loss improved from 1.00163 to 1.00148
Epoch 03793: val_loss improved from 1.00148 to 1.00140
Epoch 03794: val_loss improved from 1.00140 to 1.00138
Epoch 03802: val_loss improved from 1.00138 to 1.00131
Epoch 03803: val_loss improved from 1.00131 to 1.00129
Epoch 03804: val_loss improved from 1.00129 to 1.00115
Epoch 03805: val_loss improved from 1.00115 to 1.00107
Epoch 03806: val_loss improved from 1.00107 to 1.00093
Epoch 03807: val_loss improved from 1.00093 to 1.00081
Epoch 03810: val_loss improved from 1.00081 to 1.00072
Epoch 03812: val_loss improved from 1.00072 to 1.00063
Epoch 03834: val_loss improved from 1.00063 to 0.99981
Epoch 03835: val_loss improved from 0.99981 to 0.99943
Epoch 03837: val_loss improved from 0.99943 to 0.99932
Epoch 03841: val_loss improved from 0.99932 to 0.99897
Epoch 03843: val_loss improved from 0.99897 to 0.99845
Epoch 03901: val_loss improved from 0.99845 to 0.99845
Epoch 03916: val_loss improved from 0.99845 to 0.99821
Epoch 03917: val_loss improved from 0.99821 to 0.99816
Epoch 03918: val_loss improved from 0.99816 to 0.99809
Epoch 03951: val_loss improved from 0.99809 to 0.99770
Epoch 03952: val_loss improved from 0.99770 to 0.99703
Epoch 03953: val_loss improved from 0.99703 to 0.99680
Epoch 03954: val_loss improved from 0.99680 to 0.99658
Epoch 03955: val_loss improved from 0.99658 to 0.99651
Epoch 03956: val_loss improved from 0.99651 to 0.99644
Epoch 03957: val_loss improved from 0.99644 to 0.99623
Epoch 03958: val_loss improved from 0.99623 to 0.99607
Epoch 03959: val_loss improved from 0.99607 to 0.99600
Epoch 03960: val_loss improved from 0.99600 to 0.99594
Epoch 03961: val_loss improved from 0.99594 to 0.99585
Epoch 03962: val_loss improved from 0.99585 to 0.99579
Epoch 03964: val_loss improved from 0.99579 to 0.99570
Epoch 03965: val_loss improved from 0.99570 to 0.99566
Epoch 03966: val_loss improved from 0.99566 to 0.99564
Epoch 03971: val_loss improved from 0.99564 to 0.99562
[training]  masked spectral distance: 0.9979713559150696, approx. progress: 0.70%
Epoch 04124: val_loss improved from 0.99562 to 0.99558
Epoch 04125: val_loss improved from 0.99558 to 0.99554
Epoch 04126: val_loss improved from 0.99554 to 0.99552
Epoch 04127: val_loss improved from 0.99552 to 0.99548
Epoch 04128: val_loss improved from 0.99548 to 0.99547
Epoch 04135: val_loss improved from 0.99547 to 0.99544
Epoch 04136: val_loss improved from 0.99544 to 0.99544
Epoch 04137: val_loss improved from 0.99544 to 0.99539
Epoch 04139: val_loss improved from 0.99539 to 0.99536
Epoch 04140: val_loss improved from 0.99536 to 0.99531
Epoch 04159: val_loss improved from 0.99531 to 0.99526
Epoch 04160: val_loss improved from 0.99526 to 0.99502
Epoch 04161: val_loss improved from 0.99502 to 0.99494
Epoch 04162: val_loss improved from 0.99494 to 0.99487
Epoch 04163: val_loss improved from 0.99487 to 0.99484
Epoch 04164: val_loss improved from 0.99484 to 0.99484
Epoch 04212: val_loss improved from 0.99484 to 0.99472
Epoch 04216: val_loss improved from 0.99472 to 0.99458
Epoch 04217: val_loss improved from 0.99458 to 0.99446
Epoch 04218: val_loss improved from 0.99446 to 0.99441
Epoch 04219: val_loss improved from 0.99441 to 0.99433
Epoch 04220: val_loss improved from 0.99433 to 0.99427
Epoch 04221: val_loss improved from 0.99427 to 0.99417
Epoch 04222: val_loss improved from 0.99417 to 0.99417
Epoch 04319: val_loss improved from 0.99417 to 0.99410
Epoch 04320: val_loss improved from 0.99410 to 0.99397
Epoch 04321: val_loss improved from 0.99397 to 0.99388
Epoch 04322: val_loss improved from 0.99388 to 0.99380
Epoch 04323: val_loss improved from 0.99380 to 0.99373
Epoch 04324: val_loss improved from 0.99373 to 0.99364
Epoch 04325: val_loss improved from 0.99364 to 0.99358
Epoch 04356: val_loss improved from 0.99358 to 0.99355
Epoch 04357: val_loss improved from 0.99355 to 0.99352
Epoch 04358: val_loss improved from 0.99352 to 0.99348
Epoch 04359: val_loss improved from 0.99348 to 0.99346
Epoch 04420: val_loss improved from 0.99346 to 0.99345
Epoch 04421: val_loss improved from 0.99345 to 0.99336
Epoch 04422: val_loss improved from 0.99336 to 0.99327
Epoch 04423: val_loss improved from 0.99327 to 0.99316
Epoch 04424: val_loss improved from 0.99316 to 0.99309
Epoch 04425: val_loss improved from 0.99309 to 0.99299
Epoch 04433: val_loss improved from 0.99299 to 0.99299
Epoch 04434: val_loss improved from 0.99299 to 0.99297
Epoch 04435: val_loss improved from 0.99297 to 0.99297
Epoch 04436: val_loss improved from 0.99297 to 0.99295
Epoch 04437: val_loss improved from 0.99295 to 0.99292
Epoch 04438: val_loss improved from 0.99292 to 0.99290
Epoch 04439: val_loss improved from 0.99290 to 0.99285
Epoch 04440: val_loss improved from 0.99285 to 0.99283
Epoch 04441: val_loss improved from 0.99283 to 0.99280
Epoch 04442: val_loss improved from 0.99280 to 0.99279
Epoch 04450: val_loss improved from 0.99279 to 0.99278
Epoch 04451: val_loss improved from 0.99278 to 0.99277
Epoch 04452: val_loss improved from 0.99277 to 0.99277
Epoch 04494: val_loss improved from 0.99277 to 0.99272
Epoch 04495: val_loss improved from 0.99272 to 0.99262
Epoch 04496: val_loss improved from 0.99262 to 0.99248
Epoch 04497: val_loss improved from 0.99248 to 0.99240
Epoch 04498: val_loss improved from 0.99240 to 0.99235
Epoch 04499: val_loss improved from 0.99235 to 0.99225
Epoch 04500: val_loss improved from 0.99225 to 0.99206
Epoch 04501: val_loss improved from 0.99206 to 0.99200
Epoch 04502: val_loss improved from 0.99200 to 0.99190
Epoch 04503: val_loss improved from 0.99190 to 0.99189
Epoch 04504: val_loss improved from 0.99189 to 0.99188
Epoch 04546: val_loss improved from 0.99188 to 0.99182
Epoch 04547: val_loss improved from 0.99182 to 0.99178
Epoch 04548: val_loss improved from 0.99178 to 0.99177
Epoch 04549: val_loss improved from 0.99177 to 0.99170
Epoch 04550: val_loss improved from 0.99170 to 0.99163
Epoch 04551: val_loss improved from 0.99163 to 0.99152
Epoch 04552: val_loss improved from 0.99152 to 0.99142
Epoch 04553: val_loss improved from 0.99142 to 0.99142
Epoch 04567: val_loss improved from 0.99142 to 0.99133
Epoch 04568: val_loss improved from 0.99133 to 0.99129
Epoch 04569: val_loss improved from 0.99129 to 0.99126
Epoch 04570: val_loss improved from 0.99126 to 0.99125
Epoch 04585: val_loss improved from 0.99125 to 0.99124
Epoch 04586: val_loss improved from 0.99124 to 0.99123
Epoch 04642: val_loss improved from 0.99123 to 0.99120
Epoch 04643: val_loss improved from 0.99120 to 0.99116
Epoch 04644: val_loss improved from 0.99116 to 0.99114
Epoch 04645: val_loss improved from 0.99114 to 0.99114
Epoch 04646: val_loss improved from 0.99114 to 0.99114
Epoch 04649: val_loss improved from 0.99114 to 0.99113
Epoch 04650: val_loss improved from 0.99113 to 0.99111
Epoch 04651: val_loss improved from 0.99111 to 0.99107
Epoch 04652: val_loss improved from 0.99107 to 0.99102
Epoch 04653: val_loss improved from 0.99102 to 0.99100
Epoch 04654: val_loss improved from 0.99100 to 0.99099
Epoch 04660: val_loss improved from 0.99099 to 0.99098
Epoch 04661: val_loss improved from 0.99098 to 0.99096
Epoch 04662: val_loss improved from 0.99096 to 0.99094
Epoch 04663: val_loss improved from 0.99094 to 0.99090
Epoch 04664: val_loss improved from 0.99090 to 0.99083
Epoch 04665: val_loss improved from 0.99083 to 0.99079
Epoch 04666: val_loss improved from 0.99079 to 0.99079
Epoch 04667: val_loss improved from 0.99079 to 0.99078
Epoch 04668: val_loss improved from 0.99078 to 0.99077
Epoch 04669: val_loss improved from 0.99077 to 0.99075
Epoch 04670: val_loss improved from 0.99075 to 0.99074
Epoch 04671: val_loss improved from 0.99074 to 0.99074
Epoch 04672: val_loss improved from 0.99074 to 0.99073
Epoch 04673: val_loss improved from 0.99073 to 0.99072
Epoch 04674: val_loss improved from 0.99072 to 0.99067
Epoch 04675: val_loss improved from 0.99067 to 0.99062
Epoch 04676: val_loss improved from 0.99062 to 0.99059
Epoch 04677: val_loss improved from 0.99059 to 0.99057
Epoch 04732: val_loss improved from 0.99057 to 0.99055
Epoch 04733: val_loss improved from 0.99055 to 0.99053
Epoch 04734: val_loss improved from 0.99053 to 0.99050
Epoch 04748: val_loss improved from 0.99050 to 0.99046
Epoch 04749: val_loss improved from 0.99046 to 0.99042
Epoch 04750: val_loss improved from 0.99042 to 0.99039
Epoch 04751: val_loss improved from 0.99039 to 0.99036
Epoch 04752: val_loss improved from 0.99036 to 0.99035
Epoch 04754: val_loss improved from 0.99035 to 0.99033
Epoch 04757: val_loss improved from 0.99033 to 0.99033
Epoch 04758: val_loss improved from 0.99033 to 0.99032
Epoch 04760: val_loss improved from 0.99032 to 0.99023
Epoch 04761: val_loss improved from 0.99023 to 0.99014
Epoch 04762: val_loss improved from 0.99014 to 0.99003
Epoch 04763: val_loss improved from 0.99003 to 0.98997
Epoch 04764: val_loss improved from 0.98997 to 0.98993
Epoch 04774: val_loss improved from 0.98993 to 0.98993
Epoch 04775: val_loss improved from 0.98993 to 0.98988
Epoch 04776: val_loss improved from 0.98988 to 0.98983
Epoch 04777: val_loss improved from 0.98983 to 0.98969
Epoch 04778: val_loss improved from 0.98969 to 0.98960
Epoch 04779: val_loss improved from 0.98960 to 0.98956
Epoch 04807: val_loss improved from 0.98956 to 0.98947
Epoch 04808: val_loss improved from 0.98947 to 0.98930
Epoch 04809: val_loss improved from 0.98930 to 0.98922
Epoch 04810: val_loss improved from 0.98922 to 0.98916
Epoch 04811: val_loss improved from 0.98916 to 0.98912
Epoch 04812: val_loss improved from 0.98912 to 0.98904
Epoch 04813: val_loss improved from 0.98904 to 0.98900
Epoch 04814: val_loss improved from 0.98900 to 0.98896
Epoch 04816: val_loss improved from 0.98896 to 0.98896
Epoch 04834: val_loss improved from 0.98896 to 0.98893
Epoch 04835: val_loss improved from 0.98893 to 0.98890
Epoch 04836: val_loss improved from 0.98890 to 0.98888
Epoch 04837: val_loss improved from 0.98888 to 0.98888
Epoch 04841: val_loss improved from 0.98888 to 0.98887
Epoch 04842: val_loss improved from 0.98887 to 0.98887
Epoch 04843: val_loss improved from 0.98887 to 0.98885
Epoch 04844: val_loss improved from 0.98885 to 0.98883
Epoch 04845: val_loss improved from 0.98883 to 0.98882
Epoch 04857: val_loss improved from 0.98882 to 0.98880
Epoch 04858: val_loss improved from 0.98880 to 0.98873
Epoch 04859: val_loss improved from 0.98873 to 0.98868
Epoch 04860: val_loss improved from 0.98868 to 0.98863
Epoch 04861: val_loss improved from 0.98863 to 0.98860
Epoch 04862: val_loss improved from 0.98860 to 0.98858
Epoch 04864: val_loss improved from 0.98858 to 0.98856
Epoch 04867: val_loss improved from 0.98856 to 0.98854
Epoch 04868: val_loss improved from 0.98854 to 0.98853
Epoch 04869: val_loss improved from 0.98853 to 0.98849
Epoch 04879: val_loss improved from 0.98849 to 0.98847
Epoch 04880: val_loss improved from 0.98847 to 0.98841
Epoch 04881: val_loss improved from 0.98841 to 0.98834
Epoch 04882: val_loss improved from 0.98834 to 0.98828
Epoch 04883: val_loss improved from 0.98828 to 0.98825
Epoch 04884: val_loss improved from 0.98825 to 0.98821
Epoch 04885: val_loss improved from 0.98821 to 0.98818
Epoch 04886: val_loss improved from 0.98818 to 0.98814
[training]  masked spectral distance: 0.9936265349388123, approx. progress: 1.18%
Epoch 05124: val_loss improved from 0.98814 to 0.98813
Epoch 05125: val_loss improved from 0.98813 to 0.98811
Epoch 05126: val_loss improved from 0.98811 to 0.98810
Epoch 05127: val_loss improved from 0.98810 to 0.98808
Epoch 05128: val_loss improved from 0.98808 to 0.98807
Epoch 05129: val_loss improved from 0.98807 to 0.98806
Epoch 05130: val_loss improved from 0.98806 to 0.98804
Epoch 05131: val_loss improved from 0.98804 to 0.98803
Epoch 05132: val_loss improved from 0.98803 to 0.98800
Epoch 05133: val_loss improved from 0.98800 to 0.98798
Epoch 05134: val_loss improved from 0.98798 to 0.98795
Epoch 05136: val_loss improved from 0.98795 to 0.98794
Epoch 05137: val_loss improved from 0.98794 to 0.98793
Epoch 05168: val_loss improved from 0.98793 to 0.98786
Epoch 05169: val_loss improved from 0.98786 to 0.98781
Epoch 05170: val_loss improved from 0.98781 to 0.98779
Epoch 05172: val_loss improved from 0.98779 to 0.98779
Epoch 05173: val_loss improved from 0.98779 to 0.98778
Epoch 05174: val_loss improved from 0.98778 to 0.98778
Epoch 05175: val_loss improved from 0.98778 to 0.98777
Epoch 05176: val_loss improved from 0.98777 to 0.98772
Epoch 05177: val_loss improved from 0.98772 to 0.98769
Epoch 05178: val_loss improved from 0.98769 to 0.98758
Epoch 05179: val_loss improved from 0.98758 to 0.98745
Epoch 05180: val_loss improved from 0.98745 to 0.98733
Epoch 05181: val_loss improved from 0.98733 to 0.98714
Epoch 05182: val_loss improved from 0.98714 to 0.98711
Epoch 05196: val_loss improved from 0.98711 to 0.98707
Epoch 05202: val_loss improved from 0.98707 to 0.98705
Epoch 05251: val_loss improved from 0.98705 to 0.98697
Epoch 05252: val_loss improved from 0.98697 to 0.98690
Epoch 05265: val_loss improved from 0.98690 to 0.98684
Epoch 05266: val_loss improved from 0.98684 to 0.98673
Epoch 05267: val_loss improved from 0.98673 to 0.98661
Epoch 05268: val_loss improved from 0.98661 to 0.98645
Epoch 05269: val_loss improved from 0.98645 to 0.98636
Epoch 05270: val_loss improved from 0.98636 to 0.98632
Epoch 05542: val_loss improved from 0.98632 to 0.98628
Epoch 05543: val_loss improved from 0.98628 to 0.98621
Epoch 05544: val_loss improved from 0.98621 to 0.98617
Epoch 05545: val_loss improved from 0.98617 to 0.98612
Epoch 05546: val_loss improved from 0.98612 to 0.98610
Epoch 05547: val_loss improved from 0.98610 to 0.98606
Epoch 05548: val_loss improved from 0.98606 to 0.98602
Epoch 05549: val_loss improved from 0.98602 to 0.98596
Epoch 05550: val_loss improved from 0.98596 to 0.98591
Epoch 05551: val_loss improved from 0.98591 to 0.98586
Epoch 05552: val_loss improved from 0.98586 to 0.98583
Epoch 05555: val_loss improved from 0.98583 to 0.98582
Epoch 05688: val_loss improved from 0.98582 to 0.98581
Epoch 05689: val_loss improved from 0.98581 to 0.98579
Epoch 05690: val_loss improved from 0.98579 to 0.98576
Epoch 05691: val_loss improved from 0.98576 to 0.98571
Epoch 05692: val_loss improved from 0.98571 to 0.98569
Epoch 05693: val_loss improved from 0.98569 to 0.98568
Epoch 05694: val_loss improved from 0.98568 to 0.98567
Epoch 05695: val_loss improved from 0.98567 to 0.98566
Epoch 05702: val_loss improved from 0.98566 to 0.98557
Epoch 05703: val_loss improved from 0.98557 to 0.98542
Epoch 05704: val_loss improved from 0.98542 to 0.98531
Epoch 05705: val_loss improved from 0.98531 to 0.98526
Epoch 05706: val_loss improved from 0.98526 to 0.98524
Epoch 05707: val_loss improved from 0.98524 to 0.98522
Epoch 05709: val_loss improved from 0.98522 to 0.98522
Epoch 05710: val_loss improved from 0.98522 to 0.98521
Epoch 05711: val_loss improved from 0.98521 to 0.98521
Epoch 05716: val_loss improved from 0.98521 to 0.98520
Epoch 05717: val_loss improved from 0.98520 to 0.98519
Epoch 05735: val_loss improved from 0.98519 to 0.98517
Epoch 05736: val_loss improved from 0.98517 to 0.98515
Epoch 05737: val_loss improved from 0.98515 to 0.98513
Epoch 05738: val_loss improved from 0.98513 to 0.98511
Epoch 05739: val_loss improved from 0.98511 to 0.98511
Epoch 05740: val_loss improved from 0.98511 to 0.98510
Epoch 05741: val_loss improved from 0.98510 to 0.98509
Epoch 05742: val_loss improved from 0.98509 to 0.98508
Epoch 05743: val_loss improved from 0.98508 to 0.98507
Epoch 05744: val_loss improved from 0.98507 to 0.98506
Epoch 05745: val_loss improved from 0.98506 to 0.98505
Epoch 05746: val_loss improved from 0.98505 to 0.98505
Epoch 05747: val_loss improved from 0.98505 to 0.98504
Epoch 05748: val_loss improved from 0.98504 to 0.98503
Epoch 05762: val_loss improved from 0.98503 to 0.98498
Epoch 05763: val_loss improved from 0.98498 to 0.98483
Epoch 05764: val_loss improved from 0.98483 to 0.98475
Epoch 05765: val_loss improved from 0.98475 to 0.98466
Epoch 05766: val_loss improved from 0.98466 to 0.98456
Epoch 05767: val_loss improved from 0.98456 to 0.98444
Epoch 05768: val_loss improved from 0.98444 to 0.98442
Epoch 05775: val_loss improved from 0.98442 to 0.98439
Epoch 05776: val_loss improved from 0.98439 to 0.98435
Epoch 05777: val_loss improved from 0.98435 to 0.98430
Epoch 05778: val_loss improved from 0.98430 to 0.98425
Epoch 05779: val_loss improved from 0.98425 to 0.98423
Epoch 05780: val_loss improved from 0.98423 to 0.98419
Epoch 05782: val_loss improved from 0.98419 to 0.98416
Epoch 05783: val_loss improved from 0.98416 to 0.98413
Epoch 05784: val_loss improved from 0.98413 to 0.98407
Epoch 05785: val_loss improved from 0.98407 to 0.98406
Epoch 05786: val_loss improved from 0.98406 to 0.98403
Epoch 05787: val_loss improved from 0.98403 to 0.98397
Epoch 05788: val_loss improved from 0.98397 to 0.98393
Epoch 05789: val_loss improved from 0.98393 to 0.98392
Epoch 05790: val_loss improved from 0.98392 to 0.98391
Epoch 05817: val_loss improved from 0.98391 to 0.98372
Epoch 05818: val_loss improved from 0.98372 to 0.98353
Epoch 05819: val_loss improved from 0.98353 to 0.98340
Epoch 05820: val_loss improved from 0.98340 to 0.98335
Epoch 05821: val_loss improved from 0.98335 to 0.98334
Epoch 05822: val_loss improved from 0.98334 to 0.98333
Epoch 05824: val_loss improved from 0.98333 to 0.98327
Epoch 05825: val_loss improved from 0.98327 to 0.98324
Epoch 05826: val_loss improved from 0.98324 to 0.98318
Epoch 05827: val_loss improved from 0.98318 to 0.98307
Epoch 05828: val_loss improved from 0.98307 to 0.98300
Epoch 05829: val_loss improved from 0.98300 to 0.98292
Epoch 05830: val_loss improved from 0.98292 to 0.98285
Epoch 05831: val_loss improved from 0.98285 to 0.98284
Epoch 05832: val_loss improved from 0.98284 to 0.98281
Epoch 05836: val_loss improved from 0.98281 to 0.98276
Epoch 05837: val_loss improved from 0.98276 to 0.98271
Epoch 05838: val_loss improved from 0.98271 to 0.98264
Epoch 05839: val_loss improved from 0.98264 to 0.98260
Epoch 05840: val_loss improved from 0.98260 to 0.98259
Epoch 05841: val_loss improved from 0.98259 to 0.98253
Epoch 05842: val_loss improved from 0.98253 to 0.98251
Epoch 05851: val_loss improved from 0.98251 to 0.98250
Epoch 05852: val_loss improved from 0.98250 to 0.98249
Epoch 05853: val_loss improved from 0.98249 to 0.98249
Epoch 05854: val_loss improved from 0.98249 to 0.98245
Epoch 05855: val_loss improved from 0.98245 to 0.98239
Epoch 05856: val_loss improved from 0.98239 to 0.98235
Epoch 05984: val_loss improved from 0.98235 to 0.98232
Epoch 05985: val_loss improved from 0.98232 to 0.98222
Epoch 05986: val_loss improved from 0.98222 to 0.98212
Epoch 05987: val_loss improved from 0.98212 to 0.98202
Epoch 05988: val_loss improved from 0.98202 to 0.98192
Epoch 05996: val_loss improved from 0.98192 to 0.98185
Epoch 05997: val_loss improved from 0.98185 to 0.98180
[training]  masked spectral distance: 0.9897512197494507, approx. progress: 1.61%
Epoch 06010: val_loss improved from 0.98180 to 0.98180
Epoch 06011: val_loss improved from 0.98180 to 0.98175
Epoch 06012: val_loss improved from 0.98175 to 0.98174
Epoch 06015: val_loss improved from 0.98174 to 0.98169
Epoch 06066: val_loss improved from 0.98169 to 0.98166
Epoch 06067: val_loss improved from 0.98166 to 0.98161
Epoch 06068: val_loss improved from 0.98161 to 0.98160
Epoch 06069: val_loss improved from 0.98160 to 0.98159
Epoch 06070: val_loss improved from 0.98159 to 0.98158
Epoch 06071: val_loss improved from 0.98158 to 0.98158
Epoch 06072: val_loss improved from 0.98158 to 0.98157
Epoch 06113: val_loss improved from 0.98157 to 0.98150
Epoch 06114: val_loss improved from 0.98150 to 0.98137
Epoch 06115: val_loss improved from 0.98137 to 0.98129
Epoch 06121: val_loss improved from 0.98129 to 0.98123
Epoch 06122: val_loss improved from 0.98123 to 0.98118
Epoch 06123: val_loss improved from 0.98118 to 0.98117
Epoch 06134: val_loss improved from 0.98117 to 0.98115
Epoch 06135: val_loss improved from 0.98115 to 0.98107
Epoch 06136: val_loss improved from 0.98107 to 0.98106
Epoch 06137: val_loss improved from 0.98106 to 0.98101
Epoch 06138: val_loss improved from 0.98101 to 0.98097
Epoch 06139: val_loss improved from 0.98097 to 0.98095
Epoch 06140: val_loss improved from 0.98095 to 0.98091
Epoch 06141: val_loss improved from 0.98091 to 0.98083
Epoch 06142: val_loss improved from 0.98083 to 0.98079
Epoch 06143: val_loss improved from 0.98079 to 0.98077
Epoch 06145: val_loss improved from 0.98077 to 0.98075
Epoch 06146: val_loss improved from 0.98075 to 0.98075
Epoch 06187: val_loss improved from 0.98075 to 0.98074
Epoch 06188: val_loss improved from 0.98074 to 0.98073
Epoch 06189: val_loss improved from 0.98073 to 0.98071
Epoch 06190: val_loss improved from 0.98071 to 0.98069
Epoch 06191: val_loss improved from 0.98069 to 0.98069
Epoch 06192: val_loss improved from 0.98069 to 0.98067
Epoch 06193: val_loss improved from 0.98067 to 0.98065
Epoch 06194: val_loss improved from 0.98065 to 0.98060
Epoch 06195: val_loss improved from 0.98060 to 0.98054
Epoch 06196: val_loss improved from 0.98054 to 0.98048
Epoch 06197: val_loss improved from 0.98048 to 0.98047
Epoch 06198: val_loss improved from 0.98047 to 0.98046
Epoch 06199: val_loss improved from 0.98046 to 0.98041
Epoch 06222: val_loss improved from 0.98041 to 0.98041
Epoch 06223: val_loss improved from 0.98041 to 0.98034
Epoch 06224: val_loss improved from 0.98034 to 0.98032
Epoch 06225: val_loss improved from 0.98032 to 0.98031
Epoch 06226: val_loss improved from 0.98031 to 0.98031
Epoch 06228: val_loss improved from 0.98031 to 0.98031
Epoch 06229: val_loss improved from 0.98031 to 0.98030
Epoch 06230: val_loss improved from 0.98030 to 0.98030
Epoch 06231: val_loss improved from 0.98030 to 0.98030
Epoch 06268: val_loss improved from 0.98030 to 0.98029
Epoch 06269: val_loss improved from 0.98029 to 0.98026
Epoch 06270: val_loss improved from 0.98026 to 0.98019
Epoch 06271: val_loss improved from 0.98019 to 0.98017
Epoch 06274: val_loss improved from 0.98017 to 0.98012
Epoch 06287: val_loss improved from 0.98012 to 0.98009
Epoch 06288: val_loss improved from 0.98009 to 0.98003
Epoch 06289: val_loss improved from 0.98003 to 0.98003
Epoch 06334: val_loss improved from 0.98003 to 0.97992
Epoch 06335: val_loss improved from 0.97992 to 0.97985
Epoch 06336: val_loss improved from 0.97985 to 0.97981
Epoch 06337: val_loss improved from 0.97981 to 0.97978
Epoch 06338: val_loss improved from 0.97978 to 0.97975
Epoch 06339: val_loss improved from 0.97975 to 0.97973
Epoch 06340: val_loss improved from 0.97973 to 0.97970
Epoch 06341: val_loss improved from 0.97970 to 0.97969
Epoch 06342: val_loss improved from 0.97969 to 0.97968
Epoch 06423: val_loss improved from 0.97968 to 0.97960
Epoch 06424: val_loss improved from 0.97960 to 0.97956
Epoch 06425: val_loss improved from 0.97956 to 0.97951
Epoch 06426: val_loss improved from 0.97951 to 0.97950
Epoch 06511: val_loss improved from 0.97950 to 0.97950
Epoch 06512: val_loss improved from 0.97950 to 0.97947
Epoch 06513: val_loss improved from 0.97947 to 0.97945
Epoch 06514: val_loss improved from 0.97945 to 0.97944
Epoch 06516: val_loss improved from 0.97944 to 0.97943
Epoch 06517: val_loss improved from 0.97943 to 0.97942
Epoch 06518: val_loss improved from 0.97942 to 0.97940
Epoch 06664: val_loss improved from 0.97940 to 0.97929
Epoch 06665: val_loss improved from 0.97929 to 0.97909
Epoch 06666: val_loss improved from 0.97909 to 0.97890
Epoch 06667: val_loss improved from 0.97890 to 0.97872
Epoch 06668: val_loss improved from 0.97872 to 0.97862
Epoch 06669: val_loss improved from 0.97862 to 0.97855
Epoch 06670: val_loss improved from 0.97855 to 0.97846
Epoch 06671: val_loss improved from 0.97846 to 0.97836
Epoch 06672: val_loss improved from 0.97836 to 0.97825
Epoch 06673: val_loss improved from 0.97825 to 0.97816
Epoch 06674: val_loss improved from 0.97816 to 0.97810
Epoch 06675: val_loss improved from 0.97810 to 0.97804
Epoch 06676: val_loss improved from 0.97804 to 0.97794
Epoch 06677: val_loss improved from 0.97794 to 0.97789
Epoch 06678: val_loss improved from 0.97789 to 0.97785
Epoch 06679: val_loss improved from 0.97785 to 0.97784
Epoch 06684: val_loss improved from 0.97784 to 0.97783
Epoch 06685: val_loss improved from 0.97783 to 0.97775
Epoch 06686: val_loss improved from 0.97775 to 0.97774
Epoch 06757: val_loss improved from 0.97774 to 0.97773
Epoch 06785: val_loss improved from 0.97773 to 0.97772
Epoch 06786: val_loss improved from 0.97772 to 0.97757
Epoch 06787: val_loss improved from 0.97757 to 0.97734
Epoch 06788: val_loss improved from 0.97734 to 0.97713
Epoch 06818: val_loss improved from 0.97713 to 0.97709
Epoch 06819: val_loss improved from 0.97709 to 0.97706
Epoch 06823: val_loss improved from 0.97706 to 0.97698
Epoch 06824: val_loss improved from 0.97698 to 0.97689
Epoch 06825: val_loss improved from 0.97689 to 0.97683
Epoch 06846: val_loss improved from 0.97683 to 0.97678
Epoch 06847: val_loss improved from 0.97678 to 0.97669
Epoch 06848: val_loss improved from 0.97669 to 0.97664
Epoch 06849: val_loss improved from 0.97664 to 0.97660
Epoch 06850: val_loss improved from 0.97660 to 0.97656
Epoch 06851: val_loss improved from 0.97656 to 0.97655
Epoch 06865: val_loss improved from 0.97655 to 0.97655
Epoch 06975: val_loss improved from 0.97655 to 0.97652
Epoch 06976: val_loss improved from 0.97652 to 0.97647
Epoch 06977: val_loss improved from 0.97647 to 0.97643
Epoch 06978: val_loss improved from 0.97643 to 0.97638
Epoch 06979: val_loss improved from 0.97638 to 0.97636
Epoch 06980: val_loss improved from 0.97636 to 0.97635
Epoch 06981: val_loss improved from 0.97635 to 0.97633
Epoch 06982: val_loss improved from 0.97633 to 0.97632
Epoch 06983: val_loss improved from 0.97632 to 0.97631
Epoch 06984: val_loss improved from 0.97631 to 0.97628
Epoch 06985: val_loss improved from 0.97628 to 0.97626
Epoch 06986: val_loss improved from 0.97626 to 0.97624
Epoch 06993: val_loss improved from 0.97624 to 0.97621
Epoch 06994: val_loss improved from 0.97621 to 0.97618
Epoch 06995: val_loss improved from 0.97618 to 0.97617
Epoch 06996: val_loss improved from 0.97617 to 0.97617
Epoch 06997: val_loss improved from 0.97617 to 0.97615
Epoch 06998: val_loss improved from 0.97615 to 0.97614
[training]  masked spectral distance: 0.9888243675231934, approx. progress: 1.71%
Epoch 07002: val_loss improved from 0.97614 to 0.97613
Epoch 07003: val_loss improved from 0.97613 to 0.97610
Epoch 07004: val_loss improved from 0.97610 to 0.97608
Epoch 07005: val_loss improved from 0.97608 to 0.97603
Epoch 07006: val_loss improved from 0.97603 to 0.97599
Epoch 07007: val_loss improved from 0.97599 to 0.97596
Epoch 07008: val_loss improved from 0.97596 to 0.97596
Epoch 07009: val_loss improved from 0.97596 to 0.97594
Epoch 07011: val_loss improved from 0.97594 to 0.97592
Epoch 07012: val_loss improved from 0.97592 to 0.97591
Epoch 07013: val_loss improved from 0.97591 to 0.97590
Epoch 07015: val_loss improved from 0.97590 to 0.97583
Epoch 07016: val_loss improved from 0.97583 to 0.97571
Epoch 07017: val_loss improved from 0.97571 to 0.97564
Epoch 07095: val_loss improved from 0.97564 to 0.97548
Epoch 07096: val_loss improved from 0.97548 to 0.97541
Epoch 07097: val_loss improved from 0.97541 to 0.97537
Epoch 07102: val_loss improved from 0.97537 to 0.97526
Epoch 07103: val_loss improved from 0.97526 to 0.97513
Epoch 07104: val_loss improved from 0.97513 to 0.97495
Epoch 07105: val_loss improved from 0.97495 to 0.97481
Epoch 07106: val_loss improved from 0.97481 to 0.97469
Epoch 07107: val_loss improved from 0.97469 to 0.97462
Epoch 07121: val_loss improved from 0.97462 to 0.97461
Epoch 07122: val_loss improved from 0.97461 to 0.97456
Epoch 07123: val_loss improved from 0.97456 to 0.97452
Epoch 07124: val_loss improved from 0.97452 to 0.97448
Epoch 07125: val_loss improved from 0.97448 to 0.97443
Epoch 07126: val_loss improved from 0.97443 to 0.97441
Epoch 07127: val_loss improved from 0.97441 to 0.97438
Epoch 07221: val_loss improved from 0.97438 to 0.97420
Epoch 07222: val_loss improved from 0.97420 to 0.97418
Epoch 07235: val_loss improved from 0.97418 to 0.97372
Epoch 07236: val_loss improved from 0.97372 to 0.97340
Epoch 07237: val_loss improved from 0.97340 to 0.97316
Epoch 07258: val_loss improved from 0.97316 to 0.97300
Epoch 07259: val_loss improved from 0.97300 to 0.97269
Epoch 07260: val_loss improved from 0.97269 to 0.97247
Epoch 07284: val_loss improved from 0.97247 to 0.97245
Epoch 07285: val_loss improved from 0.97245 to 0.97241
Epoch 07286: val_loss improved from 0.97241 to 0.97226
Epoch 07287: val_loss improved from 0.97226 to 0.97210
Epoch 07288: val_loss improved from 0.97210 to 0.97203
Epoch 07291: val_loss improved from 0.97203 to 0.97199
Epoch 07292: val_loss improved from 0.97199 to 0.97192
Epoch 07293: val_loss improved from 0.97192 to 0.97186
Epoch 07294: val_loss improved from 0.97186 to 0.97180
Epoch 07296: val_loss improved from 0.97180 to 0.97179
validation loss: 0.9777208566665649, pearson distance: 0.9659093618392944
[training]  masked spectral distance: 0.9862672090530396, approx. progress: 2.00%
[training]  masked spectral distance: 0.9858757257461548, approx. progress: 2.04%
Epoch 01782: val_loss improved from 0.97179 to 0.97176
Epoch 01783: val_loss improved from 0.97176 to 0.97171
Epoch 01784: val_loss improved from 0.97171 to 0.97165
Epoch 01785: val_loss improved from 0.97165 to 0.97161
Epoch 01786: val_loss improved from 0.97161 to 0.97157
Epoch 01787: val_loss improved from 0.97157 to 0.97153
Epoch 01788: val_loss improved from 0.97153 to 0.97149
Epoch 01789: val_loss improved from 0.97149 to 0.97146
Epoch 01790: val_loss improved from 0.97146 to 0.97145
Epoch 01791: val_loss improved from 0.97145 to 0.97143
Epoch 01796: val_loss improved from 0.97143 to 0.97142
Epoch 01797: val_loss improved from 0.97142 to 0.97140
Epoch 01798: val_loss improved from 0.97140 to 0.97138
Epoch 01799: val_loss improved from 0.97138 to 0.97136
Epoch 01800: val_loss improved from 0.97136 to 0.97135
Epoch 01801: val_loss improved from 0.97135 to 0.97133
Epoch 01802: val_loss improved from 0.97133 to 0.97130
Epoch 01803: val_loss improved from 0.97130 to 0.97125
Epoch 01804: val_loss improved from 0.97125 to 0.97123
Epoch 01805: val_loss improved from 0.97123 to 0.97119
Epoch 01806: val_loss improved from 0.97119 to 0.97117
Epoch 01807: val_loss improved from 0.97117 to 0.97114
Epoch 01808: val_loss improved from 0.97114 to 0.97110
Epoch 01809: val_loss improved from 0.97110 to 0.97105
Epoch 01810: val_loss improved from 0.97105 to 0.97102
Epoch 01811: val_loss improved from 0.97102 to 0.97098
Epoch 01812: val_loss improved from 0.97098 to 0.97094
Epoch 01813: val_loss improved from 0.97094 to 0.97088
Epoch 01814: val_loss improved from 0.97088 to 0.97084
Epoch 01815: val_loss improved from 0.97084 to 0.97080
Epoch 01816: val_loss improved from 0.97080 to 0.97078
Epoch 01817: val_loss improved from 0.97078 to 0.97073
Epoch 01818: val_loss improved from 0.97073 to 0.97071
Epoch 01819: val_loss improved from 0.97071 to 0.97068
Epoch 01820: val_loss improved from 0.97068 to 0.97067
Epoch 01821: val_loss improved from 0.97067 to 0.97064
Epoch 01822: val_loss improved from 0.97064 to 0.97061
Epoch 01823: val_loss improved from 0.97061 to 0.97059
Epoch 01824: val_loss improved from 0.97059 to 0.97057
Epoch 01825: val_loss improved from 0.97057 to 0.97055
Epoch 01826: val_loss improved from 0.97055 to 0.97053
Epoch 01827: val_loss improved from 0.97053 to 0.97052
Epoch 01828: val_loss improved from 0.97052 to 0.97050
Epoch 01829: val_loss improved from 0.97050 to 0.97049
Epoch 01830: val_loss improved from 0.97049 to 0.97048
Epoch 01831: val_loss improved from 0.97048 to 0.97047
Epoch 01832: val_loss improved from 0.97047 to 0.97046
Epoch 01833: val_loss improved from 0.97046 to 0.97043
Epoch 01834: val_loss improved from 0.97043 to 0.97041
Epoch 01835: val_loss improved from 0.97041 to 0.97041
Epoch 01836: val_loss improved from 0.97041 to 0.97040
Epoch 01837: val_loss improved from 0.97040 to 0.97040
Epoch 01838: val_loss improved from 0.97040 to 0.97039
Epoch 01839: val_loss improved from 0.97039 to 0.97038
Epoch 01840: val_loss improved from 0.97038 to 0.97037
Epoch 01841: val_loss improved from 0.97037 to 0.97036
Epoch 01842: val_loss improved from 0.97036 to 0.97034
Epoch 01843: val_loss improved from 0.97034 to 0.97034
Epoch 01870: val_loss improved from 0.97034 to 0.97034
Epoch 01871: val_loss improved from 0.97034 to 0.97028
Epoch 01872: val_loss improved from 0.97028 to 0.97025
Epoch 01873: val_loss improved from 0.97025 to 0.97020
Epoch 01874: val_loss improved from 0.97020 to 0.97015
Epoch 01875: val_loss improved from 0.97015 to 0.97010
Epoch 01876: val_loss improved from 0.97010 to 0.97004
Epoch 01877: val_loss improved from 0.97004 to 0.96999
Epoch 01878: val_loss improved from 0.96999 to 0.96993
Epoch 01879: val_loss improved from 0.96993 to 0.96990
Epoch 01880: val_loss improved from 0.96990 to 0.96988
Epoch 01881: val_loss improved from 0.96988 to 0.96985
Epoch 01882: val_loss improved from 0.96985 to 0.96984
Epoch 01883: val_loss improved from 0.96984 to 0.96981
Epoch 01884: val_loss improved from 0.96981 to 0.96977
Epoch 01885: val_loss improved from 0.96977 to 0.96974
Epoch 01886: val_loss improved from 0.96974 to 0.96972
Epoch 01887: val_loss improved from 0.96972 to 0.96969
Epoch 01888: val_loss improved from 0.96969 to 0.96968
Epoch 01889: val_loss improved from 0.96968 to 0.96967
Epoch 01890: val_loss improved from 0.96967 to 0.96967
Epoch 01891: val_loss improved from 0.96967 to 0.96965
Epoch 01892: val_loss improved from 0.96965 to 0.96964
Epoch 01896: val_loss improved from 0.96964 to 0.96964
Epoch 01898: val_loss improved from 0.96964 to 0.96961
Epoch 01899: val_loss improved from 0.96961 to 0.96955
Epoch 01900: val_loss improved from 0.96955 to 0.96952
Epoch 01901: val_loss improved from 0.96952 to 0.96949
Epoch 01902: val_loss improved from 0.96949 to 0.96947
Epoch 01903: val_loss improved from 0.96947 to 0.96945
Epoch 01904: val_loss improved from 0.96945 to 0.96942
Epoch 01905: val_loss improved from 0.96942 to 0.96937
Epoch 01906: val_loss improved from 0.96937 to 0.96935
Epoch 01907: val_loss improved from 0.96935 to 0.96931
Epoch 01908: val_loss improved from 0.96931 to 0.96927
Epoch 01909: val_loss improved from 0.96927 to 0.96922
Epoch 01910: val_loss improved from 0.96922 to 0.96919
Epoch 01911: val_loss improved from 0.96919 to 0.96916
Epoch 01912: val_loss improved from 0.96916 to 0.96911
Epoch 01913: val_loss improved from 0.96911 to 0.96909
Epoch 01914: val_loss improved from 0.96909 to 0.96907
Epoch 01915: val_loss improved from 0.96907 to 0.96906
Epoch 01916: val_loss improved from 0.96906 to 0.96904
Epoch 01917: val_loss improved from 0.96904 to 0.96903
Epoch 01918: val_loss improved from 0.96903 to 0.96902
Epoch 01919: val_loss improved from 0.96902 to 0.96902
Epoch 01920: val_loss improved from 0.96902 to 0.96901
Epoch 01921: val_loss improved from 0.96901 to 0.96901
Epoch 01927: val_loss improved from 0.96901 to 0.96900
Epoch 01928: val_loss improved from 0.96900 to 0.96897
Epoch 01929: val_loss improved from 0.96897 to 0.96894
Epoch 01930: val_loss improved from 0.96894 to 0.96891
Epoch 01931: val_loss improved from 0.96891 to 0.96887
Epoch 01932: val_loss improved from 0.96887 to 0.96883
Epoch 01933: val_loss improved from 0.96883 to 0.96879
Epoch 01934: val_loss improved from 0.96879 to 0.96876
Epoch 01935: val_loss improved from 0.96876 to 0.96874
Epoch 01936: val_loss improved from 0.96874 to 0.96871
Epoch 01937: val_loss improved from 0.96871 to 0.96867
Epoch 01938: val_loss improved from 0.96867 to 0.96864
Epoch 01939: val_loss improved from 0.96864 to 0.96860
Epoch 01940: val_loss improved from 0.96860 to 0.96856
Epoch 01941: val_loss improved from 0.96856 to 0.96854
Epoch 01942: val_loss improved from 0.96854 to 0.96851
Epoch 01943: val_loss improved from 0.96851 to 0.96849
Epoch 01944: val_loss improved from 0.96849 to 0.96847
Epoch 01945: val_loss improved from 0.96847 to 0.96846
Epoch 01946: val_loss improved from 0.96846 to 0.96844
Epoch 01947: val_loss improved from 0.96844 to 0.96843
Epoch 01948: val_loss improved from 0.96843 to 0.96841
Epoch 01949: val_loss improved from 0.96841 to 0.96839
Epoch 01950: val_loss improved from 0.96839 to 0.96838
Epoch 01951: val_loss improved from 0.96838 to 0.96837
Epoch 01952: val_loss improved from 0.96837 to 0.96836
Epoch 01953: val_loss improved from 0.96836 to 0.96834
Epoch 01954: val_loss improved from 0.96834 to 0.96833
Epoch 01955: val_loss improved from 0.96833 to 0.96832
Epoch 01956: val_loss improved from 0.96832 to 0.96830
Epoch 01957: val_loss improved from 0.96830 to 0.96829
Epoch 01958: val_loss improved from 0.96829 to 0.96828
Epoch 01959: val_loss improved from 0.96828 to 0.96828
Epoch 01960: val_loss improved from 0.96828 to 0.96828
Epoch 01962: val_loss improved from 0.96828 to 0.96827
Epoch 01963: val_loss improved from 0.96827 to 0.96827
Epoch 01964: val_loss improved from 0.96827 to 0.96827
Epoch 01965: val_loss improved from 0.96827 to 0.96827
Epoch 01966: val_loss improved from 0.96827 to 0.96826
Epoch 01967: val_loss improved from 0.96826 to 0.96826
Epoch 01968: val_loss improved from 0.96826 to 0.96826
Epoch 01969: val_loss improved from 0.96826 to 0.96825
Epoch 01970: val_loss improved from 0.96825 to 0.96823
Epoch 01971: val_loss improved from 0.96823 to 0.96821
Epoch 01972: val_loss improved from 0.96821 to 0.96819
Epoch 01973: val_loss improved from 0.96819 to 0.96816
Epoch 01974: val_loss improved from 0.96816 to 0.96814
Epoch 01975: val_loss improved from 0.96814 to 0.96813
Epoch 01978: val_loss improved from 0.96813 to 0.96813
Epoch 01979: val_loss improved from 0.96813 to 0.96811
Epoch 01980: val_loss improved from 0.96811 to 0.96807
Epoch 01981: val_loss improved from 0.96807 to 0.96802
Epoch 01982: val_loss improved from 0.96802 to 0.96798
Epoch 01983: val_loss improved from 0.96798 to 0.96795
Epoch 01984: val_loss improved from 0.96795 to 0.96792
Epoch 01985: val_loss improved from 0.96792 to 0.96789
Epoch 01986: val_loss improved from 0.96789 to 0.96786
Epoch 01988: val_loss improved from 0.96786 to 0.96784
Epoch 01989: val_loss improved from 0.96784 to 0.96784
Epoch 01990: val_loss improved from 0.96784 to 0.96782
Epoch 01991: val_loss improved from 0.96782 to 0.96781
Epoch 01992: val_loss improved from 0.96781 to 0.96780
Epoch 01993: val_loss improved from 0.96780 to 0.96778
Epoch 01994: val_loss improved from 0.96778 to 0.96776
Epoch 01995: val_loss improved from 0.96776 to 0.96775
Epoch 01996: val_loss improved from 0.96775 to 0.96775
Epoch 01998: val_loss improved from 0.96775 to 0.96773
Epoch 01999: val_loss improved from 0.96773 to 0.96771
Epoch 02000: val_loss improved from 0.96771 to 0.96769
Epoch 02001: val_loss improved from 0.96769 to 0.96768
Epoch 02002: val_loss improved from 0.96768 to 0.96765
Epoch 02003: val_loss improved from 0.96765 to 0.96764
Epoch 02004: val_loss improved from 0.96764 to 0.96763
Epoch 02005: val_loss improved from 0.96763 to 0.96760
Epoch 02006: val_loss improved from 0.96760 to 0.96759
Epoch 02007: val_loss improved from 0.96759 to 0.96757
Epoch 02008: val_loss improved from 0.96757 to 0.96755
Epoch 02009: val_loss improved from 0.96755 to 0.96754
Epoch 02010: val_loss improved from 0.96754 to 0.96753
Epoch 02011: val_loss improved from 0.96753 to 0.96750
Epoch 02012: val_loss improved from 0.96750 to 0.96749
Epoch 02013: val_loss improved from 0.96749 to 0.96747
Epoch 02014: val_loss improved from 0.96747 to 0.96746
Epoch 02015: val_loss improved from 0.96746 to 0.96743
Epoch 02016: val_loss improved from 0.96743 to 0.96741
Epoch 02017: val_loss improved from 0.96741 to 0.96739
Epoch 02018: val_loss improved from 0.96739 to 0.96737
Epoch 02019: val_loss improved from 0.96737 to 0.96737
Epoch 02024: val_loss improved from 0.96737 to 0.96736
Epoch 02025: val_loss improved from 0.96736 to 0.96735
Epoch 02026: val_loss improved from 0.96735 to 0.96735
Epoch 02027: val_loss improved from 0.96735 to 0.96733
Epoch 02028: val_loss improved from 0.96733 to 0.96732
Epoch 02030: val_loss improved from 0.96732 to 0.96732
Epoch 02033: val_loss improved from 0.96732 to 0.96731
Epoch 02034: val_loss improved from 0.96731 to 0.96730
Epoch 02035: val_loss improved from 0.96730 to 0.96727
Epoch 02036: val_loss improved from 0.96727 to 0.96725
Epoch 02037: val_loss improved from 0.96725 to 0.96724
Epoch 02051: val_loss improved from 0.96724 to 0.96720
Epoch 02052: val_loss improved from 0.96720 to 0.96716
Epoch 02053: val_loss improved from 0.96716 to 0.96710
Epoch 02054: val_loss improved from 0.96710 to 0.96705
Epoch 02055: val_loss improved from 0.96705 to 0.96700
Epoch 02056: val_loss improved from 0.96700 to 0.96696
Epoch 02057: val_loss improved from 0.96696 to 0.96692
Epoch 02058: val_loss improved from 0.96692 to 0.96689
Epoch 02059: val_loss improved from 0.96689 to 0.96686
Epoch 02060: val_loss improved from 0.96686 to 0.96683
Epoch 02061: val_loss improved from 0.96683 to 0.96680
Epoch 02062: val_loss improved from 0.96680 to 0.96677
Epoch 02063: val_loss improved from 0.96677 to 0.96677
Epoch 02064: val_loss improved from 0.96677 to 0.96676
Epoch 02069: val_loss improved from 0.96676 to 0.96673
Epoch 02070: val_loss improved from 0.96673 to 0.96669
Epoch 02071: val_loss improved from 0.96669 to 0.96666
Epoch 02072: val_loss improved from 0.96666 to 0.96665
Epoch 02086: val_loss improved from 0.96665 to 0.96662
Epoch 02087: val_loss improved from 0.96662 to 0.96660
Epoch 02088: val_loss improved from 0.96660 to 0.96659
Epoch 02098: val_loss improved from 0.96659 to 0.96658
Epoch 02099: val_loss improved from 0.96658 to 0.96655
Epoch 02100: val_loss improved from 0.96655 to 0.96651
Epoch 02101: val_loss improved from 0.96651 to 0.96647
Epoch 02102: val_loss improved from 0.96647 to 0.96647
Epoch 02105: val_loss improved from 0.96647 to 0.96644
Epoch 02106: val_loss improved from 0.96644 to 0.96639
Epoch 02107: val_loss improved from 0.96639 to 0.96631
Epoch 02108: val_loss improved from 0.96631 to 0.96623
Epoch 02109: val_loss improved from 0.96623 to 0.96615
Epoch 02110: val_loss improved from 0.96615 to 0.96609
Epoch 02111: val_loss improved from 0.96609 to 0.96604
Epoch 02112: val_loss improved from 0.96604 to 0.96600
Epoch 02113: val_loss improved from 0.96600 to 0.96596
Epoch 02114: val_loss improved from 0.96596 to 0.96593
Epoch 02115: val_loss improved from 0.96593 to 0.96589
Epoch 02116: val_loss improved from 0.96589 to 0.96585
Epoch 02117: val_loss improved from 0.96585 to 0.96581
Epoch 02118: val_loss improved from 0.96581 to 0.96577
Epoch 02119: val_loss improved from 0.96577 to 0.96576
Epoch 02120: val_loss improved from 0.96576 to 0.96576
Epoch 02124: val_loss improved from 0.96576 to 0.96573
Epoch 02125: val_loss improved from 0.96573 to 0.96567
Epoch 02126: val_loss improved from 0.96567 to 0.96564
Epoch 02127: val_loss improved from 0.96564 to 0.96564
[training]  masked spectral distance: 0.9782127141952515, approx. progress: 2.89%
Epoch 02233: val_loss improved from 0.96564 to 0.96558
Epoch 02234: val_loss improved from 0.96558 to 0.96550
Epoch 02235: val_loss improved from 0.96550 to 0.96543
Epoch 02236: val_loss improved from 0.96543 to 0.96537
Epoch 02237: val_loss improved from 0.96537 to 0.96533
Epoch 02238: val_loss improved from 0.96533 to 0.96529
Epoch 02239: val_loss improved from 0.96529 to 0.96528
Epoch 02240: val_loss improved from 0.96528 to 0.96527
Epoch 02369: val_loss improved from 0.96527 to 0.96517
Epoch 02370: val_loss improved from 0.96517 to 0.96511
Epoch 02371: val_loss improved from 0.96511 to 0.96499
Epoch 02372: val_loss improved from 0.96499 to 0.96489
Epoch 02373: val_loss improved from 0.96489 to 0.96473
Epoch 02374: val_loss improved from 0.96473 to 0.96458
Epoch 02375: val_loss improved from 0.96458 to 0.96456
Epoch 02376: val_loss improved from 0.96456 to 0.96448
Epoch 02377: val_loss improved from 0.96448 to 0.96438
Epoch 02378: val_loss improved from 0.96438 to 0.96437
Epoch 02425: val_loss improved from 0.96437 to 0.96436
Epoch 02426: val_loss improved from 0.96436 to 0.96428
Epoch 02427: val_loss improved from 0.96428 to 0.96424
Epoch 02428: val_loss improved from 0.96424 to 0.96412
Epoch 02429: val_loss improved from 0.96412 to 0.96397
Epoch 02430: val_loss improved from 0.96397 to 0.96386
Epoch 02431: val_loss improved from 0.96386 to 0.96371
Epoch 02432: val_loss improved from 0.96371 to 0.96361
Epoch 02433: val_loss improved from 0.96361 to 0.96360
Epoch 02458: val_loss improved from 0.96360 to 0.96340
Epoch 02459: val_loss improved from 0.96340 to 0.96323
Epoch 02460: val_loss improved from 0.96323 to 0.96312
Epoch 02530: val_loss improved from 0.96312 to 0.96304
Epoch 02531: val_loss improved from 0.96304 to 0.96292
Epoch 02532: val_loss improved from 0.96292 to 0.96283
Epoch 02533: val_loss improved from 0.96283 to 0.96271
Epoch 02534: val_loss improved from 0.96271 to 0.96264
Epoch 02535: val_loss improved from 0.96264 to 0.96259
Epoch 02536: val_loss improved from 0.96259 to 0.96254
Epoch 02537: val_loss improved from 0.96254 to 0.96245
Epoch 02538: val_loss improved from 0.96245 to 0.96241
Epoch 02539: val_loss improved from 0.96241 to 0.96236
Epoch 02540: val_loss improved from 0.96236 to 0.96234
Epoch 02541: val_loss improved from 0.96234 to 0.96233
Epoch 02546: val_loss improved from 0.96233 to 0.96228
Epoch 02547: val_loss improved from 0.96228 to 0.96212
Epoch 02548: val_loss improved from 0.96212 to 0.96192
Epoch 02549: val_loss improved from 0.96192 to 0.96180
Epoch 02550: val_loss improved from 0.96180 to 0.96170
Epoch 02551: val_loss improved from 0.96170 to 0.96165
Epoch 02628: val_loss improved from 0.96165 to 0.96118
Epoch 02629: val_loss improved from 0.96118 to 0.96073
Epoch 02630: val_loss improved from 0.96073 to 0.96068
Epoch 02654: val_loss improved from 0.96068 to 0.95972
Epoch 02655: val_loss improved from 0.95972 to 0.95921
Epoch 02659: val_loss improved from 0.95921 to 0.95888
Epoch 02660: val_loss improved from 0.95888 to 0.95860
Epoch 02661: val_loss improved from 0.95860 to 0.95816
Epoch 02662: val_loss improved from 0.95816 to 0.95791
Epoch 02663: val_loss improved from 0.95791 to 0.95776
Epoch 02664: val_loss improved from 0.95776 to 0.95770
Epoch 02665: val_loss improved from 0.95770 to 0.95753
Epoch 02666: val_loss improved from 0.95753 to 0.95734
Epoch 02667: val_loss improved from 0.95734 to 0.95728
Epoch 02669: val_loss improved from 0.95728 to 0.95704
Epoch 02670: val_loss improved from 0.95704 to 0.95681
Epoch 02671: val_loss improved from 0.95681 to 0.95659
Epoch 02672: val_loss improved from 0.95659 to 0.95659
Epoch 02685: val_loss improved from 0.95659 to 0.95607
Epoch 02692: val_loss improved from 0.95607 to 0.95590
Epoch 02693: val_loss improved from 0.95590 to 0.95563
Epoch 02694: val_loss improved from 0.95563 to 0.95522
Epoch 02695: val_loss improved from 0.95522 to 0.95511
Epoch 02696: val_loss improved from 0.95511 to 0.95499
Epoch 02697: val_loss improved from 0.95499 to 0.95478
Epoch 02698: val_loss improved from 0.95478 to 0.95444
Epoch 02699: val_loss improved from 0.95444 to 0.95432
Epoch 02700: val_loss improved from 0.95432 to 0.95414
Epoch 02701: val_loss improved from 0.95414 to 0.95404
Epoch 02702: val_loss improved from 0.95404 to 0.95393
Epoch 02703: val_loss improved from 0.95393 to 0.95388
Epoch 02714: val_loss improved from 0.95388 to 0.95356
Epoch 02715: val_loss improved from 0.95356 to 0.95303
Epoch 02716: val_loss improved from 0.95303 to 0.95297
Epoch 02717: val_loss improved from 0.95297 to 0.95287
Epoch 02718: val_loss improved from 0.95287 to 0.95261
Epoch 02719: val_loss improved from 0.95261 to 0.95229
Epoch 02720: val_loss improved from 0.95229 to 0.95198
Epoch 02721: val_loss improved from 0.95198 to 0.95186
Epoch 02722: val_loss improved from 0.95186 to 0.95169
Epoch 02723: val_loss improved from 0.95169 to 0.95108
Epoch 02724: val_loss improved from 0.95108 to 0.95082
Epoch 02725: val_loss improved from 0.95082 to 0.95081
Epoch 02728: val_loss improved from 0.95081 to 0.95063
Epoch 02729: val_loss improved from 0.95063 to 0.95026
Epoch 02730: val_loss improved from 0.95026 to 0.95001
Epoch 02731: val_loss improved from 0.95001 to 0.94974
Epoch 02732: val_loss improved from 0.94974 to 0.94938
Epoch 02733: val_loss improved from 0.94938 to 0.94907
Epoch 02734: val_loss improved from 0.94907 to 0.94874
Epoch 02735: val_loss improved from 0.94874 to 0.94852
Epoch 02736: val_loss improved from 0.94852 to 0.94842
Epoch 02737: val_loss improved from 0.94842 to 0.94832
Epoch 02738: val_loss improved from 0.94832 to 0.94824
Epoch 02744: val_loss improved from 0.94824 to 0.94801
Epoch 02745: val_loss improved from 0.94801 to 0.94714
Epoch 02746: val_loss improved from 0.94714 to 0.94626
Epoch 02747: val_loss improved from 0.94626 to 0.94541
Epoch 02748: val_loss improved from 0.94541 to 0.94490
Epoch 02749: val_loss improved from 0.94490 to 0.94442
Epoch 02750: val_loss improved from 0.94442 to 0.94364
Epoch 02751: val_loss improved from 0.94364 to 0.94280
Epoch 02752: val_loss improved from 0.94280 to 0.94175
Epoch 02753: val_loss improved from 0.94175 to 0.94130
Epoch 02778: val_loss improved from 0.94130 to 0.94110
Epoch 02779: val_loss improved from 0.94110 to 0.94035
Epoch 02780: val_loss improved from 0.94035 to 0.93898
Epoch 02781: val_loss improved from 0.93898 to 0.93740
Epoch 02782: val_loss improved from 0.93740 to 0.93582
Epoch 02783: val_loss improved from 0.93582 to 0.93467
Epoch 02784: val_loss improved from 0.93467 to 0.93360
Epoch 02785: val_loss improved from 0.93360 to 0.93239
Epoch 02786: val_loss improved from 0.93239 to 0.93143
Epoch 02787: val_loss improved from 0.93143 to 0.93032
Epoch 02788: val_loss improved from 0.93032 to 0.92861
Epoch 02789: val_loss improved from 0.92861 to 0.92722
Epoch 02790: val_loss improved from 0.92722 to 0.92539
Epoch 02791: val_loss improved from 0.92539 to 0.92336
Epoch 02792: val_loss improved from 0.92336 to 0.92199
Epoch 02793: val_loss improved from 0.92199 to 0.92127
Epoch 02794: val_loss improved from 0.92127 to 0.91993
Epoch 02795: val_loss improved from 0.91993 to 0.91887
Epoch 02799: val_loss improved from 0.91887 to 0.91807
Epoch 02800: val_loss improved from 0.91807 to 0.91700
Epoch 02801: val_loss improved from 0.91700 to 0.91660
Epoch 02802: val_loss improved from 0.91660 to 0.91589
Epoch 02803: val_loss improved from 0.91589 to 0.91362
Epoch 02804: val_loss improved from 0.91362 to 0.91190
Epoch 02805: val_loss improved from 0.91190 to 0.91061
Epoch 02806: val_loss improved from 0.91061 to 0.90860
Epoch 02807: val_loss improved from 0.90860 to 0.90468
Epoch 02808: val_loss improved from 0.90468 to 0.90024
Epoch 02809: val_loss improved from 0.90024 to 0.89697
Epoch 02810: val_loss improved from 0.89697 to 0.89471
Epoch 02825: val_loss improved from 0.89471 to 0.89039
Epoch 02826: val_loss improved from 0.89039 to 0.88739
Epoch 02827: val_loss improved from 0.88739 to 0.88512
Epoch 02828: val_loss improved from 0.88512 to 0.88340
Epoch 02829: val_loss improved from 0.88340 to 0.88241
Epoch 02830: val_loss improved from 0.88241 to 0.88229
Epoch 02831: val_loss improved from 0.88229 to 0.88052
Epoch 02832: val_loss improved from 0.88052 to 0.87886
Epoch 02833: val_loss improved from 0.87886 to 0.87107
Epoch 02834: val_loss improved from 0.87107 to 0.86678
Epoch 02835: val_loss improved from 0.86678 to 0.86269
Epoch 02861: val_loss improved from 0.86269 to 0.85994
Epoch 02862: val_loss improved from 0.85994 to 0.85434
Epoch 02863: val_loss improved from 0.85434 to 0.85112
Epoch 02864: val_loss improved from 0.85112 to 0.85067
Epoch 02867: val_loss improved from 0.85067 to 0.84961
Epoch 02868: val_loss improved from 0.84961 to 0.84879
Epoch 02872: val_loss improved from 0.84879 to 0.84837
Epoch 02882: val_loss improved from 0.84837 to 0.84692
Epoch 02883: val_loss improved from 0.84692 to 0.84142
Epoch 02884: val_loss improved from 0.84142 to 0.83656
Epoch 02885: val_loss improved from 0.83656 to 0.83332
Epoch 02886: val_loss improved from 0.83332 to 0.82965
Epoch 02887: val_loss improved from 0.82965 to 0.82527
Epoch 02888: val_loss improved from 0.82527 to 0.82336
Epoch 02898: val_loss improved from 0.82336 to 0.81945
Epoch 02899: val_loss improved from 0.81945 to 0.81834
Epoch 02900: val_loss improved from 0.81834 to 0.81737
Epoch 02901: val_loss improved from 0.81737 to 0.81714
Epoch 02902: val_loss improved from 0.81714 to 0.81273
Epoch 02904: val_loss improved from 0.81273 to 0.81081
Epoch 02905: val_loss improved from 0.81081 to 0.80340
Epoch 02906: val_loss improved from 0.80340 to 0.79776
Epoch 02907: val_loss improved from 0.79776 to 0.79530
Epoch 02909: val_loss improved from 0.79530 to 0.79338
Epoch 02910: val_loss improved from 0.79338 to 0.78739
Epoch 02911: val_loss improved from 0.78739 to 0.78368
Epoch 02912: val_loss improved from 0.78368 to 0.78256
Epoch 02913: val_loss improved from 0.78256 to 0.78084
Epoch 02914: val_loss improved from 0.78084 to 0.77491
Epoch 02915: val_loss improved from 0.77491 to 0.76856
Epoch 02916: val_loss improved from 0.76856 to 0.76621
Epoch 02917: val_loss improved from 0.76621 to 0.76377
Epoch 02935: val_loss improved from 0.76377 to 0.76316
Epoch 02936: val_loss improved from 0.76316 to 0.75983
Epoch 02937: val_loss improved from 0.75983 to 0.75505
Epoch 02938: val_loss improved from 0.75505 to 0.74866
Epoch 02939: val_loss improved from 0.74866 to 0.74207
Epoch 02940: val_loss improved from 0.74207 to 0.73819
Epoch 02943: val_loss improved from 0.73819 to 0.73761
Epoch 02944: val_loss improved from 0.73761 to 0.73090
Epoch 02945: val_loss improved from 0.73090 to 0.72747
Epoch 02946: val_loss improved from 0.72747 to 0.72641
Epoch 02947: val_loss improved from 0.72641 to 0.72112
Epoch 02948: val_loss improved from 0.72112 to 0.71673
Epoch 02949: val_loss improved from 0.71673 to 0.71478
Epoch 02950: val_loss improved from 0.71478 to 0.71335
Epoch 02956: val_loss improved from 0.71335 to 0.70623
Epoch 02957: val_loss improved from 0.70623 to 0.70096
Epoch 02958: val_loss improved from 0.70096 to 0.69589
Epoch 02959: val_loss improved from 0.69589 to 0.68961
Epoch 02960: val_loss improved from 0.68961 to 0.68944
Epoch 02961: val_loss improved from 0.68944 to 0.68858
Epoch 02965: val_loss improved from 0.68858 to 0.68696
Epoch 02966: val_loss improved from 0.68696 to 0.67952
Epoch 02967: val_loss improved from 0.67952 to 0.67555
Epoch 02968: val_loss improved from 0.67555 to 0.67253
Epoch 02969: val_loss improved from 0.67253 to 0.67118
Epoch 02970: val_loss improved from 0.67118 to 0.67117
Epoch 03002: val_loss improved from 0.67117 to 0.67073
Epoch 03003: val_loss improved from 0.67073 to 0.66895
Epoch 03004: val_loss improved from 0.66895 to 0.66683
Epoch 03005: val_loss improved from 0.66683 to 0.66640
Epoch 03006: val_loss improved from 0.66640 to 0.66610
Epoch 03007: val_loss improved from 0.66610 to 0.66550
Epoch 03011: val_loss improved from 0.66550 to 0.66534
Epoch 03015: val_loss improved from 0.66534 to 0.66467
Epoch 03016: val_loss improved from 0.66467 to 0.66414
Epoch 03017: val_loss improved from 0.66414 to 0.66387
Epoch 03018: val_loss improved from 0.66387 to 0.66382
Epoch 03019: val_loss improved from 0.66382 to 0.66363
Epoch 03020: val_loss improved from 0.66363 to 0.66354
Epoch 03021: val_loss improved from 0.66354 to 0.66287
Epoch 03022: val_loss improved from 0.66287 to 0.66244
Epoch 03023: val_loss improved from 0.66244 to 0.66200
Epoch 03024: val_loss improved from 0.66200 to 0.66181
Epoch 03025: val_loss improved from 0.66181 to 0.66171
Epoch 03032: val_loss improved from 0.66171 to 0.66123
Epoch 03033: val_loss improved from 0.66123 to 0.66030
Epoch 03034: val_loss improved from 0.66030 to 0.65946
Epoch 03049: val_loss improved from 0.65946 to 0.65864
Epoch 03050: val_loss improved from 0.65864 to 0.65750
Epoch 03051: val_loss improved from 0.65750 to 0.65723
Epoch 03052: val_loss improved from 0.65723 to 0.65660
Epoch 03054: val_loss improved from 0.65660 to 0.65650
Epoch 03055: val_loss improved from 0.65650 to 0.65631
Epoch 03056: val_loss improved from 0.65631 to 0.65575
Epoch 03057: val_loss improved from 0.65575 to 0.65511
Epoch 03058: val_loss improved from 0.65511 to 0.65384
Epoch 03059: val_loss improved from 0.65384 to 0.65366
Epoch 03085: val_loss improved from 0.65366 to 0.65322
Epoch 03086: val_loss improved from 0.65322 to 0.65261
Epoch 03087: val_loss improved from 0.65261 to 0.65167
Epoch 03088: val_loss improved from 0.65167 to 0.65084
Epoch 03089: val_loss improved from 0.65084 to 0.64942
Epoch 03090: val_loss improved from 0.64942 to 0.64847
Epoch 03091: val_loss improved from 0.64847 to 0.64761
Epoch 03092: val_loss improved from 0.64761 to 0.64676
Epoch 03093: val_loss improved from 0.64676 to 0.64623
Epoch 03094: val_loss improved from 0.64623 to 0.64581
Epoch 03095: val_loss improved from 0.64581 to 0.64579
Epoch 03096: val_loss improved from 0.64579 to 0.64553
Epoch 03097: val_loss improved from 0.64553 to 0.64539
Epoch 03098: val_loss improved from 0.64539 to 0.64514
Epoch 03100: val_loss improved from 0.64514 to 0.64501
Epoch 03101: val_loss improved from 0.64501 to 0.64480
Epoch 03102: val_loss improved from 0.64480 to 0.64464
Epoch 03104: val_loss improved from 0.64464 to 0.64440
Epoch 03108: val_loss improved from 0.64440 to 0.64417
Epoch 03109: val_loss improved from 0.64417 to 0.64397
Epoch 03110: val_loss improved from 0.64397 to 0.64389
Epoch 03111: val_loss improved from 0.64389 to 0.64383
Epoch 03131: val_loss improved from 0.64383 to 0.64375
Epoch 03141: val_loss improved from 0.64375 to 0.64337
Epoch 03142: val_loss improved from 0.64337 to 0.64281
Epoch 03143: val_loss improved from 0.64281 to 0.64180
[training]  masked spectral distance: 0.5793315768241882, approx. progress: 47.00%
Epoch 03144: val_loss improved from 0.64180 to 0.64033
Epoch 03145: val_loss improved from 0.64033 to 0.63881
Epoch 03146: val_loss improved from 0.63881 to 0.63791
Epoch 03147: val_loss improved from 0.63791 to 0.63687
Epoch 03148: val_loss improved from 0.63687 to 0.63542
Epoch 03149: val_loss improved from 0.63542 to 0.63443
Epoch 03150: val_loss improved from 0.63443 to 0.63266
Epoch 03151: val_loss improved from 0.63266 to 0.63172
Epoch 03152: val_loss improved from 0.63172 to 0.63099
Epoch 03153: val_loss improved from 0.63099 to 0.62985
Epoch 03154: val_loss improved from 0.62985 to 0.62838
Epoch 03155: val_loss improved from 0.62838 to 0.62687
Epoch 03156: val_loss improved from 0.62687 to 0.62582
Epoch 03157: val_loss improved from 0.62582 to 0.62411
Epoch 03158: val_loss improved from 0.62411 to 0.62254
Epoch 03159: val_loss improved from 0.62254 to 0.62150
Epoch 03160: val_loss improved from 0.62150 to 0.61981
Epoch 03161: val_loss improved from 0.61981 to 0.61968
Epoch 03165: val_loss improved from 0.61968 to 0.61950
Epoch 03170: val_loss improved from 0.61950 to 0.61887
Epoch 03171: val_loss improved from 0.61887 to 0.61833
Epoch 03182: val_loss improved from 0.61833 to 0.61642
Epoch 03183: val_loss improved from 0.61642 to 0.61258
Epoch 03184: val_loss improved from 0.61258 to 0.60996
Epoch 03185: val_loss improved from 0.60996 to 0.60928
Epoch 03186: val_loss improved from 0.60928 to 0.60779
Epoch 03188: val_loss improved from 0.60779 to 0.60750
Epoch 03189: val_loss improved from 0.60750 to 0.60614
Epoch 03190: val_loss improved from 0.60614 to 0.60604
Epoch 03191: val_loss improved from 0.60604 to 0.60590
Epoch 03192: val_loss improved from 0.60590 to 0.60542
Epoch 03193: val_loss improved from 0.60542 to 0.60513
Epoch 03200: val_loss improved from 0.60513 to 0.60512
Epoch 03201: val_loss improved from 0.60512 to 0.60453
Epoch 03202: val_loss improved from 0.60453 to 0.60300
Epoch 03203: val_loss improved from 0.60300 to 0.60105
Epoch 03204: val_loss improved from 0.60105 to 0.59928
Epoch 03205: val_loss improved from 0.59928 to 0.59626
Epoch 03206: val_loss improved from 0.59626 to 0.59281
Epoch 03207: val_loss improved from 0.59281 to 0.59057
Epoch 03208: val_loss improved from 0.59057 to 0.58955
Epoch 03209: val_loss improved from 0.58955 to 0.58893
Epoch 03210: val_loss improved from 0.58893 to 0.58805
Epoch 03211: val_loss improved from 0.58805 to 0.58684
Epoch 03212: val_loss improved from 0.58684 to 0.58570
Epoch 03217: val_loss improved from 0.58570 to 0.58477
Epoch 03218: val_loss improved from 0.58477 to 0.58451
Epoch 03219: val_loss improved from 0.58451 to 0.58421
Epoch 03220: val_loss improved from 0.58421 to 0.58383
Epoch 03221: val_loss improved from 0.58383 to 0.58246
Epoch 03222: val_loss improved from 0.58246 to 0.58144
Epoch 03223: val_loss improved from 0.58144 to 0.57991
Epoch 03224: val_loss improved from 0.57991 to 0.57828
Epoch 03225: val_loss improved from 0.57828 to 0.57711
Epoch 03226: val_loss improved from 0.57711 to 0.57597
Epoch 03227: val_loss improved from 0.57597 to 0.57486
Epoch 03228: val_loss improved from 0.57486 to 0.57392
Epoch 03229: val_loss improved from 0.57392 to 0.57309
Epoch 03230: val_loss improved from 0.57309 to 0.57159
Epoch 03231: val_loss improved from 0.57159 to 0.56944
Epoch 03232: val_loss improved from 0.56944 to 0.56722
Epoch 03233: val_loss improved from 0.56722 to 0.56550
Epoch 03234: val_loss improved from 0.56550 to 0.56395
Epoch 03235: val_loss improved from 0.56395 to 0.56329
Epoch 03236: val_loss improved from 0.56329 to 0.56206
Epoch 03237: val_loss improved from 0.56206 to 0.56174
Epoch 03238: val_loss improved from 0.56174 to 0.56149
Epoch 03239: val_loss improved from 0.56149 to 0.56093
Epoch 03240: val_loss improved from 0.56093 to 0.56019
Epoch 03241: val_loss improved from 0.56019 to 0.55843
Epoch 03242: val_loss improved from 0.55843 to 0.55698
Epoch 03243: val_loss improved from 0.55698 to 0.55545
Epoch 03244: val_loss improved from 0.55545 to 0.55512
Epoch 03245: val_loss improved from 0.55512 to 0.55390
Epoch 03246: val_loss improved from 0.55390 to 0.55248
Epoch 03247: val_loss improved from 0.55248 to 0.55157
Epoch 03248: val_loss improved from 0.55157 to 0.55033
Epoch 03249: val_loss improved from 0.55033 to 0.54970
Epoch 03250: val_loss improved from 0.54970 to 0.54824
Epoch 03251: val_loss improved from 0.54824 to 0.54760
Epoch 03253: val_loss improved from 0.54760 to 0.54691
Epoch 03254: val_loss improved from 0.54691 to 0.54620
Epoch 03255: val_loss improved from 0.54620 to 0.54513
Epoch 03256: val_loss improved from 0.54513 to 0.54317
Epoch 03257: val_loss improved from 0.54317 to 0.54081
Epoch 03258: val_loss improved from 0.54081 to 0.54077
Epoch 03266: val_loss improved from 0.54077 to 0.54026
Epoch 03267: val_loss improved from 0.54026 to 0.53972
Epoch 03268: val_loss improved from 0.53972 to 0.53895
Epoch 03269: val_loss improved from 0.53895 to 0.53738
Epoch 03270: val_loss improved from 0.53738 to 0.53589
Epoch 03271: val_loss improved from 0.53589 to 0.53425
Epoch 03272: val_loss improved from 0.53425 to 0.53271
Epoch 03273: val_loss improved from 0.53271 to 0.53080
Epoch 03275: val_loss improved from 0.53080 to 0.53057
Epoch 03276: val_loss improved from 0.53057 to 0.52944
Epoch 03277: val_loss improved from 0.52944 to 0.52826
Epoch 03278: val_loss improved from 0.52826 to 0.52787
Epoch 03279: val_loss improved from 0.52787 to 0.52768
Epoch 03280: val_loss improved from 0.52768 to 0.52646
Epoch 03281: val_loss improved from 0.52646 to 0.52557
Epoch 03282: val_loss improved from 0.52557 to 0.52514
Epoch 03284: val_loss improved from 0.52514 to 0.52502
Epoch 03287: val_loss improved from 0.52502 to 0.52488
Epoch 03288: val_loss improved from 0.52488 to 0.52398
Epoch 03289: val_loss improved from 0.52398 to 0.52309
Epoch 03290: val_loss improved from 0.52309 to 0.52229
Epoch 03291: val_loss improved from 0.52229 to 0.52127
Epoch 03292: val_loss improved from 0.52127 to 0.52062
Epoch 03293: val_loss improved from 0.52062 to 0.51997
Epoch 03294: val_loss improved from 0.51997 to 0.51933
Epoch 03295: val_loss improved from 0.51933 to 0.51896
Epoch 03300: val_loss improved from 0.51896 to 0.51861
Epoch 03301: val_loss improved from 0.51861 to 0.51777
Epoch 03302: val_loss improved from 0.51777 to 0.51710
Epoch 03303: val_loss improved from 0.51710 to 0.51677
Epoch 03310: val_loss improved from 0.51677 to 0.51618
Epoch 03311: val_loss improved from 0.51618 to 0.51548
Epoch 03312: val_loss improved from 0.51548 to 0.51498
Epoch 03313: val_loss improved from 0.51498 to 0.51437
Epoch 03314: val_loss improved from 0.51437 to 0.51401
Epoch 03317: val_loss improved from 0.51401 to 0.51307
Epoch 03318: val_loss improved from 0.51307 to 0.51233
Epoch 03319: val_loss improved from 0.51233 to 0.51136
Epoch 03334: val_loss improved from 0.51136 to 0.51113
Epoch 03335: val_loss improved from 0.51113 to 0.51068
Epoch 03336: val_loss improved from 0.51068 to 0.51047
Epoch 03338: val_loss improved from 0.51047 to 0.51041
Epoch 03344: val_loss improved from 0.51041 to 0.51039
Epoch 03345: val_loss improved from 0.51039 to 0.50967
Epoch 03346: val_loss improved from 0.50967 to 0.50950
Epoch 03347: val_loss improved from 0.50950 to 0.50913
Epoch 03353: val_loss improved from 0.50913 to 0.50900
Epoch 03354: val_loss improved from 0.50900 to 0.50848
Epoch 03355: val_loss improved from 0.50848 to 0.50766
Epoch 03356: val_loss improved from 0.50766 to 0.50725
Epoch 03357: val_loss improved from 0.50725 to 0.50657
Epoch 03358: val_loss improved from 0.50657 to 0.50606
Epoch 03360: val_loss improved from 0.50606 to 0.50589
Epoch 03361: val_loss improved from 0.50589 to 0.50585
Epoch 03363: val_loss improved from 0.50585 to 0.50558
Epoch 03364: val_loss improved from 0.50558 to 0.50540
Epoch 03365: val_loss improved from 0.50540 to 0.50534
Epoch 03366: val_loss improved from 0.50534 to 0.50464
Epoch 03368: val_loss improved from 0.50464 to 0.50457
Epoch 03369: val_loss improved from 0.50457 to 0.50437
Epoch 03386: val_loss improved from 0.50437 to 0.50428
Epoch 03387: val_loss improved from 0.50428 to 0.50347
Epoch 03388: val_loss improved from 0.50347 to 0.50335
Epoch 03419: val_loss improved from 0.50335 to 0.50326
Epoch 03420: val_loss improved from 0.50326 to 0.50262
Epoch 03421: val_loss improved from 0.50262 to 0.50018
Epoch 03495: val_loss improved from 0.50018 to 0.49885
Epoch 03496: val_loss improved from 0.49885 to 0.49809
Epoch 03553: val_loss improved from 0.49809 to 0.49205
Epoch 03597: val_loss improved from 0.49205 to 0.49112
Epoch 03598: val_loss improved from 0.49112 to 0.49046
Epoch 03703: val_loss improved from 0.49046 to 0.48938
Epoch 03704: val_loss improved from 0.48938 to 0.48894
Epoch 03711: val_loss improved from 0.48894 to 0.48407
Epoch 03768: val_loss improved from 0.48407 to 0.48397
Epoch 03769: val_loss improved from 0.48397 to 0.48123
Epoch 03770: val_loss improved from 0.48123 to 0.47895
Epoch 03856: val_loss improved from 0.47895 to 0.47866
Epoch 03857: val_loss improved from 0.47866 to 0.47840
Epoch 03926: val_loss improved from 0.47840 to 0.47737
Epoch 03933: val_loss improved from 0.47737 to 0.47723
Epoch 03934: val_loss improved from 0.47723 to 0.47630
Epoch 03951: val_loss improved from 0.47630 to 0.47362
Epoch 03952: val_loss improved from 0.47362 to 0.46782
Epoch 03953: val_loss improved from 0.46782 to 0.46729
Epoch 03961: val_loss improved from 0.46729 to 0.46597
[training]  masked spectral distance: 0.35587021708488464, approx. progress: 71.71%
Epoch 04199: val_loss improved from 0.46597 to 0.45978
Epoch 04200: val_loss improved from 0.45978 to 0.45671
[training]  masked spectral distance: 0.3184128999710083, approx. progress: 75.85%
Epoch 05262: val_loss improved from 0.45671 to 0.45550
Epoch 05783: val_loss improved from 0.45550 to 0.45541
[training]  masked spectral distance: 0.26822811365127563, approx. progress: 81.40%
[training]  masked spectral distance: 0.2364000529050827, approx. progress: 84.92%
[training]  masked spectral distance: 0.218339204788208, approx. progress: 86.91%
Epoch 08350: val_loss improved from 0.45541 to 0.45463
Epoch 08386: val_loss improved from 0.45463 to 0.45422
Epoch 08387: val_loss improved from 0.45422 to 0.45295
Epoch 08700: val_loss improved from 0.45295 to 0.45286
Epoch 08701: val_loss improved from 0.45286 to 0.45263
Epoch 09014: val_loss improved from 0.45263 to 0.45176
Epoch 09021: val_loss improved from 0.45176 to 0.45176
Epoch 09082: val_loss improved from 0.45176 to 0.45066
Epoch 09087: val_loss improved from 0.45066 to 0.45013
Epoch 09088: val_loss improved from 0.45013 to 0.44572
[training]  masked spectral distance: 0.19754767417907715, approx. progress: 89.21%
[training]  masked spectral distance: 0.18248513340950012, approx. progress: 90.88%
Epoch 10574: val_loss improved from 0.44572 to 0.44496
Epoch 10575: val_loss improved from 0.44496 to 0.44379
Epoch 10576: val_loss improved from 0.44379 to 0.44350
Epoch 10577: val_loss improved from 0.44350 to 0.43993
Epoch 10578: val_loss improved from 0.43993 to 0.43747
Epoch 10580: val_loss improved from 0.43747 to 0.43716
Epoch 10596: val_loss improved from 0.43716 to 0.43567
Epoch 10597: val_loss improved from 0.43567 to 0.43212
Epoch 10598: val_loss improved from 0.43212 to 0.43105
Epoch 10605: val_loss improved from 0.43105 to 0.43017
Epoch 10606: val_loss improved from 0.43017 to 0.42944
Epoch 10607: val_loss improved from 0.42944 to 0.42942
Epoch 10609: val_loss improved from 0.42942 to 0.42937
[training]  masked spectral distance: 0.12589477002620697, approx. progress: 97.14%
Epoch 11174: val_loss improved from 0.42937 to 0.42891
Epoch 11175: val_loss improved from 0.42891 to 0.42872
Epoch 11176: val_loss improved from 0.42872 to 0.42799
[training]  masked spectral distance: 0.10193461924791336, approx. progress: 99.79%
Epoch 12541: val_loss improved from 0.42799 to 0.42682
Epoch 12547: val_loss improved from 0.42682 to 0.42611
Epoch 12548: val_loss improved from 0.42611 to 0.42563
Epoch 12549: val_loss improved from 0.42563 to 0.42491
[training]  masked spectral distance: 0.08543725311756134, approx. progress: 99.99%
validation loss: 0.44414788484573364, pearson distance: 0.3526816964149475
[training]  masked spectral distance: 0.0827508345246315, approx. progress: 99.99%
Epoch 00966: val_loss improved from 0.42491 to 0.42453
Epoch 00967: val_loss improved from 0.42453 to 0.42393
Epoch 00968: val_loss improved from 0.42393 to 0.42385
[training]  masked spectral distance: 0.07712535560131073, approx. progress: 99.99%
Epoch 01423: val_loss improved from 0.42385 to 0.42333
Epoch 01424: val_loss improved from 0.42333 to 0.42295
[training]  masked spectral distance: 0.07193974405527115, approx. progress: 99.99%
[training]  masked spectral distance: 0.0681426152586937, approx. progress: 99.99%
[training]  masked spectral distance: 0.06475312262773514, approx. progress: 99.99%
[training]  masked spectral distance: 0.06333990395069122, approx. progress: 99.99%
[training]  masked spectral distance: 0.06385600566864014, approx. progress: 99.99%
Epoch 06651: val_loss improved from 0.42295 to 0.42227
Epoch 06652: val_loss improved from 0.42227 to 0.42201
Epoch 06664: val_loss improved from 0.42201 to 0.42177
Epoch 06665: val_loss improved from 0.42177 to 0.42078
[training]  masked spectral distance: 0.060097597539424896, approx. progress: 99.99%
[training]  masked spectral distance: 0.06190778315067291, approx. progress: 99.99%
[training]  masked spectral distance: 0.059868186712265015, approx. progress: 99.99%
validation loss: 0.45370325446128845, pearson distance: 0.3723664879798889

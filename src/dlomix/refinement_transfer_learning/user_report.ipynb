{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Report for Refinement and Transfer Learning\n",
    "\n",
    "This notebook provides a detailed analysis of data exploration and training results for an automatic refinement and transfer learning pipeline. It includes visualizations of key dataset features like amino acid distribution, sequence lengths, collision energy, and intensity values for train, validation, and test sets. Spectral angle distributions are calculated and saved before and after training, enabling comparison of model performance. Training results, including learning curves and performance metrics, are documented to highlight improvements through the training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "LOGGING_DIR = Path(os.getcwd()) / \"log_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_json_data(results_log):\n",
    "    def load_json(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def plot_amino_acid_distribution(datasets=['train', 'val', 'test']):\n",
    "        fig, axes = plt.subplots(len(datasets), 1, figsize=(18, 5 * len(datasets)), sharey=True)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            file_path = results_log / f'amino_acid_distribution_{dataset}.json'\n",
    "            if os.path.exists(file_path):\n",
    "                data = load_json(file_path)\n",
    "                alphabet_keys = data['alphabet']\n",
    "                aa_counts = data['counts']\n",
    "\n",
    "                # Filter out amino acids with a count of 0\n",
    "                filtered_keys_counts = [(k, c) for k, c in zip(alphabet_keys, aa_counts) if c > 0]\n",
    "                filtered_keys = [k for k, c in filtered_keys_counts]\n",
    "                filtered_counts = [c for k, c in filtered_keys_counts]\n",
    "\n",
    "                axes[i].bar(filtered_keys, filtered_counts, edgecolor='black')\n",
    "                axes[i].set_title(f'{dataset.capitalize()} Set')\n",
    "                axes[i].set_xlabel('Amino Acid')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_distribution(feature, xlabel='', ylabel='Frequency', is_sequence=False, transform_func=None):\n",
    "        datasets = ['train', 'val', 'test']\n",
    "        fig, axes = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "        \n",
    "        for i, dataset in enumerate(datasets):\n",
    "            file_path = results_log / f'{feature}_distribution_{dataset}.json'\n",
    "            if os.path.exists(file_path):\n",
    "                data = load_json(file_path)\n",
    "                feature_data = data['hist']\n",
    "                bin_edges = data['bin_edges']\n",
    "                axes[i].hist(bin_edges[:-1], bins=bin_edges, weights=feature_data, edgecolor='black')\n",
    "                axes[i].set_title(f'{dataset.capitalize()} Set')\n",
    "                axes[i].set_xlabel(xlabel)\n",
    "                if i == 0:\n",
    "                    axes[i].set_ylabel(ylabel)\n",
    "\n",
    "        fig.suptitle(f'{xlabel.title()} Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_amino_acid_distribution()\n",
    "    plot_distribution('collision_energy_aligned_normed', xlabel='Collision Energy')\n",
    "    plot_distribution('intensities_raw', xlabel='Intensity')\n",
    "    plot_distribution('sequence', xlabel='Sequence Length')\n",
    "    plot_distribution('precursor_charge_onehot', xlabel='Precursor Charge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot the data\n",
    "plot_json_data(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training results\n",
    "\n",
    "##### Spectral Angle Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectral_angle_distributions(results_log, datasets=['train', 'val', 'test']):\n",
    "    \"\"\"\n",
    "    Reads the spectral angle distributions from JSON files and plots them before and after training.\n",
    "\n",
    "    Args:\n",
    "        results_log: Directory where the JSON files are saved.\n",
    "        datasets: A list of strings indicating which datasets to plot ('train', 'val', 'test').\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distributions)\n",
    "    \"\"\"\n",
    "    def load_json(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    fig_before, axes_before = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "    fig_after, axes_after = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "    \n",
    "    for ax_before, ax_after, dataset in zip(axes_before, axes_after, datasets):\n",
    "        filename = results_log / f'spectral_angle_distribution_{dataset}.json'\n",
    "        if os.path.exists(filename):\n",
    "            data = load_json(filename)\n",
    "            spectral_angles_before = data['before']['spectral_angles']\n",
    "            avg_sa_before = data['before']['average_spectral_angle']\n",
    "            spectral_angles_after = data['after']['spectral_angles']\n",
    "            avg_sa_after = data['after']['average_spectral_angle']\n",
    "\n",
    "            # Plot before training\n",
    "            ax_before.hist(spectral_angles_before, bins=30, alpha=0.75, edgecolor='black')\n",
    "            ax_before.axvline(avg_sa_before, color='r', linestyle='dashed', linewidth=1)\n",
    "            ax_before.text(ax_before.get_xlim()[1] * 0.3, ax_before.get_ylim()[1] * 0.9, f'Avg. SA = {avg_sa_before:.2f}', color='r')\n",
    "            ax_before.set_title(f'{dataset.capitalize()} Set Before Training')\n",
    "            ax_before.set_xlabel('Spectral Angle')\n",
    "            if dataset == datasets[0]:\n",
    "                ax_before.set_ylabel('Frequency')\n",
    "\n",
    "            # Plot after training\n",
    "            ax_after.hist(spectral_angles_after, bins=30, alpha=0.75, edgecolor='black')\n",
    "            ax_after.axvline(avg_sa_after, color='r', linestyle='dashed', linewidth=1)\n",
    "            ax_after.text(ax_after.get_xlim()[1] * 0.3, ax_after.get_ylim()[1] * 0.9, f'Avg. SA = {avg_sa_after:.2f}', color='r')\n",
    "            ax_after.set_title(f'{dataset.capitalize()} Set After Training')\n",
    "            ax_after.set_xlabel('Spectral Angle')\n",
    "            if dataset == datasets[0]:\n",
    "                ax_after.set_ylabel('Frequency')\n",
    "        else:\n",
    "            print(f\"No data found for {dataset} set.\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectral_angle_distributions(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral angle distribution is computed using the first 1000 batches for each dataset type (training, validation, and test) to provide a representative overview. Calculating the distribution across the entire dataset would result in excessive runtime; therefore, using 1000 batches serves as a practical and efficient approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training process evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    \"\"\"\n",
    "    Plots all columns in the provided DataFrame using seaborn, indicating the start of new phases.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    num_plots = 5  # Fixed number of plots\n",
    "    fig, axes = plt.subplots(nrows=(num_plots + 1) // 2, ncols=2, figsize=(15, 15))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    phase_transitions = data['phase'].diff().ne(0).cumsum() - 1\n",
    "    colors = ['blue', 'green', 'brown']\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    columns_to_plot = [4, 6, 5, 7, 3]  # Column indices to plot\n",
    "\n",
    "    for idx, column in enumerate(data.columns[columns_to_plot]):  \n",
    "        sns.lineplot(ax=axes[idx], data=data, x='batch', y=column)\n",
    "        for i in range(phase_transitions.nunique()):\n",
    "            pt_indices = data.index[phase_transitions == i].tolist()\n",
    "            if pt_indices:\n",
    "                pt = pt_indices[0]\n",
    "                axes[idx].axvline(x=data['batch'].iloc[pt], color=colors[i % len(colors)], linestyle='--', alpha=0.5, label=f'Phase {i+1}')\n",
    "        pretty_column_name = column.replace(\"_\", \" \")\n",
    "        axes[idx].set_title(f'{pretty_column_name.title()}')\n",
    "        axes[idx].set_xlabel('batches')\n",
    "        axes[idx].set_ylabel(pretty_column_name)\n",
    "        \n",
    "        # Set log scale for learning rate plot\n",
    "        if column == 'learning_rate':\n",
    "            axes[idx].set_yscale('log')\n",
    "\n",
    "    # Remove extra axes if any\n",
    "    for i in range(num_plots, len(axes)):  \n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    fig.legend(by_label.values(), by_label.keys(), loc='upper center')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_training_results = pd.read_csv(LOGGING_DIR / 'training_log.csv')\n",
    "plot_data(logged_training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logged_training_results.iloc[-1]['val_loss'] > 0.2:\n",
    "    val_loss_value = logged_training_results.iloc[-1]['val_loss']\n",
    "    print(f\"The model didn't learn enough about the data, leading to a validation loss higher than 0.2. The current validation loss is {val_loss_value:.6f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing per phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_freezing = pd.read_csv(LOGGING_DIR / 'freeze_log.csv', header=None, names=[\"freezing\", \"status\"], skip_blank_lines=False)\n",
    "logged_freezing['Phase'] = logged_freezing['freezing'].isna().cumsum() + 1\n",
    "logged_freezing = logged_freezing.dropna().reset_index(drop=True)\n",
    "logged_freezing['Phase'] = logged_freezing['Phase'].apply(lambda x: f\"Phase {x}\")\n",
    "logged_freezing['status'] = logged_freezing['status'].astype(int)\n",
    "logged_freezing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

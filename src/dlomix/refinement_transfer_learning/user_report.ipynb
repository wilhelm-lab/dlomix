{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Report for Refinement and Transfer Learning\n",
    "\n",
    "This notebook provides a detailed analysis of data exploration and training results for an automatic refinement and transfer learning pipeline. It includes visualizations of key dataset features like amino acid distribution, sequence lengths, collision energy, and intensity values for train, validation, and test sets. Spectral angle distributions are calculated and saved before and after training, enabling comparison of model performance. Training results, including learning curves and performance metrics, are documented to highlight improvements through the training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "LOGGING_DIR = Path(os.getcwd()) / \"log_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_json_data(results_log):\n",
    "    def load_json(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def plot_amino_acid_distribution(datasets=['train', 'val', 'test']):\n",
    "        fig, axes = plt.subplots(len(datasets), 1, figsize=(18, 5 * len(datasets)), sharey=True)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            file_path = results_log / f'amino_acid_distribution_{dataset}.json'\n",
    "            if os.path.exists(file_path):\n",
    "                data = load_json(file_path)\n",
    "                alphabet_keys = data['alphabet']\n",
    "                aa_counts = data['counts']\n",
    "\n",
    "                # Filter out amino acids with a count of 0\n",
    "                filtered_keys_counts = [(k, c) for k, c in zip(alphabet_keys, aa_counts) if c > 0]\n",
    "                filtered_keys = [k for k, c in filtered_keys_counts]\n",
    "                filtered_counts = [c for k, c in filtered_keys_counts]\n",
    "\n",
    "                axes[i].bar(filtered_keys, filtered_counts, edgecolor='black')\n",
    "                axes[i].set_title(f'{dataset.capitalize()} Set')\n",
    "                axes[i].set_xlabel('Amino Acid')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_distribution(feature, xlabel='', ylabel='Frequency', is_sequence=False, transform_func=None):\n",
    "        datasets = ['train', 'val', 'test']\n",
    "        fig, axes = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "        \n",
    "        for i, dataset in enumerate(datasets):\n",
    "            file_path = results_log / f'{feature}_distribution_{dataset}.json'\n",
    "            if os.path.exists(file_path):\n",
    "                data = load_json(file_path)\n",
    "                feature_data = data['hist']\n",
    "                bin_edges = data['bin_edges']\n",
    "                axes[i].hist(bin_edges[:-1], bins=bin_edges, weights=feature_data, edgecolor='black')\n",
    "                axes[i].set_title(f'{dataset.capitalize()} Set')\n",
    "                axes[i].set_xlabel(xlabel)\n",
    "                if i == 0:\n",
    "                    axes[i].set_ylabel(ylabel)\n",
    "\n",
    "        fig.suptitle(f'{xlabel.title()} Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_amino_acid_distribution()\n",
    "    plot_distribution('collision_energy_aligned_normed', xlabel='Collision Energy')\n",
    "    # plot_distribution('intensities_raw', xlabel='Intensity')\n",
    "    plot_distribution('sequence', xlabel='Sequence Length')\n",
    "    plot_distribution('precursor_charge_onehot', xlabel='Precursor Charge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot the data\n",
    "plot_json_data(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training results\n",
    "\n",
    "##### Spectral Angle Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectral_angle_distributions(results_log, datasets=['train', 'val', 'test']):\n",
    "    \"\"\"\n",
    "    Reads the spectral angle distributions from JSON files and plots them before and after training.\n",
    "\n",
    "    Args:\n",
    "        results_log: Directory where the JSON files are saved.\n",
    "        datasets: A list of strings indicating which datasets to plot ('train', 'val', 'test').\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distributions)\n",
    "    \"\"\"\n",
    "    def load_json(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    fig_before, axes_before = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "    fig_after, axes_after = plt.subplots(1, len(datasets), figsize=(18, 6), sharey=True)\n",
    "    \n",
    "    for ax_before, ax_after, dataset in zip(axes_before, axes_after, datasets):\n",
    "        filename = results_log / f'spectral_angle_distribution_{dataset}.json'\n",
    "        if os.path.exists(filename):\n",
    "            data = load_json(filename)\n",
    "            spectral_angles_before = data['before']['spectral_angles']\n",
    "            avg_sa_before = data['before']['average_spectral_angle']\n",
    "            spectral_angles_after = data['after']['spectral_angles']\n",
    "            avg_sa_after = data['after']['average_spectral_angle']\n",
    "\n",
    "            # Plot before training\n",
    "            ax_before.hist(spectral_angles_before, bins=30, alpha=0.75, edgecolor='black')\n",
    "            ax_before.axvline(avg_sa_before, color='r', linestyle='dashed', linewidth=1)\n",
    "            ax_before.text(ax_before.get_xlim()[1] * 0.3, ax_before.get_ylim()[1] * 0.9, f'Avg. SA = {avg_sa_before:.2f}', color='r')\n",
    "            ax_before.set_title(f'{dataset.capitalize()} Set Before Training')\n",
    "            ax_before.set_xlabel('Spectral Angle')\n",
    "            if dataset == datasets[0]:\n",
    "                ax_before.set_ylabel('Frequency')\n",
    "\n",
    "            # Plot after training\n",
    "            ax_after.hist(spectral_angles_after, bins=30, alpha=0.75, edgecolor='black')\n",
    "            ax_after.axvline(avg_sa_after, color='r', linestyle='dashed', linewidth=1)\n",
    "            ax_after.text(ax_after.get_xlim()[1] * 0.3, ax_after.get_ylim()[1] * 0.9, f'Avg. SA = {avg_sa_after:.2f}', color='r')\n",
    "            ax_after.set_title(f'{dataset.capitalize()} Set After Training')\n",
    "            ax_after.set_xlabel('Spectral Angle')\n",
    "            if dataset == datasets[0]:\n",
    "                ax_after.set_ylabel('Frequency')\n",
    "        else:\n",
    "            print(f\"No data found for {dataset} set.\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectral_angle_distributions(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral angle distribution is computed using the first 1000 batches for each dataset type (training, validation, and test) to provide a representative overview. Calculating the distribution across the entire dataset would result in excessive runtime; therefore, using 1000 batches serves as a practical and efficient approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training process evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    \"\"\"\n",
    "    Plots specified columns in the provided DataFrame, indicating the start of new phases and labeling them as they appear in the data.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    unique_phases = data['phase'].unique()\n",
    "    colors = ['blue', 'green', 'brown']\n",
    "    \n",
    "    # Plot Loss and Validation Loss with new colors\n",
    "    sns.lineplot(ax=axes[0, 0], data=data, x='batch', y='loss', color='#1f77b4')  # Deep Blue for Training\n",
    "    sns.lineplot(ax=axes[0, 0], data=data, x='batch', y='val_loss', color='#b41f4c')  # Soft Red for Validation\n",
    "    for i, phase in enumerate(unique_phases):\n",
    "        pt_indices = data.index[data['phase'] == phase].tolist()\n",
    "        if pt_indices:\n",
    "            pt = pt_indices[0]\n",
    "            axes[0, 0].axvline(x=data['batch'].iloc[pt], color=colors[i % len(colors)], linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].set_title('Loss and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Batches')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "\n",
    "    # Plot Masked Pearson Correlation Distance and Validation Masked Pearson Correlation Distance\n",
    "    sns.lineplot(ax=axes[0, 1], data=data, x='batch', y='masked_pearson_correlation_distance', color='#1f77b4')\n",
    "    sns.lineplot(ax=axes[0, 1], data=data, x='batch', y='val_masked_pearson_correlation_distance', color='#b41f4c')\n",
    "    for i, phase in enumerate(unique_phases):\n",
    "        pt_indices = data.index[data['phase'] == phase].tolist()\n",
    "        if pt_indices:\n",
    "            pt = pt_indices[0]\n",
    "            axes[0, 1].axvline(x=data['batch'].iloc[pt], color=colors[i % len(colors)], linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].set_title('Masked Pearson Correlation Distance and Validation')\n",
    "    axes[0, 1].set_xlabel('Batches')\n",
    "    axes[0, 1].set_ylabel('Masked Pearson Correlation Distance')\n",
    "\n",
    "    # Plot Learning Rate \n",
    "    sns.lineplot(ax=axes[1, 0], data=data, x='batch', y='learning_rate', color='#4d4d4d') \n",
    "    for i, phase in enumerate(unique_phases):\n",
    "        pt_indices = data.index[data['phase'] == phase].tolist()\n",
    "        if pt_indices:\n",
    "            pt = pt_indices[0]\n",
    "            axes[1, 0].axvline(x=data['batch'].iloc[pt], color=colors[i % len(colors)], linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].set_title('Learning Rate')\n",
    "    axes[1, 0].set_xlabel('Batches')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "\n",
    "    # Remove the unused fourth subplot\n",
    "    fig.delaxes(axes[1, 1])\n",
    "\n",
    "    # Create a custom legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], color='#1f77b4', lw=2, label='Training'),\n",
    "        plt.Line2D([0], [0], color='#b41f4c', lw=2, label='Validation')\n",
    "    ]\n",
    "    # Add phase transitions to the legend\n",
    "    for i, phase in enumerate(unique_phases):\n",
    "        handles.append(plt.Line2D([0], [0], color=colors[i % len(colors)], linestyle='--', lw=2, label=f'Phase {phase}'))\n",
    "\n",
    "    # Add overall legend\n",
    "    fig.legend(handles=handles, loc='upper center', ncol=3)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_training_results = pd.read_csv(LOGGING_DIR / 'training_log.csv')\n",
    "plot_data(logged_training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logged_training_results.iloc[-1]['val_loss'] > 0.2:\n",
    "    val_loss_value = logged_training_results.iloc[-1]['val_loss']\n",
    "    print(f\"The model didn't learn enough about the data, leading to a validation loss higher than 0.2. The current validation loss is {val_loss_value:.6f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing per phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training log to find out the starting phase\n",
    "training_log = pd.read_csv(LOGGING_DIR / 'training_log.csv')\n",
    "start_phase = training_log['phase'].min()\n",
    "\n",
    "# Load freezing log\n",
    "logged_freezing = pd.read_csv(LOGGING_DIR / 'freeze_log.csv', header=None, names=[\"freezing\", \"status\"], skip_blank_lines=False)\n",
    "\n",
    "# Assign phases based on the start phase from training log\n",
    "logged_freezing['Phase'] = logged_freezing['freezing'].isna().cumsum() + start_phase\n",
    "logged_freezing = logged_freezing.dropna().reset_index(drop=True)\n",
    "\n",
    "# Adjust phase labeling\n",
    "logged_freezing['Phase'] = logged_freezing['Phase'].apply(lambda x: f\"Phase {x}\")\n",
    "logged_freezing['status'] = logged_freezing['status'].astype(int)\n",
    "\n",
    "# Display the processed data\n",
    "logged_freezing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlomix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
